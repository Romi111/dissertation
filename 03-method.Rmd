```{r, setup-method-fix, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE, 
                      collapse = TRUE,
                      fig.width = 6,
                      fig.asp = .618,
                      out.width = "80%", 
                      fig.align = "center", 
                      results = "hold") 
```

# Method

This is a causal comparative study, in that explanations for differences in PECs are sought after their occurrence. This study uses ESM (Hektner et al., 2007) data collected as part of a study of learners’ interest and engagement in outside-of-school STEM enrichment programs (Shumow & Schmidt, 2013). It makes use of a sequential exploratory data analysis strategy, in which qualitative data is analyzed to enrich quantitative findings (Creswell, Clark, Gutmann, & Hanson, 2003). In particular, moments in which learners are particularly engaged are identified as part of the quantitative analysis; these moments are then coded qualitatively to identify their common characteristics, first through an inductive step and then through a confirmatory step involving a second rater. While programs have been video-recorded, the video has not been coded for the aspects of work with data, and the other measures from ESM and pre-survey data are to be constructed for this study. 
## Participants

Participants will consist of 203 youth. Students in these programs are from diverse racial and ethnic backgrounds (see Table 1). Most participants are around 13 years old (from students whose age was available: *M* = 12.71, *SD* = 1.70, *min.* = 10.75, *max.* = 16.36). Detailed demographic characteristics of learners are presented in Table 1.

```{r, eval = T}
d <- tibble::tribble(
  ~Students, ~Percentage,
  "Sex",          NA,
  "Male",         50L,
  "Female",         50L,
  "Race/Ethnicity",          NA,
  "Hispanic",         48L,
  "White",          6L,
  "Black",         36L,
  "Multi-racial",          3L,
  "Asian/Pacific Islander",          7L,
  "Parent Education",          NA,
  "High School or Below",         79L,
  "Graduated from College (B.A. or B.S.)",         21L
)

d %>% 
  kable(booktabs = TRUE, caption = "Demographic characteristics of youth")
```

## Context	

The setting for this study will be nine out-of-school STEM programs designed around best practices in urban areas in the Northeast United States during the summer of 2015. These are described in the appendix with pseudonyms for the program names. 

Two intermediary organizations contracted by the urban area school districts to administer the summer programs. The two intermediaries were responsible for soliciting and enrolling youth; establishing guidelines for the design of the programs, and the goals of the programs; and provide training and professional development for the program’s staff. A key difference between the intermediary organizations was that one separated academic and enrichment-related activities, whereas, in another, which was more closely involved in the day-to-day activities of the program, the academic and enrichment components were more integrated, which may have program-specific effects on learners’ engagement. Many of the programs aim to involve learners in work with data. These learning environments bring together youth activity leaders, educators, and those with technical expertise in STEM domains. Students spent around three hours per day for four days per week for the approximately four-week programs, which were taught by youth activity leaders and scientists, engineers, and other community members with technical expertise. 

## Procedure

Students completed a pre-survey before the program. Students also completed pre- and post-course surveys of their experience in STEM, intention to pursue a STEM major or career, and questions for other motivation and engagement-related measures. At the beginning of the programs, students were introduced to the study and the phones used for data collection related to the ESM. ESM data were collected two days each week, for three weeks (weeks 2-4 of the program). In all of the programs, about equal video-recording time was dedicated to classroom and field experiences. This detail is important because programs associated with one of the intermediaries rotated between classroom and field experience days, while the other used the first half of each day for one (i.e., classroom activities) or the other (i.e., field experience days). 

Each day, students were signaled four times. These signals were at the same time for all of the students within their program, but at different times between programs and between days within programs (with the constraint that no two signals could occur less than ten minutes apart). All of the programs were video-recorded by research team members and on three occasions research team members who recorded detailed field notes on the nature of program activities. So that measures corresponding to the video and ESM data can be matched, videos include a signal from the video-recorder identifying the ESM signal to which students responded at that point in the video. 

In a reflection of the dynamic conceptualization of engagement, this study uses data collected from ESM. As such, learners are prompted at regular intervals to respond to short questions about their perceptions of their engagement and its influencers. Though time-consuming to carry out, ESM can be a powerful measure that leverages the benefits of both observational and self-report measures, allowing for some ecological validity and the use of closed-form questionnaires amenable to quantitative analysis (Csikszentmihalyi & Larson, 1987). Despite the logistic challenge of carrying out ESM in large studies, some scholars have referred to it as the “gold standard” for understanding individual’s subjective experience (Schwarz, Kahneman, & Xu, 2009). This approach has the benefit of measuring learners’ engagement at a fine grain-size: Changes in the activity on learners’ engagement, even within the same session of the program, and changes in how influencers of engagement impact engagement and how the activity may relate to engagement, can be measured. 

## Data Sources and Measures

Data sources will consist of self-reported ESM measures of engagement and learners’ perceptions of themselves and the activity, pre-survey measures of students’ interest, students’ demographic information, and video-recordings of programs. 

### ESM measures of learners’ engagement and its conditions

Measures for engagement and its conditions will be constructed from three ESM responses for engagement and two ESM responses for the conditions of engagement. The three variables for engagement are for learning (for the cognitive engagement construct), working hard (for behavioral engagement), and enjoying (for affective engagement). The variables for the conditions are for perceived challenge and perceived competence. All five items will be used to construct PECs. Each of the ESM items consisted of the item text and the following four item response options, of which students were directed to select one: Not at all (associated with the number 1 on the survey), A little (2), Somewhat (3), and Very Much (4), as presented in Table 3. 

```{r}
d <- tibble::tribble(
               ~Construct,                                                                         ~Item.text,
   "Cognitive engagement", "As you were signaled, were you learning anything or getting better at something?",
  "Behavioral engagement",                                 "As you were signaled, how hard were you working?",
   "Affective engagement",                          "As you were signaled, did you enjoy what you are doing?",
    "Perceived challenge",                     "As you were signaled, how challenging was the main activity?",
   "Perceived competence",                        "As you were signaled, were you good at the main activity?"
  )

d %>% 
  knitr::kable(booktabs = TRUE, caption = "ESM measures for profiles of engagement and its conditions (PECs)") %>% 
    kableExtra::kable_styling(latex_options = "scale_down")

```

### Survey measures of pre-interest

Measures of students’ pre-interest are used as student-level influencers of PECs. In particular, three items adapted from Vandell, Hall, O’Cadiz, and Karsh (2012) were used, with directions for students to rate their agreement with the items’ text using the same scale as the ESM items: Not at all (associated with the number 1 on the survey), A little (2), Somewhat (3), and Very Much (4). The items are presented in Table 4.

```{r}
d <- tibble::tribble(
                     ~Construct,                                                            ~`Items.text`,
  "Individual interest in STEM",               "I am interested in science / mathematics / engineering.",
                             NA,                 "At school, science / mathematics / engineering is fun",
                             NA, "I have always been fascinated by science / mathematics / engineering)"
  )

d %>% knitr::kable(booktabs = TRUE, caption = "Survey Measure Used in This Study")
```

### Codes for instructional support for work with data from the video-recordings

Different aspects of instructional support for work with data will be identified from video-recordings with the use of a coding frame with five for each of the aspects of instructional support for work with data. These codes are developed from the STEM-Program Quality Assessment (STEM-PQA; Forum for Youth Investment, 2012), an assessment of quality programming in after school programs. Specific details on how the measure aligns iwth the original STEM-PQA on which this measure is based are presented in the appendix.

```{r}
d <- tibble::tribble(
  ~`Work with data code`,                                                                                                    ~Description,                                                                                                                                                                                                                                                                                                                                                        ~`Categories from STEM-PQA`,
  "Asking questions or defining problems",                                            "Discussing and exploring topics to investigate and pose questions.",                                                   "Predict, conjecture, or hypothesize (Staff support youth in using a simulation, experiment, or model to answer questions, explore solutions, or test hypotheses (e.g., Youth run a robotics program to determine whetherit does what they expect it to; Youth try an alternate way to solve an equation and test their results against another example, etc.))",
  "Making observations",          "Watching and noticing what is happening with respect to the phenomena or problem being investigated.",                                                                        "Classify or abstract (Staff support youth in using classification and abstraction, linking concrete examples to principles, laws, categories, and formulas (e.g., Mice, porcupines, and squirrels are all rodents, rodents are all mammals; The pool ball moved because for every action, there is an equal and opposite reaction; etc.))",
  "Generating data", "Figuring out how or why to inscribe an observation as data and generating coding frames or measurement tools.",                                                                                                                                                                               "Collect data or measure (Staff support youth in collecting data or measuring (e.g., Youth use rulers or yardsticks to measure length; Youth count the number of different species of birds observed in a specific location, etc.))",
  NA,                                                                                                              NA, "Highlight precision and accuracy (Staff highlight value of precision and accuracy in measuring, observing, recording, or calculating (e.g., measurement error can impact an experiment or conclusion; measure twice, cut once; scientist always need to double-check their claculations before drawing conclusions; you must observe carefully to see the difference between various species of sparrows, etc.))",
  "Data modeling",  "Understanding and explaining phenomena using models of the data that account for variability or uncertainty.",                                                       "Simulate, experiment, or model (Staff support youth in using a simulation, experiment, or model to answer questions, explore solutions, or test hypotheses (e.g., Youth run a robotics program to determine whether it does what they expect it to; Youth try an alternate way to solve an equation and test their results against another example, etc.))",
  "Interpreting and communicating findings",                                                               "Discussing and sharing and presenting findings.",                                                                                                                                                                                               "Analyze (Staff support youth in analyzing data to draw conclusions (e.g., after an experiment, youth are asked to use results to make a generalization like \"Your heartbeat increases when you exercise\", etc.))"
)


d[, -3] %>%
  knitr::kable(booktabs = TRUE, caption = "Coding Frame for Instructional Support for Work With Data") %>% 
  kableExtra::kable_styling(latex_options = "scale_down")
```

<!-- ### Codes for activity -->

<!-- Codes for the activity are created on the basis of codes for the student activity. A coding frame with the following categories was used to code all of the data. This coding frame included categories for both inquiry-based (i.e., laboratory activties and those in which youth are creating products) as well as more traditional (i.e., listening to a lecture). A code for inquiry-based activities, reflecting activities in which students may be doing more hands-on work that may be supportive of work with data, was created. This code had a value of one if either laboratory activities or those in which youth are creating products had a value of one and was otherwise zero. -->

### Demographic variables

In addition to the measures described in this section, demographic information for youths’ gender and their racial and ethnic group will be used to construct demographic variables for gender and membership in an under-represented (in STEM) group; membership in an under-represented group will be identified on the basis of students’ racial and ethnic group being Hispanic, African American, Asian or Pacific Islanders, or native American. 

## Data Analysis

Before analyzing data to answer the research questions, preliminary analyses will be carried out. The steps for both preliminary and the primary analyses are described in this section.

Preliminary analyses. Codes for the aspects of work with data will be created from coding videos of the activity occurring immediately before learners were signaled to respond to a survey as part of the ESM. Before one rater independently codes the video associated with all of the signals, inter-rater reliability between the primary and a secondary rater will be established. The coding frame in Table 5 will be used to code a random sample of the videos associated with 30 of the ESM responses. The coding frame will be used to code for the presence of one and only one of the codes for the aspects of work with data. The agreement between the original and second rater will be calculated using Fleiss’ kappa, with a value above .70 indicating satisfactory agreement. If the disagreement is not satisfactory, then cases in which the raters disagreed will be discussed and resolved, and a different sample of videos associated with ESM responses will be coded again. Following the satisfactory agreement, all of the videos associated with ESM signals will be coded independently: In order to provide to the coder the context to the video segments to be coded, all of the video segments will be viewed (but only those associated with ESM signals will be coded).

First-order Pearson correlations, frequency, range, mean, skew, kurtosis, and standard deviations will be examined for all variables including ESM measures for challenge, competence, cognitive, behavioral and affective engagement, and for the pre-survey measure for interest. In addition, the frequency of the codes for aspects of work with data, and the number of responses by student, program, and moment will be examined.

### Primary analyses for RQ #1

To answer this question, PECs will be constructed using on the basis of five variables: cognitive, behavioral, and affective engagement and learners’ perceptions of challenge and competence. Answers to this question will help to understand how the aspects of engagement relate to both one another and to key conditions that influence engagement. 

To create PECs, a mixture modeling approach will be carried out. Mixture modeling is an approach for identifying distinct distributions, or mixtures of distributions, of measured variables. A type of mixture modeling within a latent variable modeling framework, Latent Profile Analysis (LPA; Harring & Hodis, 2016; Muthen, 2004) is used in this study, in particular, to identify the number and nature of PECs. LPA allows for capturing the multidimensional nature of engagement. From this approach, different parameters - means, variances, and covariances - are freely estimated across profiles, fixed to be the same across profiles, or constrained to be zero. The MPlus software is commonly used to estimate these models (see [here](https://www.statmodel.com/examples/mixture.shtml)) using the expectation-maximization (EM) algorithm to obtain the maximum likelihood estimates for the parameters.

LPA can be used to identify common patterns in learners’ ESM responses as part of a person-oriented analysis to construct PECs. These profiles make it possible to analyze the multivariate data collected on engagement in a way that balances the parsimony of a single model for all learners with a recognition of individual differences in how learners’ experience each of the dimensions of engagement together at the same time. A key benefit of the use of LPA, in addition to likelihood estimation-based fit indices, is probabilities of an observation being a member of a cluster, unlike in hierarchical and k-means cluster analysis, for which an observation is hard classified exclusively into one cluster. 

As part of LPA, different models that determine whether and how different parameters (i.e., means, variances, and covariances) are estimated. In addition, the number of profiles to estimate must be provided by the analyst. Determining the number of profiles depends on fit statistics (such as information criteria and the entropy statistic) as well as concerns of parsimony and interpretability. In general, the approach to choosing the model is similar to choosing the number of profiles, requiring deciding on the basis of evidence from multiple sources. The models are described in-depth in the appendix.

Profiles will be constructed with the five self-reported ESM measures for cognitive, behavioral, and affective engagement and perceptions of challenge and competence. Once this step is carried out, the probability of a response being associated with a profile of engagement and its conditions will be used as the dependent variable for subsequent analyses. An interface to the MCLUST software will be developed and used to carry out the LPA. The number of profiles will be determined on the basis of the log-likelihood and bootstrapped likelihood ratio test, entropy, Akaike Information Criteria, and Bayesian Information Criteria statistics, as well as concerns of parsimony and interpretability. Because of sampling error possible through the resampling needed for this approach, the cross-validation will be repeated at least 30 times for each candidate profile solution. This analysis can help us to understand how patterns in higher or lower levels of the variables used to construct the profiles group together in PECs, providing insight into both how engagement is commonly experienced as a meta-construct as well as how key conditions influence engagement. 

### Primary analyses for RQ #2

To answer this question, on how well the aspects of instructional support for work with data predict the PECs, first, indicators for activities coded for any of the five aspects of work with data and either of the other two activities will be used to predict each PEC. This will help us to understand how work with data, in general, is different from other activities in terms of predicting each PEC. Next, how each of the five aspects of work with data, as well as the other activities, predict each PEC will be explored. This will help us understand how learners engage in specific aspects of work with data.

Due to similar mixed-effects models used to analyze data to answer RQ #2 and #3, the data analysis strategy for these steps is described together here. First, the general approach used for specifying the mixed effects is first described, followed by details about how the models will be used to provide answers to the specific research questions.

All of the models will use random effects for learner, momentary, and program effects. Learner and moment can be considered to be crossed with both nested within the program. Because the outcome from LPA is not a hard classification (i.e., an observation is in a profile—or not) but a probability, the outcome is treated as a continuous variable. There will be as many models as profiles identified in the preliminary analysis; so, the profile will be different between models. A bottom-up model-building process (West, Welch, & Galecki, 2014), in which a more complex model is constructed on the basis of and continually compared to a more simple model, is used. 

First, null models with only the random parts (i.e., random learner, momentary, and program effects) will be specified. Then, the predictors will be added to the model with the main effects of the variables added to the null mixed effects model. The main effects are for the aspects of work with data and instructional support for the aspects of work with data as well as individual interest in STEM (as a control variable). Note that the interaction between individual interest in STEM and the aspects of work with data is added in a separate step, as described in the next section. The model with the random effects for the learner, moment, and program and with the direct effects of all the predictor variables is presented below.

Here, the probability of a response being associated with a profile is predicted by the direct effects of indicators for the aspects of instructional support work with data measured at the momentary level, their individual interest in STEM measured at the youth level, and the random learner, moment, and program effects. The general specification for the models for learner i during moment j in program k is written as:

<!-- $$ -->
<!-- For\quad learner\quad i\quad during\quad moment\quad j\quad in\quad program\quad k:\\ \\ Pr({ { profile }_{ ijk })\quad =\quad  }\\ \\ Fixed\quad parts:\\ \\ { \beta  }_{ 00 }\quad +\\ { { \beta  }_{ 01 }(Indicator\quad for\quad support\quad for\quad asking\quad questions) }_{ j }\quad +\\ { { \beta  }_{ 02 }(Indicator\quad for\quad support\quad for\quad making\quad observations) }_{ j }\quad +\\ { { \beta  }_{ 03 }(Indicator\quad for\quad support\quad for\quad generating\quad data) }_{ j }\quad +\\ { { \beta  }_{ 04 }(Indicator\quad for\quad support\quad for\quad data\quad modeling) }_{ j }\quad +\\ { { \beta  }_{ 06 }(Dummy\quad code\quad for\quad female\quad gender) }_{ i }\quad +\\ { { \beta  }_{ 07 }(Dummy\quad code\quad for\quad member\quad of\quad under-represented\quad group) }_{ i }\quad +\\ { { \beta  }_{ 08 }(Pre-program\quad STEM\quad interest) }_{ i }\quad +\\ \\ Random\quad parts:\\ \\ { \alpha  }_{ learner }(Youth\quad effect)_{ i }\quad +\\ { { { \alpha  }_{ moment } }(Momentary\quad effect) }_{ j }\quad +\\ { { \alpha  }_{ program }(Program\quad effect) }_{ k }\quad +\\ { \varepsilon  }_{ ijk }\\ \\ Where\quad { \alpha  }_{ learner },\quad { \alpha  }_{ moment },\quad and\quad { \alpha  }_{ program }\quad are\quad assumed\quad to\quad \~ \quad N({ { \mu  }_{ \alpha  } },\quad { \sigma  }_{ \alpha  }^{ 2 }) -->
<!-- $$ -->

Findings associated with this research question will help to understand how learners engage during different aspects of work with data and how engagement during the aspects of data differ from engagement during non-instructional activities. Another benefit of these models is the variance components, which can be interpreted in terms of the intraclass correlations. Because momentary and learner random effects are crossed and both nested with the program random effects, estimates for each of these random effects can provide information on the sources of unexplained variability in the PECs, thus helping us to understand the amount of variation that variables at each of the levels of the random effects (learner, moment, and program) can be explained.

<!-- ### Primary analyses for RQ #3 -->

<!-- To answer this question, on how well the code for the inquiry-based activity predicts the PECs, first, indicators for activities coded for any of the five aspects of work with data will be interacted with a dummy code indicating instructional support in general, created on the basis of any of the variables for instructional support for work with data being equal to 1. This dummy coded variable will then be interacted with any of the aspects of work with data with relations to the PECs. Second, each of aspects of work with data with relations to the PECs will be interacted with a dummy code for the specific aspect of instructional support (in Table 6).  This will help us to understand how work with data, in general and in terms of specific forms of instructional support, differs from other activities and how support from the instructor can contribute to more engaging work with data. -->

### Primary analyses for RQ #3

To answer this question, on how the relationships between work with data and the PECs depends on student characteristics, the direct effects of pre-program interest in STEM, gender, and under-represented minority [URM] status, without other predictor variables, will be expored. Then, models with these variables and the composite variable for instructional support for work with data will be specified.These analyses will be carried out separately for relations between work with data (on its own, corresponding to the analyses carried out for RQ #3) and work with data with instructional support (for RQ #4). Next, for any specific aspect of work with data that significantly predicts each PEC, the same will be carried out, so that the interaction between individual interest in STEM and the specific aspect of work with data will be used to predict each PEC. These interactions between individual interest in STEM and the dummy codes for aspects of work with data will be added to the model specification for RQ #2. Answers to this question will help us learn how the relationships between the PECs and the aspects of work with data vary on the basis of a trait-like characteristic of the learner may have important impacts. Given the exploratory nature of discovering which PECs emerge and how other factors relate to them, specific hypotheses are not made at this time.

### Primary analyses for RQ #4

To answer this question, on the common characteristics of potentially adaptive PECs, a sequential exploratory data analysis strategy is used. While the activity in terms of the aspects of work with data and the other activities likely predicts differences in PECs, there may be other characteristics that predict PECs, and those characteristics that predict potentially adaptive, or beneficial to students’ learning, PECs may be useful to identify both for interpreting findings from the present study and for future research. To answer this question, heterogeneity in terms of how the aspects of work with data relate to the PECs will first be identified. For example, if constructing measures is found to be associated with both potentially adaptive and potentially maladaptive PECs, then videos associated with this aspect of work with data will be interrogated further for this research question. 

The use of mixed effects models as part of the earlier research questions provides an especially useful strategy for selecting cases because the random moment effects represent moments that are associated with especially higher probabilities of responses associated with the different PECs. PECs will be identified and then coded qualitatively as part of an Extreme Case Approach. Selection of cases in this way also addresses a key challenge of the Extreme Case Approach, namely, how to present the variability among cases that may be selected because they are so different from the others—and from one another. The videos to select will be identified on the basis of moment-specific predictions accounting for all of the variables used to investigate the relations examined as part of question 2. If the moment-specific prediction for a potentially adaptive profile is especially positive and large, this suggests that there are characteristics of this moment that help to explain how students engaged in the aspects of work with data in highly engaging ways. Similarly, if the moment-specific prediction for a potentially maladaptive profile is especially negative and large, this suggests that there are characteristics of this moment that help to explain how this activity was not highly engaging. This analysis can help us to develop an account of what may distinguish these extreme cases from the majority with respect to the factors that influence engagement in work with data, as well as what may be particular to each specific case (Jahnukainen, 2010). Note that as part of an sequential exploratory mixed methods design, the focus of this qualitative analysis may shift based on what the results of the quantitative analyses suggest. 

To code the data, three research assistants trained for approximately eight hours over four meetings. Then, each research assistant coded all of the segments associated with one of the videos. After the coding was complete, the three research assistants and I met to discuss how well the coding frame and potential sources of disagreement. Then, two coders coded every segment that was coded for at least one of the aspects of instructional support for work with data. After each program, the coders met to discuss potential issues that emerged throughout the coding, and to clarify how they applied the coding frame. As this was open-ended coding with the aim to provide greater detail and context for the findings associated with research questions #2-#4, establishing reliability among the coders was not carried out. The coders sought to document:

* The characteristics of instructional support for work with data
* Other aspects of the instructional context that impacts student work with data

Note that while the first of the two aspects focuses on the support provided by the instructor, the second aspect focuses on how students engage in work with data in ways that on occasion diverge (in ways productive and not productive in terms of student work with data) from what would be expected on the basis of the instructional support. This coding resulted in around three to four sentence notes associated with each segment from each of two raters. Then, I reviewed these notes with the aim to identify themes based on enriching and better understanding the findings for research questions #2-#4 and, beyond these findings, to better understand the nature of work with data in summer STEM programs.

## Power Analysis

Few publications and tools address the question of statistical power for models with crossed random effects (Westfall, Kenny, & Judd, 2014). To carry out power analysis for detecting the minimum detectable effect for the relationship between one of the aspects of work with data and profiles of engagement, Westfall et al.’s (2017) software Power Analysis for General Anova designs to calculate power for models with arbitrarily complex random effects structures is used. The power, or [replace], was set to 0.80. The results of the power analysis indicated that a minimum detectable d (effect size) is 0.43, a moderate effect (Cohen, 1992). 

## Sensitivity Analysis

For observational studies, such as the present study, it can be important to determine how robust an inference is to alternative explanations. One approach to addressing this is sensitivity analysis, which involves quantifying the amount of bias that would be needed to invalidate an inference (hypothetically, this bias might be due to omitted or confounding variables, measurement, missing data, etc.). Using the approach described in Frank, Maroulis, Duong, and Kelcey (2013), I carried out sensitivity analysis for inferences we made relative to our key findings. The result, and what is used to interpret and contextualize findings, is a numeric value for each effect that indicates the proportion of the estimate that would have to be biased in order to invalidate the inference: higher values indicate more robust estimates in that the inferences would still hold even if there were substantial bias in the estimate. 

## Limitations

This study has three primary limitations. First, this study does not consider outcomes from engaging, such as the products of neither students’ work, nor the specific cognitive capabilities they develop through their participation. Second, the context for this study is suited to understanding engagement in aspects of work with data but not explicitly designed for it, and learning environments that deliberately support work with data over a long period may demonstrate different patterns of engagement than those examined in this study because of the focus on and sequencing of the aspects of work with data, which may make it more (or less) cognitively, behaviorally, or affectively engaging than is determined in this study. Third, this program is not representation of all outside-of-school programs, as many of the programs were based on characteristics of model STEM enrichment programs; as a result, engagement may be different in other STEM enrichment programs depending on characteristics of the programs and their activities, and findings from this study should be interpreted in terms of programs that share similar features in terms of their design. 
