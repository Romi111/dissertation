# Method

```{r, setup-method, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE, 
                      collapse = TRUE,
                      error = TRUE,
                      fig.width = 6,
                      fig.asp = .618,
                      out.width = "80%", 
                      fig.align = "center", 
                      results = "hold",
                      knitr.kable.na = '') 

options(knitr.kable.NA = '')
```

In recognition of the challenge of studying engagement in learning environments where factors related to activities, learners, and each of the nine programs all interact at the same time, this study uses a methodological approach suited to studying engagement as a dynamic, multi-faceted experience. Specifically, this study employs the Experience Sampling Method (ESM; Hektner, Schmidt, & Csikszentmihalyi, 2007) where learners answer short questions about their experience when signaled. This approach is both sensitive to changes in engagement over time, as well as between learners and allows us to understand engagement and how factors impact it in more nuanced and complex ways (Turner & Meyer, 2000).

## Participants

Participants consist of 203 youth. Youth in these programs are from diverse racial and ethnic backgrounds (see Table 1). Most participants are around 13 years old (from youth whose age was available: *M* = 12.71, *SD* = 1.70, *min.* = 10.75, *max.* = 16.36). Detailed demographic characteristics of learners are presented in the table. 

```{r, eval = T}
d <- tibble::tribble(
  ~Youth, ~Percentage,
  "Sex",          NA,
  "Male",         50L,
  "Female",         50L,
  "Race/Ethnicity",          NA,
  "Hispanic",         48L,
  "White",          6L,
  "Black",         36L,
  "Multi-racial",          3L,
  "Asian/Pacific Islander",          7L,
  "Parent Education",          NA,
  "High School or Below",         79L,
  "Graduated from College (B.A. or B.S.)",         21L
)

d %>% 
  kable(booktabs = TRUE, caption = "Demographic characteristics of youth")
```

## Context	

The setting for this study is nine out-of-school STEM programs designed around best practices in urban areas in the Northeast United States during the summer of 2015. These are described in the appendix with pseudonyms for the program names. Two intermediary organizations contracted by the urban area school districts to administer the summer programs. The two intermediaries were responsible for soliciting and enrolling youth; establishing guidelines for the design of the programs, and the goals of the programs; and provide training and professional development for the program’s staff. A key difference between the intermediary organizations was that one separated academic and enrichment-related activities, whereas, in another, which was more closely involved in the day-to-day activities of the program, the academic and enrichment components were more integrated, which may have program-specific effects on learners’ engagement. Many of the programs aim to involve learners in work with data. These learning environments bring together youth activity leaders, educators, and those with technical expertise in STEM domains. Youth spent around three hours per day for four days per week for the approximately four-week programs, which were taught by youth activity leaders and scientists, engineers, and other community members with technical expertise. 

## Procedure

Youth completed a pre-survey before the program. Youth also completed pre-course surveys of their experience in STEM, intention to pursue a STEM major or career, and questions for other motivation and engagement-related measures. At the beginning of the programs, youth were introduced to the study and the phones used for data collection related to the ESM. ESM data were collected two days each week, for three weeks (weeks 2-4 of the program). In all of the programs, about equal video-recording time was dedicated to classroom and field experiences. This detail is important because programs associated with one of the intermediaries rotated between classroom and field experience days, while the other used the first half of each day for one (i.e., classroom activities) or the other (i.e., field experience days). 

Each day, youth were signaled four times. These signals were at the same time for all of the youth within their program, but at different times between programs and between days within programs (with the constraint that no two signals could occur less than ten minutes apart). All of the programs were video-recorded by research team members and on three occasions research team members also recorded detailed field notes on the nature of program activities. So that measures corresponding to the video and ESM data can be matched, videos include a signal from the video-recorder identifying the ESM signal to which youth responded at that point in the video. 

In a reflection of the dynamic conceptualization of engagement, this study uses data collected from ESM. As such, learners are prompted at regular intervals to respond to short questions about their perceptions of their engagement and its influencers. Though time-consuming to carry out, ESM can be a powerful measure that leverages the benefits of both observational and self-report measures, allowing for some ecological validity and the use of closed-form questionnaires amenable to quantitative analysis (Csikszentmihalyi & Larson, 1987). Despite the logistic challenge of carrying out ESM in large studies, some scholars have referred to it as the “gold standard” for understanding individual’s subjective experience (Schwarz, Kahneman, & Xu, 2009). This approach has the benefit of measuring learners’ engagement at a fine grain-size: Changes in the activity on learners’ engagement, even within the same session of the program, and changes in how influencers of engagement impact engagement and how the activity may relate to engagement, can be measured. 

## Data Sources and Measures

Data sources consist of self-reported ESM measures of engagement and learners’ perceptions of themselves and the activity, pre-survey measures of youths’ interest, youths’ demographic information, and video-recordings of programs. 

### ESM measures of learners’ engagement and its conditions for the profiles

Measures for engagement and its conditions were constructed from three ESM responses for engagement and two ESM responses for the conditions of engagement. The three variables for engagement are for learning (for the cognitive engagement construct), working hard (for behavioral engagement), and enjoying (for affective engagement). The variables for the conditions are for perceived challenge and perceived competence. All five items are used to construct profiles. Each of the ESM items consisted of the item text and the following four item response options, of which youth were directed to select one: Not at all (associated with the number 1 on the survey), A little (2), Somewhat (3), and Very Much (4), as presented in Table 3. 

```{r}
d <- tibble::tribble(
               ~Construct,                                                                         ~Item.text,
   "Cognitive engagement", "As you were signaled, were you learning anything or getting better at something?",
  "Behavioral engagement",                                 "As you were signaled, how hard were you working?",
   "Affective engagement",                          "As you were signaled, did you enjoy what you are doing?",
    "Perceived challenge",                     "As you were signaled, how challenging was the main activity?",
   "Perceived competence",                        "As you were signaled, were you good at the main activity?"
  )

d %>% 
  knitr::kable(booktabs = TRUE, caption = "ESM measures for profiles of engagement and its conditions") %>% 
    kableExtra::kable_styling(latex_options = "scale_down")

```

### Survey measures of pre-interest

Measures of youths’ pre-interest are used as youth-level influencers of the profiles. In particular, three items adapted from Vandell, Hall, O’Cadiz, and Karsh (2012) were used, with directions for youth to rate their agreement with the items’ text using the same scale as the ESM items: Not at all (associated with the number 1 on the survey), A little (2), Somewhat (3), and Very Much (4). The measure was constructed by taking the maximum value for the scales for the different content areas (science, mathematics, and engineering), so that the value for a youth whose response for the science scale was 2.5 and for the mathematics scale was 2.75 would be 2.5. The items are presented in Table 4.

```{r}
d <- tibble::tribble(
                     ~Construct,                                                            ~`Items.text`,
  "Individual interest in STEM",               "I am interested in science / mathematics / engineering.",
                             NA,                 "At school, science / mathematics / engineering is fun",
                             NA, "I have always been fascinated by science / mathematics / engineering)"
  )

d %>% 
  knitr::kable(booktabs = TRUE, caption = "Measure for pre-program interest in STEM") %>% 
  kable_styling(full_width = FALSE)
```

### Codes from video-recordings for work with data

Different aspects of work with data are identified from video-recordings with the use of a coding frame with five for each of the aspects of work with data. These codes are developed from the STEM-Program Quality Assessment (STEM-PQA; Forum for Youth Investment, 2012), an assessment of quality programming in after school programs.	For the PQA, raters contracted by American Institute of Research (AIR) were trained in the use of the PQA measure during February, 2017. Raters completed a four-hour online training module on the overall PQA tool and then attended an in-person two-day training led by a trainer from the David P. Weikart Center for Youth Program Quality, the tool’s publisher, where they learned about the instrument, trained on its use, and then established inter-rater reliability with a master coder. For the STEM-PQA, three of the same raters contracted by AIR to overall PQA measure used the STEM-PQA scored one video segment, for which there were no disagreements on scoring across the four raters on any items. If any of the raters encountered into a situation that was difficult to score, they would all discuss the issue by telephone or more often by email after viewing the video in question and reach a consensus on how to score the specific item. Programs were divided up among all of the raters, so raters coded some of the videos for all of the programs.

 Specific details on how the measure aligns with the original STEM-PQA on which this measure is based are presented in the appendix.

```{r}
d <- tibble::tribble(
  ~Code,                                                                                                    ~Description,                                                                                                                                                                                                                                                                                                                                                        ~`Categories from STEM-PQA`,
  "Asking questions or defining problems",                                            "Discussing and exploring topics to investigate and pose questions.",                                                   "Predict, conjecture, or hypothesize (Staff support youth in using a simulation, experiment, or model to answer questions, explore solutions, or test hypotheses (e.g., Youth run a robotics program to determine whetherit does what they expect it to; Youth try an alternate way to solve an equation and test their results against another example, etc.))",
  "Making observations",          "Watching and noticing what is happening with respect to the phenomena or problem being investigated.",                                                                        "Classify or abstract (Staff support youth in using classification and abstraction, linking concrete examples to principles, laws, categories, and formulas (e.g., Mice, porcupines, and squirrels are all rodents, rodents are all mammals; The pool ball moved because for every action, there is an equal and opposite reaction; etc.))",
  "Generating data", "Figuring out how or why to inscribe an observation as data and generating coding frames or measurement tools.",                                                                                                                                                                               "Collect data or measure (Staff support youth in collecting data or measuring (e.g., Youth use rulers or yardsticks to measure length; Youth count the number of different species of birds observed in a specific location, etc.))",
  NA,                                                                                                              NA, "Highlight precision and accuracy (Staff highlight value of precision and accuracy in measuring, observing, recording, or calculating (e.g., measurement error can impact an experiment or conclusion; measure twice, cut once; scientist always need to double-check their claculations before drawing conclusions; you must observe carefully to see the difference between various species of sparrows, etc.))",
  "Data modeling",  "Understanding and explaining phenomena using models of the data that account for variability or uncertainty.",                                                       "Simulate, experiment, or model (Staff support youth in using a simulation, experiment, or model to answer questions, explore solutions, or test hypotheses (e.g., Youth run a robotics program to determine whether it does what they expect it to; Youth try an alternate way to solve an equation and test their results against another example, etc.))",
  "Interpreting and communicating findings",                                                               "Discussing and sharing and presenting findings.",                                                                                                                                                                                               "Analyze (Staff support youth in analyzing data to draw conclusions (e.g., after an experiment, youth are asked to use results to make a generalization like \"Your heartbeat increases when you exercise\", etc.))"
)


d[, -3] %>%
  knitr::kable(booktabs = TRUE, caption = "Coding Frame for Work With Data") %>% 
  kableExtra::kable_styling(latex_options = "scale_down")
```

### Demographic variables used
 
In addition to the measures described in this section, demographic information for youths’ gender and their racial and ethnic group are used to construct demographic variables for gender and membership in an under-represented (in STEM) group; membership in an under-represented group are identified on the basis of youths’ racial and ethnic group being Hispanic, African American, Asian or Pacific Islanders, or native American. 

## Data Analysis

The steps for both preliminary and the primary analyses are described in this section.

### Preliminary analyses

First-order Pearson correlations and the frequency, range, mean, and standard deviations are first examined for all variables. In addition, the frequency of the codes for aspects of work with data, and the numbers of responses by youth, program, and moment are examined.

### Analysis for Research Question #1 (on the frequency and nature of work with data)

First, the frequency of the codes for the individual aspects of work with data from the PQA measure of work with data (described above), and the composite for work with data, which was simply the sum of the codes for the individual aspects of work with data, are calculated. Then, to present a qualitative description, all of the segments were coded, moments associated with specific codes were identified and used to provide examples for each of the aspects of work with data.

To code the data, three research assistants were trained for approximately eight hours over four meetings. Then, each research assistant coded all of the segments associated with one of the videos. The guiding questions for the qualitative coding were as follows:

* What are youth doing?
* When asking questions or defining problems is coded, what, if any are the questions or problems? Who is asking the question (i.e teacher or student)
* When making observations is coded, what are youth doing?
* When generating data is coded, how, if they are, are youth collecting or recording data?
* When analyzing or modeling data is coded, what analysis are they doing, or what models are they using? Are they talking about variability or uncertainty? If so, how?
* When interpreting and communicating findings is coded, what are youth interpreting or how are they communicating?
* Is there a phenomena being studied or a problem being investigated? If so, what is the phenomena being studied or problem being investigated?
* Is the activity related to something youth have previously done or learned about? If so, how?
* Is the activity related to something youth will do or will learn about? If so, how?
* Is the activity collaborative? If so, what is happening?
* Is anything else of interest or that is noteworthy going on?

After the coding was complete, the three research assistants and I met to discuss how well the coding frame and potential sources of disagreement. Then, two coders coded every segment that was coded for at least one of the aspects of work with data. This coding took around 75 hours of coding by the research assistants. After coding all of the segments associated with each program, the coders met to discuss potential issues that emerged throughout the coding, and to clarify how they applied the coding frame. 

I then read through all of the codes for all of the segments, and made notes associated with each of the five aspects of work with data (i.e., asking questions, making observations, and the other aspects). These notes focused on whether and in what ways there appeared to be alignment between the codes for work with data (from the PQA) and the conceptual framework for work with data. I used these notes to write detailed descriptions of each of the aspects of work with data. I also calculated how many time these aligned (i.e., how many times both the PQA code for an aspect of work with data was supported by the qualitative code) for each aspect of work with data.

### Analysis for Research Question #2 (what profiles of engagement emerge)

#### Background information on Latent Profile Analysis (LPA)

LPA can be used to identify common patterns in learners’ ESM responses as part of a person-oriented analysis to construct the profiles. These profiles make it possible to analyze the multivariate data collected on engagement in a way that balances the parsimony of a single model for all learners with a recognition of individual differences in how learners’ experience each of the dimensions of engagement together at the same time. A key benefit of the use of LPA, in addition to likelihood estimation-based fit indices, is probabilities of an observation being a member of a cluster (unlike in cluster analysis). LPA also provides *probabilities* of an observation being in a particular profile, unlike cluster analysis, which involves exclusively classifying an observation into one profile.

This question addresses what profiles emerged from the ESM responses for cognitive, behavioral, and affective engagement, and perceptions of challenge and competence. To answer this question, profiles are constructed with the five self-reported ESM measures for cognitive, behavioral, and affective engagement and perceptions of challenge and competence. Once this step is carried out, the probability of a response being associated with a profile of engagement and its conditions are used as the dependent variable for subsequent analyses. Answers to this question will help to understand how the aspects of engagement relate to both one another and to key conditions that influence engagement. 

To create the profiles, a mixture modeling approach is carried out. Mixture modeling is an approach for identifying distinct distributions, or mixtures of distributions, of measured variables. A type of mixture modeling within a latent variable modeling framework, Latent Profile Analysis (LPA; Harring & Hodis, 2016; Muthen, 2004) is used in this study. LPA allows for capturing the multidimensional nature of engagement and its conditions, as is the goal of the analysis for the present study.

#### Selecting a model on the basis of fit indices and other techniques

As part of LPA, different models that determine whether and how different parameters (i.e., means, variances, and covariances) are estimated. In addition, the number of profiles to estimate must be provided by the analyst. Determining the number of profiles depends on fit statistics (such as information criteria and the entropy statistic) as well as concerns of parsimony and interpretability. In general, the approach to choosing the model is similar to choosing the number of profiles, requiring deciding on the basis of evidence from multiple sources. The models are described in-depth in the appendix. The number of profiles are determined on the basis of the log-likelihood and bootstrapped likelihood ratio test, entropy, Akaike Information Criteria, and Bayesian Information Criteria statistics, as well as concerns of parsimony and interpretability. 

First, I examined a wide range of model types (i.e., the parameterization of the model, with the six options described in the appendix as candidates) and the numbers of profiles. These roughly became more complex, with additional parameters estimated, as the number for the model type increases from one to six. This step is taken to select candidate solutions to investigate in more detail. In order to carry out this analysis, I followed guidelines recommended by the developers of the MPlus software (Asparouhov & Muthen, 2012; Muthen & Muthen, 2017) as well as those making recommendations about its use (Geiser, 2012). 

To select a model for use in subsequent analyses, the log-likelihood (LL), a range of information criteria (AIC, BIC, sample adjusted BIC [SABIC], consistent AIC [CAIC]), statistics about the quality of the profile assignments (entropy, which represents the mean posterior probability), statistical tests (Vu-Lo-Mendell-Rubin LRT [VLMR], Lo-Mendell-Rubin LRT [LMR], and the bootstrapped LRT [BLRT]), and concerns of interpretability and parsimony are used. Past research suggests that BIC, CAIC, SABIC, and BLRT are most helpful for selecting the correct number of profiles (Nylund, Asparouhov, & Muthen, 2007). For the entropy statistic, higher values are considered better, though scholars have suggested that the entropy statistic not be used for model selection (Lubke & Muthen, 2007). Of the three statistical tests, the bootstrapped is considered to be the best indicator of which of two models, one nested (with certain parameters fixed to 0) within the other, fits better, but it is also the most computationally-intensive to carry out (Asparouhov & Muthen, 2012).

### Statistical software developed

The MPlus software is used to carry out LPA as part of this study. In order to more flexibly carry out LPA, an open-source tool, tidyLPA (Rosenberg, Schmidt, & Beymer, ), was developed. This tool provides interfaces to both the MPlus software and to the open-source mclust software. In addition to being used as part of this study, this package is provided free of use to other analysts as the first tool dedicated to carrying out LPA as part of the R software. More details on the statistical software developed and included in the Appendix.

### Analysis for Research Question #3 (how work with data relates to engagement)

Broadly, this question is focused on how work with data, as coded from video-recordings of the programs, relates to the profiles. For the primary results for this question, linear models that account for the cross-classification of the moment and youth are used and for the "nesting" of both within each of the nine programs are used. For the outcome (*y* variable), the probability of a response belonging to the profile is used; thus, there are six models, for each of the six profiles, for each specification of the predictor (*x*) variables.

To answer this question, on how well the aspects of work with data predict the profiles, first, indicators for activities coded for any of the five aspects of work with data and either of the other two activities are used to predict each PEC. The *lme4* R package (Bates, Martin, Bolker, & Walker, 2015) is used. All of the models for this and research question #4 use random effects for learner, momentary, and program effects. Learner and moment can be considered to be crossed with both nested within the program. Because the outcome from LPA is not a hard classification (i.e., an observation is in a profile—or not) but a probability, the outcome is treated as a continuous variable. First, null models with only the random parts (i.e., random learner, momentary, and program effects) are specified. Then, the predictors are added to the model with the main effects of the variables added. Lastly, a model with the work with data composite (constructed as a sum of the values of the variables for the aspects of work with data) is specified. 

### Analysis for Research Question #4 (how youth characteristics relate to engagement)

Research question #4 is focused on how the relationships of work with data differ on the basis of pre-program interest and other youth characteristics--their gender and URM status. Like for the previous two research questions, models that account for the cross-classification of the moment and the youth are used. Findings from models with pre interest, gender, and URM status are first carried out. Then, models with these variable and the individual aspects and composite of work with data are added and then models with the interaction between these characteristics and the composite.

## Sensitivity Analysis

For observational studies, such as the present study, it can be important to determine how robust an inference is to alternative explanations. One approach to addressing this is sensitivity analysis, which involves quantifying the amount of bias that would be needed to invalidate an inference (hypothetically, this bias might be due to omitted or confounding variables, measurement, missing data, etc.). Using the approach described in Frank, Maroulis, Duong, and Kelcey (2013), I carried out sensitivity analysis for inferences we made relative to our key findings. I used the R package konfound (Rosenberg, Xu, & Frank, 2018). The result, and what is used to interpret and contextualize findings, is a numeric value for each effect that indicates the proportion of the estimate that would have to be biased in order to invalidate the inference: higher values indicate more robust estimates in that the inferences would still hold even if there were substantial bias in the estimate. 
