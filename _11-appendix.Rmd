# Appendix

## Appendix: STEM-PQA alignment

```{r}
tibble::tribble(
  ~Work.With.Data.Codes.Originally.Proposed,                                                                                                    ~Description, ~Categories.from.STEM-PQA.(Already.Coded).With.Descriptions,
  "Asking questions or defining problems",                                            "Discussing and exploring topics to investigate and pose questions.",                       "Predict, conjecture, or hypothesize",
  "Making observations",          "Watching and noticing what is happening with respect to the phenomena or problem being investigated.",                                      "Classify or abstract",
  "Generating data", "Figuring out how or why to inscribe an observation as data and generating coding frames or measurement tools.", "Collect data or measure; Highlight precision and accuracy",
  NA,                                                                                                              NA,                                                          NA,
  "Data modeling",  "Understanding and explaining phenomena using models of the data that account for variability or uncertainty.",                            "Simulate, experiment, or model",
  "Interpreting and communicating findings",                                                               "Discussing and sharing and presenting findings.",                            "Analyze; Use symbols or models",
  NA,                                                                                                              NA,                                                          NA,
  NA,                                                                                                              NA,                                                          NA
) %>% 
  knitr::kable(booktabs = TRUE, linesep = "", caption = "Alignment of codes for instructional support for work with data and the STEM-PQA") %>% 
  ```

## Appendix: Method additional materials

### Statistical software developed

The functions in tidyLPA dynamically generate MPlus syntax, so that, for example, a user can simply provide a data frame with variables to be used in the analysis, the specification for one of six models, the number of profiles to be estimated as part of the analysis, and a number of fine-grained options concerning the estimation and the output generated. From these inputs, a data file for MPlus is prepared and saved, the model syntax is created and saved in a model input file, the model is run, and the output, including the "savedata", or the data with its associated posterior probabilities and profile assignments, is returned to R for use plots or in subsequent analyses. 

Because of the considerable time that it takes to generate MPlus model syntax (i.e., when choosing to specify a model with different parameters or when changing the number of profiles to be estimated as part of the solution), this package makes it easier to carry out LPA in a flexible way, while retaining the power of the MPlus software. While this functionality makes it considerably easier to carry out LPA, it requires that MPlus be purchased and installed. Because of this, the R package I developed also includes wrapper functions to an open-source tool, mclust (Scrucca, Fop, Murphy, & Raftery, 2016). This is a very widely-used package for mixture modeling. While some authors have suggested that it can be used to carry out LPA (Oberski, 2016), a key challenge for analysts using it concerns specifying the models. This is because the models are described in terms of the geometric properties of the multivariate distributions being estimated (i.e., "spherical, equal volume"), rather than in terms of whether and how the means, variances, and covariances are estimated. This R package corresponds LPA models to the mclust models and provides the same functionality that the functions that use MPlus provide, namely, preparing data, running the model, and returning the output or use in subsequent analyses. As part of incorporating the mclust functionality, the functions that use MPlus and those that use mclust have been benchmarked (Rosenberg, 2018). Despite leading to identical results (in most cases) for small datasets, because of differences in how the E-M algorithm is initialized as well as other estimation-related differences, output will likely not be identical for many analyses. 

### Appendix: Descriptive statistics additional materials

The Spearman rank (because the data were dichotomous) correlations among the aspects of instructional support for work with data are presented. The variables were moderately correlated, with *rho* values between .18 and .50. These suggest that signals are assocaited

```{r}
dfs <- df %>% distinct(beep_ID_new, dm_ask, dm_obs, dm_gen, dm_mod, dm_composite)

names(dfs)[2:6] <- c("Asking Questions", "Making Observations", "Generating Data", "Data Modeling", "Communicating Findings", "Composite of All Codes")
```

```{r}
dfs %>%
  select(-beep_ID_new) %>%
  corrr::correlate(method = "spearman") %>%
  fashion() %>%
  slice() %>%
  knitr::kable(booktabs = TRUE, linesep = "", caption = "Correlations among codes for instructional support for work with data (and composite of all codes)")
```

### Appendix: Program descriptions

```{r}
d <- tibble::tribble(
  ~Program.Name,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ~Program.Description,
  "Island Explorers",                                                                                                                                                                                  "A science-focused program that aims to help youth develop expertise on one species found in the local ecosystem by reading and writing about related content for up to an hour per day; undertaking data collection and analysis tasks to learn about the local ecosystem and how to communicate scientific data; developing vocabulary about the local ecosystem; using art to learn and communicate information; and publishing a book illustrating important elements of the species being studied. Located in both the classroom and local ecosystem. 27 students who are rising 6th graders. Youth spend the morning in more academically-oriented sessions in a classroom setting, while afternoon sessions involved STEM-oriented enrichment sessions taking place outside (the program was associated with Outward Bound) with an emphasis on exploration of the local ecosystem.",
  "The Ecosphere",                                                                                                                                                                                                                                                                       "A science-focused program that aims to help youth to explore the marine life of Narragansett Bay. Efforts were undertaken to build youth content knowledge in the areas of ecosystem preservation, marine biology, and water quality, and related skills, such as questioning, showing initiative, data collection, measuring, maintaining an ecosystem, and analyzing water samples. Located in a classroom setting, shoreline, and science education center. 27 youth who are rising 6th to 9th graders. Youth attended programming in a classroom at an area middle school and in a field-based setting on alternating days. Field-based settings included a science education center at a community-based organization and field trips to sites in the community related to the program's focus.",
  NA,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           NA,
  "Zoology Partners",                                                                                                                                                                                                                                                                                                                               "A science-focused program that aims to support youth's development of content knowledge related to the issue of endangered species, including how species become endangered, processes for monitoring ecosystem viability and population levels, solutions to prevent species from becoming endangered, and approaches to reviving populations that are currently endangered. Located in the classroom as well as zoos, parks, and other natural areas. 25 youth who are rising 6th to 9th graders. Youth attended programming in a classroom at an area middle school and in a field-based setting on alternating days. Field-based settings included a local zoo and field trips to sites in the community related to the program's focus.",
  "Marine Investigators",  "A science-focused program that aims to provide youth with opportunities to learn about and experience Narragansett Bay; examine human impacts on the local ecosystem, including how the geography of the Bay helped influence human history and how the history of humans along the shoreline has impacted the Bay, and begin the process of cultivating a sense of stewardship among participating youth for caring for and protecting the Bay in the future. Located in the classroom, shoreline along the bay, ship on the bay, and various field locations associated with bay health. 19 youth who are rising 7th to 9th graders. Youth attended programming in a classroom at an area middle school and in a field-based setting on alternating days. Field-based settings included the local bay shoreline, a voyage on a marine education ship researching in the Bay, and field trips to sites in the community related to the program's focus. During the span of the program, youth had the opportunity to participate in both a water quality research study.",
  "Comunidad de Aprendizaje",                                                                                                                                                                                                                                                                                                                                                                                                        "A STEM-focused program that aims to help youth improve basic skills in mathematics and develop an interest in STEM content and entrepreneurship. Primarily in the classroom setting. 33 students who are rising 5th to 8th graders. Morning sessions are characterized by direct instruction in mathematics for individual grade levels and mixed grade level afternoon enrichment sessions in either robotics or dance. The direct instruction component of the programs was organized around a theme of promoting entrepreneurship with the goal of helping participating youth better see the relevance of mathematics to future career goals and opportunities.",
  "Jefferson House",                                                                                                                                                                                                                                                                                                                             "A STEM-focused program that aims to support youth's development of basic math skills, the program was primarily focused on helping youth develop problem solving, self-improvement, and critical thinking skills. Located in a classroom. 11 youth who are rising 7th graders. The youth spent the morning in more academically-oriented sessions in a classroom setting focusing on basic skill development, while afternoon sessions involved STEM-oriented enrichment sessions involving media, art, and nutrition. Enrichment offerings varied by day, with math sessions occurring twice per week, alternating with academically oriented sessions in the am that were oriented at supporting skill development in English/language arts.",
  NA,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           NA,
  "Uptown Architecture",                                                                                                                                                                                                                                                                                  "An engineering-focused program that aims to support youth's participation in a process to design and build an outdoor learning space for use at the middle school where the program was housed. A key focus of the program was to provide youth with the opportunity to use design thinking as a problem-solving tool and have the experience of affecting their community positively through the design/build process. Located in a classroom, building shop, and various field locations. 18 youth who were rising 6th to 9th graders. Youth attended programming in a classroom at an area middle school and in a building shop located at a community-based organization on alternating days, while also taking field trips to locations associated with the program's overall theme.",
  "Building Mania",                                                                                                                                                                                                                                                                         "An engineering-focused program that aims to provide youth with the opportunity to experiment with designing and using simple machines. A goal of the program is to have youth engage in the engineering design process by determining a need, brainstorming possible designs, selecting a design, planning and drawing out the design, creating and testing and revising it, and producing a final machine. Located in the classroom, design labs, and other local locations. 24 youth who are rising 6th to 9th graders. Youth attended programming in a classroom at an area middle school and a field-based setting on alternating days. Field-based settings included a design lab at a community-based organization and field trips to sites in the community related to the program's focus.",
  "Adventures in Mathematics",                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             "A mathematics-focused program that aims to help youth to develop the basic math skills and prevent summer learning loss among participating youth through direct instruction and participation in math-related games. Located primarily in the classroom. 20 youth who are rising 8th to 10th graders. Youth participated in direct instructions in mathematics and math-related games in small groups. Program content was aligned with the state's standards in mathematics."
)

d %>% 
  filter(!is.na(Program.Name)) %>% 
  knitr::kable(booktabs = TRUE, caption = "STEM Enrichment Program Names and Their Descriptions") %>% 
  landscape()
```

### Appendix: Research Question #1 additional materials

### Model specifications details

Here, the six models that are possible to specify in LPA are described in terms of how the variables used to create the profiles are estimated. Note that *p* represents different profiles and each parameterization is represented by a 4 x 4 covariance matrix and therefore would represent the parameterization for a four-profile solution. In all of the models, the means are estimated freely in the different profiles. Imagine that each row and column represents a different variable, i.e., the first row (and column) represents broad interest, the second enjoyment, the third self-efficacy, and the fourth another variable, i.e., future goals and plans. Models 1 and 3 meet the assumption of independence, that is, that, after accounting for their relations with the profile, the variables used to estimate the profiles are independent (Collins & Lanza, 2010). They estimate variable variances but do not estimate covariances (i.e., as can be seen, the covariance matrices are "diagonal," without any off-diagonal parameters that are estimated). These models are estimated by default in MPlus, although these assumptions can be relaxed (Muthen & Muthen, 2017). Importantly, this does not mean the variables used to create the profile are assumed to be not related; as Collins and Lanza (2010) explain:

> The local independence assumption refers only to conditioning on the latent variable. It does not imply that in a data set that is to be analyzed, the observed variables are independent. In fact, it is the relations among the observed variables that are explained by the latent classes. An observed data set is a mixture of all the latent classes. Independence is assumed to hold only within each latent class, which is why it is called "local".

Despite the assumption of independence, as Collins and Lanza (2010), Muthen and Muthen (2017), and others (i.e., Pastor et al., 2007; Vermunt & Magidson, 2002) note, it can be lifted to improve model fit, though these models without the assumption of independence may be better described as general or Gaussian mixture models (Fraley et al., 2017). 

#### Varying means, equal variances, and covariances fixed to 0 (model 1)

In this model, which corresponds to the mclust model wit the name "EEI", the variances are estimated to be equal across profiles, indicated by the absence of a p subscript for any of the diagonal elements of the matrix. The covariances are constrained to be zero, as indicated by the 0's between every combination of the variables. Thus, this model is highly constrained but also parsimonious: the profiles are estimated in such a way that the variables' variances are identical for each of the profiles, and the relationships between the variables are not estimated. In this way, less degrees of freedom are taken used to explain the observations that make up the data. However, estimating more parameters--as in the other models--may better explain the data, justifying the addition in complexity that their addition involves (and their reduction in degrees of freedom).

$$
\left[ \begin{matrix} { \sigma  }_{ 1 }^{ 2 } & 0 & 0 & 0 \\ 0 & { \sigma  }_{ 2 }^{ 2 } & 0 & 0 \\ 0 & 0 & { \sigma  }_{ 3 }^{ 2 } & 0 \\ 0 & 0 & 0 & { \sigma  }_{ 4 }^{ 2 } \end{matrix} \right] 
$$

#### Varying means, equal variances, and equal covariances (model 2)

This model corresponds to the mclust model "EEE". In this model, the variances are still constrained to be the same across the profiles, although now the covariances are estimated (but like the variances, are constrained to be the same across profiles). Thus, this model is the first to estimate the covariance (or correlations) of the variables used to create the profiles, thus adding more information that can be used to better understand the characteristics of the profiles (and, potentially, better explain the data).

$$
\left[ \begin{matrix} { \sigma  }_{ 1 }^{ 2 } & { \sigma  }_{ 21 } & { \sigma  }_{ 31 } & { \sigma  }_{ 41 } \\ { \sigma  }_{ 12 } & { \sigma  }_{ 2 }^{ 2 } & { \sigma  }_{ 23 } & { \sigma  }_{ 24 } \\ { \sigma  }_{ 13 } & { \sigma  }_{ 12 } & { \sigma  }_{ 3 }^{ 2 } & { \sigma  }_{ 33 } \\ { \sigma  }_{ 14 } & { \sigma  }_{ 12 } & { \sigma  }_{ 12 } & { \sigma  }_{ 4 }^{ 2 } \end{matrix} \right] 
$$

#### Varying means, varying variances, and covariances fixed to 0 (model 3)

This model corresponds to the mclust model "VVI" and allows for the variances to be freely estimated across profiles. The covariances are constrained to zero. Thus, it is more flexible (and less parsimonious) than model 1, but in terms of the covariances, is more constrained than model 2.

$$ 
\left[ \begin{matrix} { \sigma  }_{ 1p }^{ 2 } & 0 & 0 & 0 \\ 0 & { \sigma  }_{ 2p }^{ 2 } & 0 & 0 \\ 0 & 0 & { \sigma  }_{ 3p }^{ 2 } & 0 \\ 0 & 0 & 0 & { \sigma  }_{ 4p }^{ 2 } \end{matrix} \right] 
$$

#### Varying means, varying variances, and equal covariances (model 4)

This model, which specifies for the variances to be freely estimated across the profiles and for the covariances to be estimated to be equal across profiles, extends model 3. Unfortunately, this model cannot be specified with mclust, though it can be with MPlus; this model *can* be used with the functions to interface to MPlus described below.

$$
\left[ \begin{matrix} { \sigma  }_{ 1p }^{ 2 } & { \sigma  }_{ 21 } & { \sigma  }_{ 31 } & { \sigma  }_{ 41 } \\ { \sigma  }_{ 12 } & { \sigma  }_{ 2p }^{ 2 } & { \sigma  }_{ 23 } & { \sigma  }_{ 24 } \\ { \sigma  }_{ 13 } & { \sigma  }_{ 12 } & { \sigma  }_{ 3p }^{ 2 } & { \sigma  }_{ 33 } \\ { \sigma  }_{ 14 } & { \sigma  }_{ 12 } & { \sigma  }_{ 12 } & { \sigma  }_{ 4p }^{ 2 } \end{matrix} \right] 
$$

#### Varying means, equal variances, and varying covariances (model 5)

This model specifies the variances to be equal across the profiles, but allows the covariances to be freely estimated across the profiles. Like model 4, this model cannot be specified with mclust, though it can be with MPlus. Again, this model *can* be used with the functions to interface to MPlus described below.

$$
\left[ \begin{matrix} { \sigma  }_{ 1 }^{ 2 } & { \sigma  }_{ 21p } & { \sigma  }_{ 31p } & { \sigma  }_{ 41p } \\ { \sigma  }_{ 12p } & { \sigma  }_{ 2 }^{ 2 } & { \sigma  }_{ 23p } & { \sigma  }_{ 24p } \\ { \sigma  }_{ 13p } & { \sigma  }_{ 12p } & { \sigma  }_{ 3 }^{ 2 } & { \sigma  }_{ 33p } \\ { \sigma  }_{ 14p } & { \sigma  }_{ 12p } & { \sigma  }_{ 12p } & { \sigma  }_{ 4 }^{ 2 } \end{matrix} \right] \quad 
$$

#### Varying means, varying variances, and varying covariances (model 6)

This model corresponds to the mclust model "VVV". It allows the variances and the covariances to be freely estimated across profiles. Thus, it is the most complex model, with the potential to allow for understanding many aspects of the variables that are used to estimate the profiles and how they are related. However, it is less parsimonious than all of the other models, and the added parameters should be considered in light of how preferred this model is relative to those with more simple specifications. 

$$
\left[ \begin{matrix} { \sigma  }_{ 1p }^{ 2 } & { \sigma  }_{ 21p } & { \sigma  }_{ 31p } & { \sigma  }_{ 41p } \\ { \sigma  }_{ 12p } & { \sigma  }_{ 2p }^{ 2 } & { \sigma  }_{ 23p } & { \sigma  }_{ 24p } \\ { \sigma  }_{ 13p } & { \sigma  }_{ 12p } & { \sigma  }_{ 3p }^{ 2 } & { \sigma  }_{ 33p } \\ { \sigma  }_{ 14p } & { \sigma  }_{ 12p } & { \sigma  }_{ 12p } & { \sigma  }_{ 4p }^{ 2 } \end{matrix} \right] 
$$

### Model 1 candidate solutions

#### Model: 1, Profiles: 3

This solution is characterized by: 

- a **full** profile, profile 2 (though with more modestly high levels of challenge) 
- a **universally low** profile, profile 1 (again with more modestly - in this case low - levels of challenge)
- an **all moderate** profile, profile 3, characterized by levels of all of the variables close to the mean, profile 3

The number of observations associated with each of the profiles is somewhat balanced, with the all moderate profile demonstrating a higher number of observations (*n* = 1,288) than the full (*n* = 897) and universally low (*n* = 773) profiles. The log-likelihood was replicated many (more than 10) times. Because the profiles associated with this solution all demonstrated the same overall pattern (i.e., all five variables are high, low, or moderate), on the basis of interpretability, this particular solution may not be useful in terms of understanding how youth experience engagement and its conditions. 

<!-- While replicated, the lowest log-likelihood associated with this solution demonstrated a very large decrease in the log-likelihood relative to those associated with the 2nd, 3rd, and 4th lowest log-likelihood solutions. This could suggest that the model is under-identified (Asparouhov & Muthen, 2012). -->

```{r, spec-solutions-m1_3, cache = FALSE, eval = FALSE}
m1_3 <- estimate_profiles_mplus(df,  
                                dm_cog_eng, dm_beh_eng, dm_aff_eng, dm_challenge, dm_competence,
                                starts = c(600, 120),
                                model = 1,
                                n_profiles = 3,
                                include_BLRT=TRUE,
                                n_processors = 6, remove_tmp_files = FALSE)

write_rds(m1_3, "data/models/m1_3.rds")
```

```{r, m1_3p, cache = FALSE, eval = TRUE}
m1_3 <- read_rds("data/models/m1_3.rds")
plot_profiles_mplus(m1_3, to_scale = TRUE)
```

```{r, m1_3p-ll, cache = FALSE, eval = FALSE}
extract_LL_mplus() %>% slice(1:10) %>% knitr::kable(format = "latex")
```

#### Model: 1, Profiles: 4

This solution is characterized by: 

- a **full** profile, profile 2
- a **universally low** profile, profile 1
- an **all moderate** profile, profile 3. 
- a **competent but not engaged or challenged** profile, with high levels of competence and low levels of engagement and challenge

Most profiles are in the all moderate profile (*n* = 1,288), with a large number in the full (*n* = 920) profile, and fewer in the universally low and competent (*n* n = 427) but not engaged or challenged profiles (*n* = 415). With somewhat more purchase in terms of its interpretability than the solution for model 1 with three profiles, like that solution, this one may not be as useful as more complex models for understanding youth's experiences.

The log-likelihood was replicated many (more than 10) times. 

```{r, spec-solutions-m1_4, cache = FALSE, eval = FALSE}
m1_4 <- estimate_profiles_mplus(df,  
                                dm_cog_eng, dm_beh_eng, dm_aff_eng, dm_challenge, dm_competence,
                                starts = c(600, 120),
                                model = 1,
                                n_profiles = 4,
                                include_BLRT=TRUE,
                                n_processors = 6, remove_tmp_files = FALSE)
write_rds(m1_4, "data/models/m1_4.rds")
```

```{r, m1_4p, cache = FALSE, eval = TRUE}
m1_4 <- read_rds("data/models/m1_4.rds")
plot_profiles_mplus(m1_4, to_scale = TRUE)
```

```{r, m1_4p-ll, cache = FALSE, eval = FALSE}
extract_LL_mplus() %>% slice(1:10) %>% knitr::kable()
```

#### Model: 1, Profiles: 5

This solution is characterized by: 

- a **full** profile, profile 5
- a **universally low** profile, profile 3
- an **all moderate** profile, profile 3, though with moderate levels of affective engagement than in similar profiles associated with the four and five profile solutions, perhaps suggesting that a different profile than in those solutions
- an **only behavioral** profile, profile 2, with moderate levels of behavioral engagement, very low affective engagement, and moderately (low) levels of cognitive engagement and challenge and competence
- an **only affective** profile, profile 4, with moderate levels of affective engagement, low levels of behavioral engagement, and moderately (low) levels of cognitive engagement and challenge and competence

The number of observations associated with each of the profiles is somewhat balanced, with a large number in the full profile (*n* = 928), a moderate number of observations in the universally low (*n* = 667) and all moderate (*n* = 643) profiles, and fewer observations in the only behaviorally engaged (*n* = 375) and only affective engaged (*n* = 345)  profiles. This solution primarily distinguishes between affective and behavioral engagement; unlike the solution for model 1 with four profiles, there is not a competent but not engaged or challenged profile. This may suggest that solutions with a greater number of profiles represents both the distinction between behavioral and affective engagement highlighted by profiles in this solution as well as profiles that are characterized by higher or lower levels of the conditions for engagement (i.e., competence). The log-likelihood was replicated four times. 

```{r, spec-solutions-m1_5, cache = FALSE, eval = FALSE}
m1_5 <- estimate_profiles_mplus(df,  
                                dm_cog_eng, dm_beh_eng, dm_aff_eng, dm_challenge, dm_competence,
                                starts = c(600, 120),
                                model = 1,
                                n_profiles = 5,
                                include_BLRT=TRUE,
                                n_processors = 6, remove_tmp_files = FALSE)
write_rds(m1_5, "data/models/m1_5.rds")
```

```{r, m1_5p, cache = FALSE, eval = TRUE}
m1_5 <- read_rds("data/models/m1_5.rds")
plot_profiles_mplus(m1_5, to_scale = TRUE)
```

```{r, m1_5p-ll, cache = FALSE, eval = FALSE}
extract_LL_mplus() %>% slice(1:10) %>% knitr::kable(format = "latex")
```

#### Model: 1, Profiles: 6 (alternate)

This solution is characterized by: 

- a **full** profile, profile 6
- a **universally low** profile, profile 1
- an **engaged and competent but not challenged** profile, profile 3
- a **challenged** profile, profile 2
- a **highly challenged** profile, profile 3
- a **moderately low** profile, profile 5

The number of observations are not very balanced, with the moderately low profile with a large number of observations (*n* = 852) and the challenged, engaged and competent but not challenged, and full profiles with moderate numbers of observations (from 464 to 619 observations), and low numbers of observations exhibited by universally low (*n* = 280) and 
highly challenged (*n* = 158) profiles. This--and, critically, the lower log-likelihood of the other model 1, six profile solution--suggests that this solution is not preferred. However, the very different profiles that emerge for this solution suggest that there might not be a somewhat under-identified solution associated with model 1 and six profiles. 

```{r, spec-solutions-m1_6-alt, cache = FALSE, eval = FALSE}
m1_6_alt <- estimate_profiles_mplus(df,  
                                    dm_cog_eng, dm_beh_eng, dm_aff_eng, dm_challenge, dm_competence,
                                    starts = c(600, 120),
                                    model = 1,
                                    n_profiles = 6,
                                    include_BLRT=TRUE,
                                    n_processors = 6, remove_tmp_files = FALSE,
                                    optseed = 49221)
write_rds(m1_6_alt, "data/models/m1_6_alt.rds")
```

```{r, m1_6p-alt, cache = FALSE, eval = TRUE}
m1_6_alt <- read_rds("data/models/m1_6_alt.rds")
plot_profiles_mplus(m1_6_alt, to_scale = TRUE)
```

```{r, m1_6p-ll-alt, eval = FALSE, cache = FALSE}
extract_LL_mplus() %>% slice(1:10) %>% knitr::kable()
```

#### Model: 1, Profiles: 7 (alternate)

When investigating an alternate solution (associated with the second lowest log-likelihood) for the model 1, seven profile solution, we can see that even for the solutions associated with other log-likelihoods, the profiles that can be identified are very similar. One minor distinction concerns the **competent but not engaged or challenged** profile, which in the alternate solution is associated with neutral levels of affective engagement, compared to moderately low levels of affective engagement in the solution with the lowest log-likelihood. Because five of the seven profiles associated with both of these model 1, seven profile solutions seem to be distinct from those identified from simpler model 1 solutions, investigation of this alternate solution provides additional evidence that these profiles are not associated with an under-identified model and that simpler models may be preferred over these seven profile solutions.

```{r, spec-solutions-m1_7-other-LL, cache = FALSE, eval = FALSE}
m1_7_alt <- estimate_profiles_mplus(df,  
                                    dm_cog_eng, dm_beh_eng, dm_aff_eng, dm_challenge, dm_competence,
                                    starts = c(600, 120),
                                    model = 1,
                                    n_profiles = 7,
                                    include_BLRT=TRUE,
                                    n_processors = 6, remove_tmp_files = FALSE, optseed = 597614)
write_rds(m1_7_alt, "data/models/m1_7_alt.rds")
```

```{r, m1_7-other-LL-p, eval = TRUE, cache = FALSE}
m1_7_alt <- read_rds("data/models/m1_7_alt.rds")
plot_profiles_mplus(m1_7_alt, to_scale = TRUE)
```

```{r, m1_7-other-LL-ll, cache = FALSE, eval = FALSE}
extract_LL_mplus() %>% slice(1:10) %>% knitr::kable()
```

### Model 2 candidate solutions

#### Model: 2, Profiles: 3

This solution is characterized by: 

- a **universally low** profile, profile 1, associated with moderate (low) and low levels of all of the variables; this profile is similar to the universally low profile identified as part of other solutions, although with more moderate values for some of the variables (especially cognitive engagement)
- a **competent but not challenged** profile, profile 2, characterized by high competence and low challenge
- a **challenged** profile, profile 3, characterized by very high challenge and moderate (high) levels of the other variables, similar to the challenged profile found as part of the model 1, four profile solution, but with higher levels of competence, which are moderately high in this solution but moderately low for the other solution.

The number of observations associated with each solution is fairly balanced, with the most in the challenged profile (*n* = 1,241), followed by the universally low (*n* = 954 observations) and competent but not challenged (*n* = 763) profiles. This solution is very different than the three profile solution that was interpreted for model 1. Model 2 differs from model 1 in that covariances between the variables are estimated (they are constrained to be the same are across the profiles). The log-likelihood was replicated (at least) ten times. Thus, this and other solutions associated with model 2 include information about how the variables relate. Including this information seems to be associated with profiles that differentiate the groups on the basis of the levels of each of the variables in more distinct ways: the model 1, three profile solution was characterized by high, moderate, or low levels of all variables for each of the three profiles.

```{r, spec-solutions-for-model2, cache = FALSE, eval = FALSE}
m2_3 <- estimate_profiles_mplus(df,  
                                dm_cog_eng, dm_beh_eng, dm_aff_eng, dm_challenge, dm_competence,
                                starts = c(600, 120),
                                model = 2,
                                n_profiles = 3,
                                include_BLRT=TRUE,
                                n_processors = 6, remove_tmp_files = FALSE)
write_rds(m2_3, "data/models/m2_3.rds")
```

```{r, m2_3p, eval = TRUE, cache = FALSE}
m2_3 <- read_rds("data/models/m2_3.rds")
plot_profiles_mplus(m2_3, to_scale = TRUE)
```

```{r, m2_3p-ll, cache = FALSE, eval = FALSE}
extract_LL_mplus() %>% slice(1:10) %>% knitr::kable()
```

#### Model: 2, Profiles: 4

This solution is characterized by: 

- a **universally low** profile, profile 1
- a **challenged** profile, profile 2
- a **highly challenged** profile, profile 4
- an **engaged and competent but not challenged** profile, profile 3

The number of observations in each of the profiles is not very balanced, with more than 1,000 observations in both the universally low (*n* = 1,029) and challenged (*n* = 1,106) profiles, a moderate number if the engaged and competent but not challenged profile (*n* = 688), and very few in the highly challenged (*n* = 135) profile. The log-likelihood was replicated three times. While each of these profiles has been identified in another solution, the small number of observations in the highly challenged profile suggests that this solution be interpreted with some skepticism because of the potentially limited utility (and statistical power associated with the use) of the profiles in subsequent analyses.

```{r, spec-solutions-model2-4, cache = FALSE, eval = FALSE}
m2_4 <- estimate_profiles_mplus(df,  
                                dm_cog_eng, dm_beh_eng, dm_aff_eng, dm_challenge, dm_competence,
                                starts = c(600, 120),
                                model = 2,
                                n_profiles = 4,
                                include_BLRT=TRUE,
                                n_processors = 6, remove_tmp_files = FALSE)
write_rds(m2_4, "data/models/m2_4.rds")
```

```{r, m2_4p, eval = TRUE, cache = FALSE}
m2_4 <- read_rds("data/models/m2_4.rds")
plot_profiles_mplus(m2_4, to_scale = TRUE)
```

```{r, m2_4p-ll, cache = FALSE, eval = FALSE}
extract_LL_mplus() %>% slice(1:10) %>% knitr::kable()
```

#### Model: 2, Profiles: 5

This solution is characterized by: 

- a **universally low** profile, profile 1
- a **full** profile, profile 4, although with very high levels of challenged (in addition to high levels of all of the other variables), making this profile similar to that (challenged) profile
- a **highly challenged** profile, profile 5
- an **all moderate** profile, profile 3, although with moderately lower levels of competence than is found in profiles associated with other solutions
- a **competent but not challenged** profile, profile 2, similar to the competent but not challenged or engaged profile, but with neutral, rather than low, levels of the engagement variables

The number of observations associated with each of the profiles is not very balanced, with a very large number of observations in the all moderate profile (*n* = 1,113) and a large number in the competent but not challenged profile (*n* = 871), a moderate number in the full profile (*n* = 573), and very few in the universally low (*n* = 271) and challenged but not competent (*n* = 130) profiles. The log-likelihood was replicated four times. Like for the model 2, four profile solution, the small number of observations associated with two of the profiles suggests that this solution should be interpreted with some caution. 

```{r, spec-solutions-model2-5, cache = FALSE, eval = FALSE}
m2_5 <- estimate_profiles_mplus(df,  
                                dm_cog_eng, dm_beh_eng, dm_aff_eng, dm_challenge, dm_competence,
                                starts = c(600, 120),
                                model = 2,
                                n_profiles = 5,
                                include_BLRT=TRUE,
                                n_processors = 6, remove_tmp_files = FALSE)
write_rds(m2_5, "data/models/m2_5.rds")
```

```{r, m2_5p, eval = TRUE, cache = FALSE}
m2_5 <- read_rds("data/models/m2_5.rds")
plot_profiles_mplus(m2_5, to_scale = TRUE)
```

```{r, m2_5p-ll, cache = FALSE, eval = FALSE}
extract_LL_mplus() %>% slice(1:10) %>% knitr::kable()
```


### Exploration of a wide range of models

```{r, compare-solutions-overall-stats, eval = FALSE}
d <- compare_solutions_mplus(df,
                             dm_cog_eng, dm_beh_eng, dm_aff_eng, dm_challenge, dm_competence,
                             starts = c(600, 120),
                             n_profiles_min = 2,
                             n_profiles_max = 10,
                             return_stats_df = TRUE,
                             return_table = TRUE,
                             n_processors = 6,
                             save_models = TRUE,
                             include_BLRT = TRUE)

write_rds(d, "data/overall-stats-for-all-models.rds")
```

```{r, printing-solutions-overall-stats, eval = F}
# d <- read_rds("data/overall-stats-for-all-models.rds")
overall_stats_for_all_models[[1]] %>%
  knitr::kable(format = "latex", booktabs = TRUE, caption = "Overall statistics for all models", linesep = "") %>%
  kableExtra::kable_styling(latex_options = "scale_down") %>%
  kableExtra::landscape()
```

### In-depth statistics for particular models


```{r, reading-overall-stats}
overall_stats_for_all_models <- readr::read_rds("data/overall-stats-for-all-models.rds")
```

```{r, printing-solutions-spec-stats, eval = TRUE}
overall_stats_for_all_models[[2]] %>%
  arrange(model, n_profile) %>%
  select(-model, `Number of Profiles` = n_profile) %>%
  mutate(VLMR = paste_stats(VLMR_val, VLMR_p),
         LMR = paste_stats(LMR_val, LMR_p),
         BLRT = paste_stats(BLRT_val, BLRT_p)) %>%
  select(everything(), -VLMR_val, -VLMR_p, -LMR_val, -LMR_p, -BLRT_val, -BLRT_p) %>%
  knitr::kable(format = "latex", caption = "Solutions for models that converged with replicated LL", booktabs = TRUE, linesep = "") %>%
  kableExtra::kable_styling(latex_options = "scale_down") %>%
  kableExtra::landscape() %>%
  kableExtra::group_rows("Model 1", 1, 7) %>%
  kableExtra::group_rows("Model 2", 8, 11)
```

```{r, model1, eval = T, cache = F, out.width = "50%", fig.cap = "Fit statistics for model 1 solutions"}
overall_stats_for_all_models[[2]] %>%
  select(n_profile:CAIC, Entropy) %>%
  filter(model == 1) %>%
  mutate(AIC = AIC * -1,
         LL = LL * -1) %>%
  gather(key, val, -n_profile, -model) %>%
  ggplot(aes(x = n_profile, y = val)) +
  geom_point() +
  geom_line() +
  facet_grid(key ~ model, scales = "free") +
  theme_bw() +
  xlab("Number of Profiles") +
  ylab(NULL)
```

```{r, model2, eval = T, cache = F, out.width = "40%", fig.cap = "Fit statistics for model 2 solutions"}

overall_stats_for_all_models[[2]] %>%
  select(n_profile:CAIC, Entropy) %>%
  filter(model == 2) %>%
  mutate(AIC = AIC * -1,
         LL = LL * -1) %>%
  gather(key, val, -n_profile, -model) %>%
  ggplot(aes(x = n_profile, y = val)) +
  geom_point() +
  geom_line() +
  facet_grid(key ~ model, scales = "free") +
  theme_bw() +
  xlab("Number of Profiles") +
  ylab(NULL)
```

Looking across the statistics presented, some general ideas about which models are to be preferred emerge. Solutions are interpreted first for each model individually and then across models with the goal of choosing a smaller number of models to investigate in more detail.

For solutions associated with model 1, the decrease (indicating a preferred model) in information criteria becomes smaller as the number of profiles increases from 5 to 6 and 6 to 7. A solution associated with 8 profiles did not replicate the log-likelihood and the VLMR and LMR suggest that the solution associated with 9 profiles did not fit better than that with 8 profiles, suggesting that models with 7 or fewer profiles be preferred. Considering these models, the entropy statistic increases by a large amount between the solution associated with 4 and 5 profiles (and then decreases slightly between 5 and 6 and 6 and 7 profile solutions), suggesting (but not providing conclusive evidence) that models 5, 6, or 7 may be preferred. The bootstrapped LRT suggests that, until the log-likelihood is not replicated, every more complex model be selected. Taking these pieces of evidence into conclusion, for model 1, solutions associated with 4 through 7 may be considered in more depth, with an emphasis on solutions associated with profiles with 5 and 6 profiles on the basis of the slowing of the decrease in the information criteria associated with the solutions with greater profiles than these, and the increase in the entropy from 4 to 5 (and 6) profile solutions.

For solutions associated with model 2, only those associated with 2-5 profile solutions were associated with log-likelihoods that were replicated. For these four models, the log-likelihood decreased in a mostly consistent way, such that changes in the decrease are not as evident as those associated with model 1. The entropy statistic decreases from 2 to 3 profile solutions, increases from 3 to 4 profile solutions, and then decreases slightly from 4 to 5 profile solutions, providing some information that models associated with 4 profiles be preferred to the others. All of the LRTs suggest that the more complex model be selected, not providing clear information about which solutions are to be preferred. On the basis of these pieces of evidence, models with 3, 4, and 5 solutions may be considered in more depth. However, there is a lack of consistent evidence favoring more or less complex models.

### Comparison of model 1 and model 2 type solutions

When looking across solutions, some overall patterns in terms of what profiles emerge and some directions for which models are to be selected for use in subsequent analysis can be identified. First, overall patterns are discussed. In the table, which profiles emerge from which solution is presented.

There is a wide range of profiles. Some appear very commonly, particularly those (full and universally low) characterized by high or low levels across all of the variables. Moderate profiles, both all moderate (characterized by moderately high levels across all of the variables) and moderately low (characterized by low levels across all of the variables), also appeared commonly, particularly for the solutions for model 1.

```{r, compare-profiles-by-solution}
d <- readxl::read_xlsx("tables/prof-assignments.xlsx")

d[, -1] %>%
  knitr::kable(format = "latex", booktabs = TRUE, caption = "Profile assignments by LPA solution (models and numbers of profiles)", linesep = "", align=rep('c', 13)) %>%
  kableExtra::kable_styling(latex_options = "scale_down") %>%
  kableExtra::landscape() %>%
  kableExtra::row_spec(0, angle = -60) %>%
  kableExtra::group_rows("Model 1", 1, 6) %>%
  kableExtra::group_rows("Model 2", 7, 9)
```


#### Model type: 1, Profiles: 7

This solution is characterized by:

- A *full* profile, profile 7
- A *universally low* profile, profile 1
- A *competent but not engaged or challenged* profile, profile 2, characterized by high competence and moderate (low) or low levels of engagement and challenge
- A *moderately low* profile, profile 3, characterized by moderately low levels of all of the variables
- A *challenged* profile, profile 4, characterized by high challenge, moderate (high) levels of engagement, and moderate (low) levels of competence
- A *highly challenged* profile, profile 5, characterized by patterns similar to those of the challenged profile, but with higher challenge and with low levels of both engagement and challenge
- A *challenged but not engaged or competent* profile, profile 6, characterized by low levels of challenge, and high levels of engagement and competence

The number of observations associated with each of the profiles is not very balanced, with few (*n* = 181) observations associated with the universally low profile and few (*n* = 222) observations associated with the highly challenged profile. The number of observations associated with the other profiles ranged from 317 to 651. Distinct from other solutions, none of the other five profiles were found in the other model 1 solutions. Two pairs of the profiles--challenged and highly challenged and universally low and moderately low--exhibited similar patterns among the variables that were distinguished by different mean levels. The log-likelihood was replicated twice, with the next lowest log-likelihood being replicate four times, possibly warranting further investigation. Taken together, this solution raises questions about whether it may be too complex, possibly suggesting preference for model 1 five and six profile solutions.

```{r, spec-solutions-m1_7, cache = FALSE, eval = FALSE}
m1_7 <- estimate_profiles_mplus(df,
                                dm_cog_eng, dm_beh_eng, dm_aff_eng, dm_challenge, dm_competence,
                                starts = c(600, 120),
                                model = 1,
                                n_profiles = 7,
                                include_BLRT=TRUE,
                                n_processors = 6, remove_tmp_files = FALSE)
write_rds(m1_7, "data/models/m1_7.rds")
```

```{r, m1_7p, cache = FALSE, eval = TRUE,  eval = FALSE, fig.width = 7, fig.asp = .618, out.width = "90%", fig.align = "center"}
m1_7 <- readr::read_rds("data/models/m1_7.rds")

m1_7 %>%
  tidyLPA::plot_profiles_mplus(to_center = TRUE, to_scale = TRUE) +
  scale_x_discrete(labels = c("Universally low (n = 181)",
                              "Competent but not engaged or challenged (n = 317)",
                              "Moderately low (n = 651)",
                              "Challenged (n = 569)",
                              "Highly challenged (n = 222)",
                              "Engaged and competent but not challenged (n = 568)",
                              "Full (n = 450)")) +
  xlab(NULL) +
  ylab("Z-score") +
  viridis::scale_fill_viridis("",
                              limits = c("DM_AFF_E", "DM_BEH_E", "DM_COG_E", "DM_CHALL", "DM_COMPE"),
                              labels = c("Affective", "Behavioral", "Cognitive", "Challenge", "Competence"), discrete = TRUE) +
  theme(plot.margin = margin(1, 0, 0, 1, "cm"))

m1_7 <- readr::read_rds("data/models/m1_7.rds")

m1_7 %>%
  tidyLPA::plot_profiles_mplus(to_center = FALSE, to_scale = FALSE) +
  scale_x_discrete(labels = c("Universally low (n = 181)",
                              "Competent but not engaged or challenged (n = 317)",
                              "Moderately low (n = 651)",
                              "Challenged (n = 569)",
                              "Highly challenged (n = 222)",
                              "Engaged and competent but not challenged (n = 568)",
                              "Full (n = 450)")) +
  xlab(NULL) +
  ylab("Value") +
  viridis::scale_fill_viridis("",
                              limits = c("DM_AFF_E", "DM_BEH_E", "DM_COG_E", "DM_CHALL", "DM_COMPE"),
                              labels = c("Affective", "Behavioral", "Cognitive", "Challenge", "Competence"), discrete = TRUE) +
  theme(plot.margin = margin(1, 0, 0, 1, "cm"))
```

```{r, m1_7-ll, cache = FALSE, eval = FALSE}
extract_LL_mplus() %>% slice(1:10) %>% knitr::kable()
```

##### Looking across model 1 and model 2 type solutions

The model 1, six and seven profile solutions are compelling because both show profiles that are distinguished by dimensions of engagement and its conditions (challenge and competence). Note that for this model, only the means and variances are estimated (and so no covariances are estimated), and the variances are constrained to be the same across the profiles. While this is a very restrictive model, it, along with the model 3 type (which did not lead to solutions for any of the numbers of profiles specified) also is a standard model for LPA, in that it meets the assumption of local independence (of the variables that make up the profiles--unlike for models in which covariances are estimated) typical common to LPA (see Muthen & Muthen, 2016). While some of the solutions associated with the model 2 type did reach solutions, these demonstrated less appealing properties in terms of their fit statistics as well as their interpretability and with respect to concerns of parsimony. Thus, while no covariances are estimated for the model 1 type solutions, there is no requirement that these be specified; their benefit, when models associated with them are preferred, is that they can provide better fit: they can be used to better explain or predict the data in a sample, but their inclusion also means that over-fitting the model to the data can become a greater concern.

For each solution, alternate solutions associated with higher log-likelihoods were explored. One advantage of the six profile solution is that most of its profiles can also be identified in solutions with fewer profiles. For the six profile solutions, this alternate solution was very different, whereas for the seven profile solutions, this alternate solution was highly similar. The model solutions exhibit a less clear pattern in terms of which profiles appear when. All else being equal, on the basis of parsimony, the model 1, six profile solution may be preferred and is selected for use in subsequent analyses.

## Appendix: Models for research question #2 and #3 with the seven-profile solution

<!-- # Model start here -->


```{r}
df <- read_csv("data/for-seven-profiles.csv")
m1_7 <- readr::read_rds("data/models/m1_7.rds")

cc <- df %>% select(dm_cog_eng:dm_competence) %>% complete.cases()
C <- select(m1_7, C)

C_p_m <- select(m1_7, CPROB1:CPROB7) %>% 
  apply(MARGIN = 1, FUN = max)

C_p <- select(m1_7, CPROB1:CPROB7)

df_ss <- df[cc, ]

df_ss <- bind_cols(df_ss, C_p)
df_ss <- mutate(df_ss, 
                profile = C,
                profile_p = C_p_m)

d <- df_ss %>% select(contains("dm"), participant_ID, program_ID, beep_ID = beep_ID_new, profile, profile_p, overall_pre_interest, youth_activity_rc, inquiry_based, inquiry_based_three, classroom_versus_field_enrichment, gender_female, urm, contains("cprob"), dm_composite_di)

d <- d %>% 
  mutate(
    # profile_1 = ifelse(profile == 1, 1, 0),
    # profile_2 = ifelse(profile == 2, 1, 0),
    # profile_3 = ifelse(profile == 3, 1, 0),
    # profile_4 = ifelse(profile == 4, 1, 0),
    # profile_5 = ifelse(profile == 5, 1, 0),
    # profile_6 = ifelse(profile == 6, 1, 0),
    # profile_7 = ifelse(profile == 7, 1, 0),
    profile_1_p = CPROB1,
    profile_2_p = CPROB2,
    profile_3_p = CPROB3,
    profile_4_p = CPROB4,
    profile_5_p = CPROB5,
    profile_6_p = CPROB6,
    profile_7_p = CPROB7
  )

labs <- c("Universally low",
          "Competent but not engaged or challenged",
          "Moderately low",
          "Challenged",
          "Highly challenged",
          "Engaged and competent but not challenged",
          "Full")
```

```{r, rq2-0-null-i, cache = FALSE, eval = TRUE}
m1 <- lmer(profile_1_p ~ 1 +
             (1 | participant_ID) +
             (1 | beep_ID) +
             (1 | program_ID),
           data = d)

m2 <- lmer(profile_2_p ~ 1 +
             (1 | participant_ID) +
             (1 | beep_ID) +
             (1 | program_ID),
           data = d)

m3 <- lmer(profile_3_p ~ 1 +
             (1 | participant_ID) +
             (1 | beep_ID) +
             (1 | program_ID),
           data = d)

m4 <- lmer(profile_4_p ~ 1 +
             (1 | participant_ID) +
             (1 | beep_ID) +
             (1 | program_ID),
           data = d)

m5 <- lmer(profile_5_p ~ 1 +
             (1 | participant_ID) +
             (1 | beep_ID) +
             (1 | program_ID),
           data = d)

m6 <- lmer(profile_6_p ~ 1 +
             (1 | participant_ID) +
             (1 | beep_ID) +
             (1 | program_ID),
           data = d)

m7 <- lmer(profile_7_p ~ 1 +
             (1 | participant_ID) +
             (1 | beep_ID) +
             (1 | program_ID),
           data = d)
```

```{r, rq2-1-all-vars-com-keep-i, cache = FALSE}
m1a <- lmer(profile_1_p ~ 1 +
              # gender_female +
              # urm + 
              dm_ask + dm_obs + dm_gen + dm_mod + dm_com +
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m2a <- lmer(profile_2_p ~ 1 +
              # gender_female +
              # urm + 
              dm_ask + dm_obs + dm_gen + dm_mod + dm_com +
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m3a <- lmer(profile_3_p ~ 1 +
              # gender_female +
              # urm + 
              dm_ask + dm_obs + dm_gen + dm_mod + dm_com +
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m4a <- lmer(profile_4_p ~ 1 +
              # gender_female +
              # urm + 
              dm_ask + dm_obs + dm_gen + dm_mod + dm_com +
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m5a <- lmer(profile_5_p ~ 1 +
              # gender_female +
              # urm + 
              dm_ask + dm_obs + dm_gen + dm_mod + dm_com +
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m6a <- lmer(profile_6_p ~ 1 +
              # gender_female +
              # urm + 
              dm_ask + dm_obs + dm_gen + dm_mod + dm_com +
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m7a <- lmer(profile_7_p ~ 1 +
              # gender_female +
              # urm + 
              dm_ask + dm_obs + dm_gen + dm_mod + dm_com +
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)
```

```{r, rq2-2-composite-keep-i, cache = FALSE}
m1b <- lmer(profile_1_p ~ 1 +
              dm_composite +
              # gender_female +
              # urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m2b <- lmer(profile_2_p ~ 1 +
              dm_composite +
              # gender_female +
              # urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m3b <- lmer(profile_3_p ~ 1 +
              dm_composite +
              # gender_female +
              # urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m4b <- lmer(profile_4_p ~ 1 +
              dm_composite +
              # gender_female +
              # urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m5b <- lmer(profile_5_p ~ 1 +
              dm_composite +
              # gender_female +
              # urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m6b <- lmer(profile_6_p ~ 1 +
              dm_composite +
              # gender_female +
              # urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m7b <- lmer(profile_7_p ~ 1 +
              dm_composite +
              # gender_female +
              # urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)
```

```{r, rq2-2-composite-keep-2-i, cache = FALSE}
m1bi <- lmer(profile_1_p ~ 1 +
               dm_composite_di +
               (1 | participant_ID) +
               (1 | beep_ID) +
               (1 | program_ID),
             data = d)

m2bi <- lmer(profile_2_p ~ 1 +
               dm_composite_di +
               # gender_female +
               # urm + 
               (1 | participant_ID) +
               (1 | beep_ID) +
               (1 | program_ID),
             data = d)

m3bi <- lmer(profile_3_p ~ 1 +
               dm_composite_di +
               # gender_female +
               # urm + 
               (1 | participant_ID) +
               (1 | beep_ID) +
               (1 | program_ID),
             data = d)

m4bi <- lmer(profile_4_p ~ 1 +
               dm_composite_di +
               # gender_female +
               # urm + 
               (1 | participant_ID) +
               (1 | beep_ID) +
               (1 | program_ID),
             data = d)

m5bi <- lmer(profile_5_p ~ 1 +
               dm_composite_di +
               # gender_female +
               # urm + 
               (1 | participant_ID) +
               (1 | beep_ID) +
               (1 | program_ID),
             data = d)

m6bi <- lmer(profile_6_p ~ 1 +
               dm_composite_di +
               # gender_female +
               # urm + 
               (1 | participant_ID) +
               (1 | beep_ID) +
               (1 | program_ID),
             data = d)

m7bi <- lmer(profile_7_p ~ 1 +
               dm_composite_di +
               # gender_female +
               # urm + 
               (1 | participant_ID) +
               (1 | beep_ID) +
               (1 | program_ID),
             data = d)
```

```{r, rq3-2-all-vars-sep-interaction-keep-i, eval = TRUE}
m1c <- lmer(profile_1_p ~ 1 +
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m2c <- lmer(profile_2_p ~ 1 +
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m3c <- lmer(profile_3_p ~ 1 +
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m4c <- lmer(profile_4_p ~ 1 +
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m5c <- lmer(profile_5_p ~ 1 +
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m6c <- lmer(profile_6_p ~ 1 +
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m7c <- lmer(profile_7_p ~ 1 +
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)
```

```{r, rq3-2-all-vars-com-keep-i, eval = TRUE}
m1d <- lmer(profile_1_p ~ 1 +
              dm_composite + 
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m2d <- lmer(profile_2_p ~ 1 +
              dm_composite + 
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m3d <- lmer(profile_3_p ~ 1 +
              dm_composite + 
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m4d <- lmer(profile_4_p ~ 1 +
              dm_composite + 
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m5d <- lmer(profile_5_p ~ 1 +
              dm_composite + 
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m6d <- lmer(profile_6_p ~ 1 +
              dm_composite + 
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m7d <- lmer(profile_7_p ~ 1 +
              dm_composite + 
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)
```

```{r, rq3-2-all-vars-sep-keep-i, eval = TRUE}
m1di <- lmer(profile_1_p ~ 1 +
               dm_ask + dm_obs + dm_gen + dm_mod + dm_com +
               overall_pre_interest +
               gender_female +
               urm + 
               (1 | participant_ID) +
               (1 | beep_ID) +
               (1 | program_ID),
             data = d)

m2di <- lmer(profile_2_p ~ 1 +
               dm_ask + dm_obs + dm_gen + dm_mod + dm_com +
               overall_pre_interest +
               gender_female +
               urm + 
               (1 | participant_ID) +
               (1 | beep_ID) +
               (1 | program_ID),
             data = d)

m3di <- lmer(profile_3_p ~ 1 +
               dm_ask + dm_obs + dm_gen + dm_mod + dm_com +
               overall_pre_interest +
               gender_female +
               urm + 
               (1 | participant_ID) +
               (1 | beep_ID) +
               (1 | program_ID),
             data = d)

m4di <- lmer(profile_4_p ~ 1 +
               dm_ask + dm_obs + dm_gen + dm_mod + dm_com +
               overall_pre_interest +
               gender_female +
               urm + 
               (1 | participant_ID) +
               (1 | beep_ID) +
               (1 | program_ID),
             data = d)

m5di <- lmer(profile_5_p ~ 1 +
               dm_ask + dm_obs + dm_gen + dm_mod + dm_com +
               overall_pre_interest +
               gender_female +
               urm + 
               (1 | participant_ID) +
               (1 | beep_ID) +
               (1 | program_ID),
             data = d)

m6di <- lmer(profile_6_p ~ 1 +
               dm_ask + dm_obs + dm_gen + dm_mod + dm_com +
               overall_pre_interest +
               gender_female +
               urm + 
               (1 | participant_ID) +
               (1 | beep_ID) +
               (1 | program_ID),
             data = d)

m7di <- lmer(profile_7_p ~ 1 +
               dm_ask + dm_obs + dm_gen + dm_mod + dm_com +
               overall_pre_interest +
               gender_female +
               urm + 
               (1 | participant_ID) +
               (1 | beep_ID) +
               (1 | program_ID),
             data = d)
```

```{r, rq3-2-all-vars-interaction-inq-keep-i, eval = TRUE}
m1e <- lmer(profile_1_p ~ 1 +
              overall_pre_interest*dm_composite + 
              gender_female*dm_composite +
              urm*dm_composite + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m2e <- lmer(profile_2_p ~ 1 +
              overall_pre_interest*dm_composite + 
              gender_female*dm_composite +
              urm*dm_composite + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m3e <- lmer(profile_3_p ~ 1 +
              overall_pre_interest*dm_composite + 
              gender_female*dm_composite +
              urm*dm_composite + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m4e <- lmer(profile_4_p ~ 1 +
              overall_pre_interest*dm_composite + 
              gender_female*dm_composite +
              urm*dm_composite + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m5e <- lmer(profile_5_p ~ 1 +
              overall_pre_interest*dm_composite + 
              gender_female*dm_composite +
              urm*dm_composite + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m6e <- lmer(profile_6_p ~ 1 +
              overall_pre_interest*dm_composite + 
              gender_female*dm_composite +
              urm*dm_composite + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m7e <- lmer(profile_7_p ~ 1 +
              overall_pre_interest*dm_composite + 
              gender_female*dm_composite +
              urm*dm_composite + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)
```

<!-- # Models start running here -->

```{r, rq2-1-tab-i, eval = FALSE}
l <- list(m1a, m2a, m3a, m4a, m5a, m6a, m7a)
o <- map_df(l, tidy_model)
write_rds(o, "data/rq2-1-tab-i.rds")
```

```{r, rq2-1-tab-pres-i}
o <- read_rds("data/rq2-1-tab-i.rds")

o <- mutate(o, model = c(
  str_c("profile_", 1:7)))

o %>% 
  select(model, 
         intercept = `(Intercept)`,
         dm_ask, dm_obs, dm_gen, dm_mod, dm_com,
         beep_ID_ICC = beep_ID_ICC,
         participant_ID_ICC,
         program_ID_ICC) %>% 
  mutate(model = labs) %>% 
  knitr::kable(format = "latex", booktabs = TRUE, caption = "Results of mixed effects models for instructional support for work with data as separate variables", linesep = "") %>% 
  kableExtra::kable_styling(latex_options = "scale_down") %>% 
  kableExtra::landscape()
```

```{r, just-composite-mod-i, eval = FALSE}
l <- list(m1b, m2b, m3b, m4b, m5b, m6b, m7b)
o <- map_df(l, tidy_model)
write_rds(o, "data/comp-l-i.rds")
```

```{r, just-composite-red-i, eval = TRUE}
o <- read_rds("data/comp-l-i.rds")
o <- mutate(o, model = c(
  str_c("profile_", 1:7)))

o %>% 
  select(model, 
         intercept = `(Intercept)`,
         dm_composite,
         beep_ID_ICC = beep_ID_ICC,
         participant_ID_ICC,
         program_ID_ICC) %>% 
  mutate(model = labs) %>% 
  knitr::kable(format = "latex", booktabs = TRUE, caption = "Results of mixed effects models for the composite", linesep = "") %>% 
  kableExtra::kable_styling(latex_options = "scale_down") %>% 
  kableExtra::landscape()
```

```{r, just-composite-mod-2-i, eval = FALSE}
l <- list(m1bi, m2bi, m3bi, m4bi, m5bi, m6bi, m7bi)
o <- map_df(l, tidy_model)
write_rds(o, "data/comp-l-2-i.rds")
```

```{r, just-composite-red-2-i, eval = TRUE}
o <- read_rds("data/comp-l-2-i.rds")
o <- mutate(o, model = c(
  str_c("profile_", 1:7)))

o %>% 
  select(model, 
         intercept = `(Intercept)`,
         dm_composite_di,
         beep_ID_ICC = beep_ID_ICC,
         participant_ID_ICC,
         program_ID_ICC) %>% 
  mutate(model = labs) %>% 
  knitr::kable(format = "latex", booktabs = TRUE, caption = "Results of mixed effects models for the composite", linesep = "") %>% 
  kableExtra::kable_styling(latex_options = "scale_down") %>% 
  kableExtra::landscape()
```

```{r, md-block-for-rq4-i, eval = FALSE}
l <- list(m1c, m2c, m3c, m4c, m5c, m6c, m7c)
o <- map_df(l, tidy_model)
write_rds(o, "data/md-block-for-rq4-i")
```

```{r, reading-for-rq4-i, eval = TRUE}
o <-read_rds("data/md-block-for-rq4-i")

o <- mutate(o, model = c(
  str_c("profile_", 1:7)))

o %>% 
  select(model, 
         intercept = `(Intercept)`,
         overall_pre_interest,
         gender_female,
         urm,
         beep_ID_ICC = beep_ID_ICC,
         participant_ID_ICC,
         program_ID_ICC) %>% 
  mutate(model =labs) %>% 
  knitr::kable(format = "latex", booktabs = TRUE, caption = "Results of mixed effects models with interest and other characteristics", linesep = "") %>% 
  kableExtra::kable_styling(latex_options = "scale_down") %>% 
  kableExtra::landscape()
```

```{r, pre-int-ind-i, eval = FALSE}
l <- list(m1di, m2di, m3di, m4di, m5di, m6di, m7di)
o <- map_df(l, tidy_model)
write_rds(o, "data/pre-int-ind-interaction-i.rds")
```

```{r, pre-int-ind-present-i, eval = TRUE}
o <- read_rds("data/pre-int-ind-interaction-i.rds")
o <- mutate(o, model = c(
  str_c("profile_", 1:7)))

o %>% 
  select(model, 
         intercept = `(Intercept)`,
         dm_ask, dm_obs, dm_gen, dm_mod, dm_com,
         overall_pre_interest,
         gender_female,
         urm,
         beep_ID_ICC = beep_ID_ICC,
         participant_ID_ICC,
         program_ID_ICC) %>% 
  mutate(model = labs) %>% 
  knitr::kable(format = "latex", booktabs = TRUE, caption = "Results of mixed effects models with interest and other characteristics and the aspects of work with data", linesep = "") %>% 
  kableExtra::kable_styling(latex_options = "scale_down") %>% 
  kableExtra::landscape()
```

```{r, pre-int-i, eval = FALSE}
l <- list(m1d, m2d, m3d, m4d, m5d, m6d, m7d)
o <- map_df(l, tidy_model)
write_rds(o, "data/pre-int-no-interaction-i.rds")
```

```{r, pre-int-int-present-i, eval = TRUE}
o <- read_rds("data/pre-int-no-interaction-i.rds")
o <- mutate(o, model = c(
  str_c("profile_", 1:7)))

o %>% 
  select(model, 
         intercept = `(Intercept)`,
         dm_composite,
         overall_pre_interest,
         gender_female,
         urm,
         beep_ID_ICC = beep_ID_ICC,
         participant_ID_ICC,
         program_ID_ICC) %>% 
  mutate(model = labs) %>% 
  knitr::kable(format = "latex", booktabs = TRUE, caption = "Results of mixed effects models with interest and other characteristics and the composite work with data", linesep = "") %>% 
  kableExtra::kable_styling(latex_options = "scale_down") %>% 
  kableExtra::landscape()
```

```{r, pre-int-interactions-i, eval = FALSE}
l <- list(m1e, m2e, m3e, m4e, m5e, m6e, m7e)
o <- map_df(l, tidy_model)
write_rds(o, "data/pre-int-interaction-i.rds")
```

```{r, pre-int-comp-i, eval = TRUE}
o <- read_rds("data/pre-int-interaction-i.rds")
o <- mutate(o, model = c(
  str_c("profile_", 1:7)))

o %>% 
  select(model, 
         intercept = `(Intercept)`,
         dm_composite,
         overall_pre_interest,
         gender_female,
         urm,
         `overall_pre_interest:dm_composite`,
         `dm_composite:gender_female`,
         `dm_composite:urm`,
         beep_ID_ICC = beep_ID_ICC,
         participant_ID_ICC,
         program_ID_ICC) %>% 
  mutate(model = labs) %>% 
  knitr::kable(format = "latex", booktabs = TRUE, caption = "Results of mixed effects models with the interactions between interest and other characactistics and the composite for work with data", linesep = "") %>% 
  kableExtra::kable_styling(latex_options = "scale_down") %>% 
  kableExtra::landscape()
```
