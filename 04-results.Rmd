# Results

```{r, setup-results, include =FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      collapse = TRUE,
                      error = TRUE,
                      fig.width = 6,
                      fig.asp = .618,
                      out.width = "80%",
                      fig.align = "center",
                      results = "hold",
                      knitr.kable.na = '')

source("04-results.R")
```

In this section, results associated with the preliminary analysis and the five research questions are presented. 

## Preliminary results

### Descriptive statistics for the engagement measures

First, descriptive statistics for the engagement measures (the five variables that were used to estimate the profiles) are presented in Table 4.1. These values suggest moderately high levels of the three indicators of engagement (with mean values between 2.768 (*SD* = 1.063) for cognitive engagement, and 2.863 (*SD* = 1.044), for behavioral engagement, on one-four scales) and high perceptions of competence (*M* = 3.000 (*SD* = 0.952)) and lower perceptions of challenge (*M* = 2.270 (*SD* = 1.117)).

```{r}
oo <- d_red %>% 
  select(dm_cog_eng,
         dm_beh_eng,
         dm_aff_eng,
         dm_challenge,
         dm_competence) %>% 
  psych::describe(.) %>% 
  as.data.frame() %>% 
  tibble::rownames_to_column("var") %>% 
  select(var, n, mean, sd) %>% 
  mutate(var = c("Cog. eng.",
                 "Beh. eng.",
                 "Aff. eng.",
                 "Challenge", 
                 "Competence"))

names(oo) <- c("", "n", "Mean", "SD")

oo %>% 
  knitr::kable(format = "latex", booktabs = TRUE, caption = "Descriptive statistics for study variables", linesep = "", digits = 3)
```

### Correlations among the study variables

Next, correlations between the variables that are used to create the profiles are presented in Table 4.2. These correlations among the variables used to construct the profiles, which range from *r* = .08 through *r* = .60 (all statistically significant), show moderate relations.

<!-- Pre-interest was correlated with the variables used to construct the profiles to a small degree (with *r* values ranging between .06 and .14). -->

```{r}
p <- d_red %>%
  select(overall_pre_interest, dm_cog_eng:dm_competence) %>%
  corrr::correlate() %>%
  corrr::shave() %>%
  corrr::fashion() %>% 
  mutate(rowname = c("Pre-interest",
                     "Cog. eng.",
                     "Beh. eng.",
                     "Aff. eng.",
                     "Challenge", 
                     "Competence"))

names(p) <- c("",
              "Pre-interest",
              "Cog. eng.",
              "Beh. eng.",
              "Aff. eng.",
              "Challenge", 
              "Competence")

p %>% 
  knitr::kable(booktabs = TRUE, format = "latex", linesep = "", caption = "Correlations among study variables")
```

## Results for Research Question #1

### Frequency of work with data

Of the 236 instructional episodes, 170 (72%) were coded as involving *any* of the aspects of work with data. These 236 episodes are used as the sample for this study because the proportion of episodes that included any of the aspects of work with data is a question asked in this study. Table 4.3 includes the frequency of the specific aspects of work with data, with interpreting and communicating findings being the most present (occurring in 47% of the coded instructional episodes), followed by generating data (in 45% of the instructional episodes), asking questions (in 39%), data modeling (29%), and then making observations (26%).

```{r}
data_frame(`Aspect of Work With Data` = c("Asking Questions", "Making Observations", "Generating Data", "Data Modeling", "Communicating Findings"),
           Proportion = c(.389, .258, .453, .288, .470),
           N = c(92, 61, 107, 68, 111)) %>%
  knitr::kable(booktabs = TRUE, linesep = "", caption = "Proportion of signals for which each of the aspects of work with data was present")
```

Note that these results are for codes applied to approximately ten-minute (video-recorded) instructional episodes and that the aspects of work with data could co-occur. On average, there were 1.86 (*SD* = 1.61) aspects of work with data present in each 10-minute instructional episode. This indicates that, on average, youth were engaged in around two of aspects of the work with data during each instructional episode. There was considerable variation in the extent to which these types of work with data were supported in each program (see Appendix C).

### The nature of work with data

For these results, the different aspects of work with data were looked at in more detail using an open-ended, qualitative approach in order to better understand the nuance and the specific nature of what was going on during these episodes in terms of how students work with data. This coding showed there to be distinct, qualitative differences in the particular ways youth worked with data.

<!-- One affordance of these programs relevant to these informal and to K-12 learning environments concerns selecting activities that are engaging to youth.  This appeared to be a powerful activity, one that was coded as involving all five aspects of work with data according to the measures for instructional support for work with data; this type of activity seemed to suggest that instructional support for work with data may impact youth's engagement. -->

<!-- Another affordance concerned the relevance of the program to youth's lives. For example, in the *Building Mania* program, youth are involved in engineering design (i.e., identifying a problem and designing a solution), particularly around the use of simple machines. In a day in the classroom setting, youth are creating, testing, and revising catapults. In the next day, youth visit an area University, and are led in a discussion by a physicist who works with particle colliders. In this example, the expertise of the physicist, who explicitly mentions the benefits of engaging in the engineering design process and the importance of combining engineering to addressing problems (such as mitigating the damage of earthquakes), seems to be highly relevant to what youth are doing in their class. In these two days of class, youth are engaged in different aspects of work with data as indicated by the codes for instructional support for work with data (collecting data on the efficacy of their designs in the classroom day, and asking questions in the subsequent day, particularly); these seem to suggest, like the example of work work with data from the *Marine Investigators* program, affordances of work with data for summer STEM programs. -->

<!-- different aspects of instructional support for work with data were emphasized to different degrees based on the focus of the program. -->

#### Asking questions or identifying problems

Among the instructional episodes that involved asking questions, qualitative descriptions revealed that around one-third (36/92, or 39%) explicitly demonstrated youth working to understand the phenomenon or problem they were investigating. For example, in an instructional episode during the *Ecosphere* program in which youth constructed inclined tables to study how water moved throughout the ecosystem, the youth activity leader prompted youth to generate hypotheses of what would happen when water was poured onto the table, before pouring the water. 

Other instructional episodes (56/92; 61%) involved questions that were not focused on predicting, conjecturing, or hypothesizing, but rather on asking a more general type of question question; or, the instructor asking youth questions. In the former case, youth were asking generic questions about understanding the assignment, task, or even the phenomena. For example, in the *Marine Investigators* program, youth visited a water treatment site, and were provided opportunities to ask questions about what they observed: However, youths' questions were not questions that could then be answered with empirical data, but were rather to clarify their understanding. In the latter, instructors were asking youth questions (i.e., math-related prompts or questions to elicit their conceptual understanding). 

#### Making observations

In the instructional episodes when the STEM-PQA revealed that youth were making observations, the vast majority (49/57, or 86%) of these were focused on observing phenomenon in the field, or, in the case of engineering-focused programs, noticing what was going on with a particular design. For example, in the *Building Mania* program, youth constructed Rube Goldberg machines; youth were prompted by the activity leaders to notice how changes in their design led to differences in how far objects were launched or rolled. 

Sometimes, while youth did make observations of phenomena, they faced the challenge of using that data in subsequent activities and through engaging in other aspects of work with data. For example, the science-focused programs (*Island Explorers*, *The Ecosphere*, and *Marine Investigators*) all emphasized making observations, but these observations were not frequently written down or entered into a spreadsheet as data. For example, in *Marine Investigators*, youth observed how the Atlantic Ocean brings salt water into the (freshwater) bay. Youth observe "buffers" between the salt and freshwater, but do not collect or otherwise generate data related to their observations. 

In a small number of cases (8/57; 14%) making observations was focused on making observations not of phenomena, but of the instructor. For example, in the *Adventures in Mathematics* program, youth observed other youth or the youth activity leader solving a mathematics problem.

#### Generating data

In about half (48/102; 47%) of the episodes that involved generating data, youth were writing down their own observations of a phenomenon, recording information from experiments, or recording the results of a trial (in engineering contexts). For example, in the *Marine Investigators* program, youth collected pieces of recyclable plastic, bringing them back to the classroom and counting them for each location they were collected. 

As described above, there were cases in which youth made observations but did not record data or use their observations as a context to generate data. Similarly, in a number of the cases in which youth generated data, they did not use the data they generated in subsequent activities. In the engineering-focused programs (*Uptown Architecture*, *Crazy Machines*, and *Dorchester House*, youth often generated data that resulted from their engineering designs (and communicated and interpreted their findings,) but did not model this data as a regular part of their activities. 

The other half (54/102; 53%) of the cases involved youth recording pieces of data that were provided verbally by the activity leader or were focused on collecting specimens (but not writing them down or entering them into a spreadsheet). For example, again in the *Marine Investigators* program, youth used nets to collect saltwater organisms, which they then transported in buckets back to the classroom setting for subsequent analysis. While these specimens were collected to serve as data for a future activity, there was no recording observed during the episode. In another example, in the *Ecosphere* program, youth collected water samples in the field. They then brought these samples to the classroom and tested the water, involving students in both collecting and, to a degree, generating data (by noting the pH levels of the water). However, later in the day, youth created a small-scale model (with inclined trays of dirt, rocks, and plants) of an ecosystem, in which they added food coloring to determine the impacts of chemicals and acid rain. Youth then interpreted and discussed these findings, but did not connect the discussion to the water samples youth collected and tested earlier. This activity presented an opportunity for deeper engagement in generating data through using generated data in subsequent activities, in which youth could interpret and communicate findings related to the state of the water in their ecosystem; but, instead, it was potentially limiting in terms of youth's engagement in work with data.

#### Data modeling

A large majority (49/68, 72%) of the instructional episodes coded (with the STEM-PQA) for data modeling were focused on youths' uses of statistical and mathematical models. For example, in the *Comunidad de Aprendizaje* program, youth accessed nationally-representative data and were tasked to solve problems, like finding out what percentage of people engage in particular activities, like donating to charity. In another example, in the *Marine Investigators*, youth participated in activities designed to help them understand water quality in their ecosystem. Youth collected trash from sites around their community (in different "districts") and then brought the trash and recyclable plastic back to the classroom. Then, the youth activity leaders involved youth in an ambitious data modeling activity. The aim was to figure out how much plastic enters local waterways. As a part of this activity, youth activity leaders asked youth not only to determine the quantity of trash that entered the waterways, but asked youth about *why* youth thought about and used math in particular ways (i.e., by adding the quantity of trash collected and then extrapolating from this quantity to the amount from across the entire city over the course of the year). This appeared to be an ambitious and powerful data modeling activity. 

Related, while the data modeling activities were often ambitious, youth activity leaders sometimes faced the challenge of linking data modeling activities to other aspects of work with data. For example, the mathematics-focused programs, such as the *Adventures in Mathematics* program, the youth activity leaders recognizing that youth had difficulty solving equations, used duct tape and a "hippity hoppity", building on an earlier activity in which youth considered what constituted a rate, on how many "hops" it would take someone to move from one end of the line of duct tape to the other; the youth activity leader than asked youth to consider how far they could move in one hop and to consider how they could find out many hops it would take, using a mathematical equation. In this activity, youth were supported in their attempts to approach mathematics problem-solving in creative ways. However, apart from data modeling, other aspects of work with data were rarely present, and most of the data that youth worked with was provided by the teacher or considered in the abstract.

In a smaller number of instructional episodes not focused on ambitious data modeling activities (19/68; 28%), this aspect of work with data was present when the youth activity leader, rather than students, was doing the modeling, or the model was not one that could generate data. For example, in the *Marine Investigators* program, a youth activity leader used a plush toy seal designed to teach youth about anatomy and the dangers of aquatic mammals consuming trash and recyclables. 

#### Interpreting and communicating findings

In around half (49/103, 48%) of the instructional episodes in which youth were interpreting and communicating findings (as coded by the STEM-PQA), youth were sharing what they found from an investigation or the results of using the product they designed. For example, in the *Comunidad de Aprendizaje* program, youth participated in an activity designed to support their thinking about creating a product to bring to market; the youth activity leaders described this as being akin to the television show the *Shark Tank*. In one instructional episode, the youth activity leader asks youth to think of an idea that would make an investor willing to invest in; students shared their ideas, describing what their ideas was, why it was a good idea, how much they could sell it for, and what their profit would be, while fielding questions from youth activity leaders and their peers. Interpreting and communicating findings was also commonly present in instructional episodes in which youth were debating the findings of an investigation, such as the results of calculations for the amount of recyclables entering waterways (in *Marine Investigators*). 

In the approximately half of the responses that were not focused on youth sharing what they found from an investigation (54/103, 52%), youth were most commonly communicating about topics other than the results of an investigation or design process, such as trying to find out the answer to a discrete question posed by the youth activity leader, or the youth activity leader was who was doing the interpreting and communicating. For example, in the *Adventures in Mathematics* program, the youth activity leader helped youth to solve problems on a worksheet, asking guiding questions to help youth start to solve problems on their own. In the latter case (the youth activity leader was doing the interpreting and communicating), youth commonly engaged in other aspects of work with data (i.e., generating data), but the youth activity leader compiled, modeled, and then interpreted the data that the youth generated.

## Results for Research Question #2: What profiles of youth engagement emerge from experiential data collected in the programs?

A relatively simple model (with varying means, equal variances, and covariances fixed to 0) with six profiles was selected for use in subsequent analyses. This model has profiles characterized by both varying levels on both the indicators of engagement--cognitive, behavioral, and affective--and youths' perceptions of challenge and competence. In addition, the number of observations across the profiles is relatively balanced. 

This means that *six distinct profiles* were identified in the data. This selection was on the basis of fit statistics, statistical tests, and concerns of interpretability and parsimony. The solution demonstrated superior fit on the basis of the information criteria (AIC and BIC) and on the basis of the measure of classification accuracy (entropy). Note that a seven profile solution with the same specifications regarding means, variances and covariances was also a similarly good fit (and is presented in Appendix F), but the six profile solution was ultimately chosen on the basis of parsimony and interpretability. 

For the six profiles, presented below in Figure 4.1 and 4.2. Figure 4.1 shows the profiles with variables that were centered to have a mean equal to 0 and a standard deviation of 1. Thus, the *y*-axis for this plot is labeled "Z-score").Figure 4.2 shows the profiles with the raw data (not transformed). Thus, the *y*-axis for this plot is labeled "Value." Both plots are presented because they provide different insight into the composition of the profiles: those with the centered variables highlights positive and negative departures from the mean value for each variable, making differences between the profiles distinct. The plot with the raw data instead highlights the reported values of the variables, emphasizing the values of the variables in the profiles in the same units youth considered when they responded (and potentially highlighting similarities that may seem very different in the plot with the centered data).

```{r, fig.width = 7, fig.asp = .618, out.width = "100%", fig.cap = "The six profiles of engagement (with variable values standardized)"}

m1_6 <- read_rds("data/models/m1_6.rds")

p1 <- m1_6 %>%
  plot_profiles_mplus(to_center = TRUE, to_scale = TRUE) +
  scale_x_discrete("", 
                   limits = c("Profile 2 (n = 667)",
                              "Profile 1 (n = 370)",
                              "Profile 4 (n = 345)",
                              "Profile 5 (n = 638)",
                              "Profile 3 (n = 450)",
                              "Profile 6 (n = 488)"),
                   labels = c("Universally low (n = 667)",
                              "Only behavioral (n = 370)",
                              "Only affective (n = 345)",
                              "All moderate (n = 638)",
                              "Engaged and competent but not challenged (n = 450)",
                              "Full (n = 488)")) +
  xlab(NULL) +
  ylab("Z-score") +
  viridis::scale_fill_viridis("",
                              limits = c("DM_AFF_E", "DM_BEH_E", "DM_COG_E", "DM_CHALL", "DM_COMPE"),
                              labels = c("Affective", "Behavioral", "Cognitive", "Challenge", "Competence"), discrete = TRUE) +
  theme(plot.margin = margin(1, 0, 0, 1, "cm"))

p1
```

```{r, fig.cap = "The six profiles of engagement (with raw variable values)"}
m1_6 <- read_rds("data/models/m1_6.rds")

p2 <- m1_6 %>%
  plot_profiles_mplus(to_center = FALSE, to_scale = FALSE) +
  scale_x_discrete("", 
                   limits = c("Profile 2 (n = 667)",
                              "Profile 1 (n = 370)",
                              "Profile 4 (n = 345)",
                              "Profile 5 (n = 638)",
                              "Profile 3 (n = 450)",
                              "Profile 6 (n = 488)"),
                   labels = c("Universally low (n = 667)",
                              "Only behavioral (n = 370)",
                              "Only affective (n = 345)",
                              "All moderate (n = 638)",
                              "Engaged and competent but not challenged (n = 450)",
                              "Full (n = 488)")) +
  xlab(NULL) +
  ylab("Value") +
  viridis::scale_fill_viridis("",
                              limits = c("DM_AFF_E", "DM_BEH_E", "DM_COG_E", "DM_CHALL", "DM_COMPE"),
                              labels = c("Affective", "Behavioral", "Cognitive", "Challenge", "Competence"), discrete = TRUE) +
  theme(plot.margin = margin(1, 0, 0, 1, "cm")) +
  coord_cartesian(ylim = c(1, 4))

p2
```

This solution is characterized by:

- A *universally low* profile, characterized by low levels of working hard, learning something new, and enjoying the activity, and perceptions challenge and competence
- An *only behaviorally engaged* profile, with moderate levels of working hard, very low enjoyment of the activity, and moderately (low) levels of learning something new and challenge and competence
- An *only affectively engaged* profile, with moderate levels of enjoyment, low levels of hard work, and moderately (low) levels of cognitive learning something new, challenge, and competence
- A *all moderate* profile, with moderate levels of the three indicators of working hard, learning something new, enjoying the activity, challenge, and competence
- An *engaged and competent but not challenged* profile, characterized by high levels of working hard, learning something new, enjoying the activity, and competence, but with low levels of challenge
- A *full* profile, with high levels of working hard, learning something new, enjoying the activity, challenge, and competence

The number of observations associated with each of the profiles is somewhat balanced, with the universally low profile with the largest number of observations (*n* = 667), followed by the all moderate profile (*n* = 638). Each of the other four profiles were associated with 300 to 400 observations. 

## Results for Research Question #3: What sources of variability are there for the profiles of engagement?

The remaining analyses use the six profiles described above. How youth are engaging is a function of who they are as an individual, what they happen to be doing during a particular instructional episode, and which youth program they are enrolled in, as well as random variation. This analysis seeks to identify how much of the variation is at each of these levels through using null models, or models only with the indicators for the three levels (youth, instructional episode, and program). These models can show how much variability in the profiles issystematic at these different levels and is potentially attributable to each of these types of factors. These null models may also suggest something about where you might want to be looking to explain sources of youth's engagement.

For all six profiles, the ICCs at the program level were very small, from 0.00 to 0.023. This suggests that very little variability can be explained simply by the program. For the instructional episode level, the ICCs were also very small, ranging from 0.004 to 0.01. Finally, the youth-level ICCs ranged from .093 to .432. Looking across these values, most of the explained variability in the responses is associated with youth; the program and instructional episode levels were associated with very small values, suggesting that variables at these levels have minimal variability to explain. In turn, this suggests that these variables, including those for work with data, may not have strong effects in terms of their relations with the profiles.

```{r}
i <- read_rds("data/m1-6.rds") 

ii <- i %>% 
  select(beep_ID_ICC:program_ID_ICC) %>% 
  mutate(order = c(2, 1, 4, 5, 3, 6)) %>% 
  arrange(order) %>% 
  mutate(profile = c("Universally low (n = 667)",
                     "Only behavioral (n = 370)",
                     "Only affective (n = 345)",
                     "All moderate (n = 638)",
                     "Engaged and competent but not challenged (n = 450)",
                     "Full (n = 488)"))

names(ii) <- c("Instructional Episode", "Youth", "Program")

ii %>% 
  knitr::kable(format = "latex", booktabs = TRUE, caption = "Intra-class correlation (ICC) values for each of the three levels", linesep = "")
```

In terms of specific ICCs at the youth level, the value for the youth-level ICC was highest for the *Full* profile (*ICC* = .432), suggesting that some youth have a strong tendency to be fully engaged (possibly due to their initial interest or other individual characteristics and differences). The other profile characterized by a consistent pattern across all of the variables--the *Universally low* profile--had a modest value for the ICC at the youth level (*ICC* = .267). Finally, a large amount of variability is associated with the residual (variance that is not associated with the program, instructional episode, or youth levels). This suggests that there is wide variation in students' responses that may not be readily explained or predicted by variables *at one level alone*. Remaining unexplained variability is captured by the residual term. Some youth from particular programs may engage during some episode instructional episodes in very high or low ways that are not captured by modeling the variability at each of these levels alone.

<!-- . E.g., the ICCs above suggest that a lot of the variation in profiles may be due to the youth level. So the next question is, do youth have stable modes of engagement (are there some kids who are just always fully engaged)? You want to look at the extent to which individual youth exhibit stable profiles of engagement across the many different things they did in the summer program. Then explain your 60%, 40% results in more user friendly language. I see the pdf version has a histogram that is supposed to relate to this idea of within-person stability. I see no reference to that model in the text here (there should be one), and what’s in the histogram is not actually described here at all. The information that’s presented in that figure is potentially helpful for understanding the profiles, but as written and displayed it’s confusing. What I think that histogram says (and I’m not sure) is that there were a handful of kids (7 or 8?) who were always in the exact same profile every time they responded (but we don’t know which profile this is). Most kids, however, did not have a “dominant” profile in this way, but rather engaged in a whole variety of ways across the course of the study. In fact for most kids, the most frequent profile was observed 50% of the time or less. You need to think of a more straightforward way to  explain that histogram if you are going to include it.   -->

As presented in Figure 4.3, the value of the mean for the proportion of responses for each youth in the profile they reported most was .540 (*SD* = .194, *min* = .182, *max* = 1.00). When *Full* engagement was reported by a youth more than any other profile, they reported it, on average, in just over 60% of their responses. No other profile that youth reported most was associated (on average) with a larger proportion of their responses. This suggests that even when youth report (relatively) stable engagement over the course of their time in the programs, they still engage in a variety of different ways. When youth reported *All moderate* engagement (the profile with the lowest proportion of responses, on average, when reported more than any other), they reported it in just less than 40% of their responses. This suggests that though youth may report one profile far more than others--and that youth who report particular (i.e., *Full*) profiles more than others may reflect somewhat stable engagement, there still exists substantial variability in youths' engagement.

In sum, these findings show substantial variabiliy in the profiles present at the youth level, with less explained by the program youth were in or the nature of the particular instructional episode present at the time youth were signaled. 

```{r, fig.cap = "Histogram of the proportion of responses for each youth in the profile they reported most"}
p <- d %>%
  count(participant_ID, profile) %>%
  spread(profile, n, 0) %>%
  gather(profile, n, -participant_ID) %>%
  group_by(participant_ID) %>%
  mutate(n_p = n / sum(n)) %>%
  select(-n) %>%
  summarize(m_n_p = max(n_p))

ggplot(p, aes(x = m_n_p)) +
  geom_histogram(bins = 50) +
  theme_bw() +
  xlab("Proportion of responses for each youth in the profile they reported most") +
  ylab("Number of Youth") +
  theme(text = element_text(family = "Times"))
```

## Results for Research Question #4: Aspects of work with data and engagement

Six models (one for each profile) were specified, with the probability of a response being associated with one of the six profiles as the dependent variable. Only the coefficients associated with the aspects of work with data are interpreted in this section in order to provide results for this research question, because the youth characteristics are interpreted for research question #5. As described earlier, the results were practically the same for these sets of predictors being included in either separate or the same model, so they are included in the same model. 

The results for this and the next research question are presented in Table 4.5. In this table, each column represent the output from one of the six different models. For example, the first column includes the results for the model with the probability of a response being associated with the *Only behavioral* profile as the dependent variable. The cells down the rows contain the coefficients (and their standard errors and asterisks indicating statistical significance) for each of the predictor variables.Note that the *p*-values are calculated using the Kenward-Rogers approximation.

There were minimal relations between work with data and the profiles. Note that there were only significant relations with the *Full* profile (see the column with the column name *Full* for these results). One relation that was statistically significant was for the relations between modeling data and the *Full* profile ($\beta$ = 0.034 (0.017), *p* = .020; *partial R^2* = .002). Another was between generating data and the *Full* profile ($\beta$ = 0.027 (0.015), *p* = .033; *partial R^2* = .002): When youth were either modeling or generating data, they were more likely to be fully engaged. The effect of asking questions was marginally significant upon the probability of a response being associated with the *All moderate* profile ($\beta$ = 0.023 (0.017), *p* = .090; *partial R^2* = .001). The effect sizes suggest very small effects in substantive terms.

After finding there to be few relations between the aspects of work with data and engagement, sensitivity analysis for the statistical significant effects was carried out. This revealed that the effect of modeling data on *Full* engagement was more robust than that for generating data (also upon *Full* engagement): 9.835% of the effect of modeling would have to be due to bias to invalidate the inference about its effect, whereas only 1.884% of the effect of generating data would need to be due to bias to invalidate the inference about its effect. Thus, neither of these findings is particularly robust. Further explanations and investigations of these effects are the focus on research question #5 (in terms of the effect of youth characteristics) and are also discussed in the next chapter. 

```{r, eval = T}
o <- read_rds("data/m1d-6d.rds")
o <- mutate(o, model = c(
  str_c("profile_", 1:6)))

oo <- o %>%
  select(model,
         intercept = `(Intercept)`,
         overall_pre_interest,
         gender_female,
         urm,
         dm_ask, dm_obs, dm_gen, dm_mod, dm_com
  ) %>% 
  mutate(the_order = c(2, 1, 4, 5, 3, 6)) %>% 
  arrange(the_order) %>%
  select(-the_order) %>% 
  mutate(model = c("Universally low",
                   "Only behavioral",
                   "Only affective",
                   "All moderate",
                   "Engaged and competent but not challenged",
                   "Full"))

names(oo) <- c("Profile", "Intercept", "Pre-interest", "Gender-Female", "URM status", "Asking", "Observing", "Generating", "Modeling", "Communicating")

re_p <- function(x) {
  o <- str_sub(x, start = -6, end = -2)
  str_c(str_remove(x, " \\(p.*"), ifelse(o < .05, "*", ""))
}

oo <- mutate_all(oo, re_p)

oo %>% 
  t() %>% 
  knitr::kable(format = "latex", booktabs = TRUE, caption = "Results of mixed effects models with the interactions between interest and other characactistics and the composite for work with data", linesep = "") %>%
  kableExtra::kable_styling(latex_options = "scale_down") %>%
  kableExtra::landscape() %>% 
  group_rows("Youth characteristics", 2, 4) %>% 
  group_rows("Aspects of Work With Data", 5, 9)
```

## Results for Research Question #5: Youth characteristics and engagement

Like for the results for research question #4, each column is associated with the results for a single model, again in Table 4.5. For example, the first row is again associated with the results for the model predicting the probability of the *Only behavioral* profile, with the cells across the columns containing the coefficients, their standard errors, and their *p*-values. 

Yyouth who enter the program with higher levels of STEM interest are more likely to report being in the *engaged and competent but not challenged* profile ($\beta$ = 0.039, *p* = .009; *partial R^2* = .001). In other words, youth who are more interested at the outset of the program report working harder, learning more, enjoying themselves more, and feeling more competent when they are actually involved in a program activities, though they also report lower levels of challenge. For this effect, 17.879% would be needed to invalidate the inference, suggesting a moderately robust effect. Female students reported more often than males that they were in the *universally low engagement* profile, though this difference did not quite reach statistical significance ($\beta$ = 0.037, *p* = .051; *partial R^2* = .006). The effect size again suggest very small effects in substantive terms. For this effect, 17.843% of the bias would need to be removed (or the effect would need to be larger by this percentage) to sustain the inference. The moderately large amount of bias that would need to be removed for the effect of being female (on the *Universally low* profile) to be significant suggests that this effect should not be seriously interpreted.

These few, small findings were more surprising than the similarly minimal relations observed for work with data: as the null models indicate, there were large ICCs (a large proportion of the variability in the outcome variables) at the youth-level (as pre-interest, gender, and URM status were variables associated with this level). However it appears that the youth level variables of interest to this study were not effective at explaining much of this variability. This is discussed further in the next chapter.
