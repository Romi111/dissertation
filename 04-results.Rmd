# Results

In this section, results in terms of the research questions are presented.

```{r, setup-results-fix, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE, 
                      collapse = TRUE,
                      error = TRUE,
                      fig.width = 6,
                      fig.asp = .618,
                      out.width = "80%", 
                      fig.align = "center", 
                      results = "hold",
                      knitr.kable.na = '') 

```

```{r}
options(knitr.kable.NA = '')
```

```{r, loading-packages}
library(tidyverse)
library(lme4)
library(corrr)
library(jmRtools)
library(tidyLPA)
library(kableExtra)
library(sjPlot)
library(broom)
library(broom.mixed)
library(konfound)
source("helpers.R")
```

```{r, loading-data, eval = F}
esm <- read_csv("/Volumes/SCHMIDTLAB/PSE/data/STEM-IE/STEM-IE-esm.csv")
pre_survey_data_processed <- read_csv("/Volumes/SCHMIDTLAB/PSE/data/STEM-IE/STEM-IE-pre-survey.csv")
post_survey_data_partially_processed <- read_csv("/Volumes/SCHMIDTLAB/PSE/data/STEM-IE/STEM-IE-post-survey.csv")
video <- read_csv("/Volumes/SCHMIDTLAB/PSE/data/STEM-IE/STEM-IE-video.csv")
pqa <- read_csv("/Volumes/SCHMIDTLAB/PSE/data/STEM-IE/STEM-IE-pqa.csv")
attendance <- read_csv("/Volumes/SCHMIDTLAB/PSE/data/STEM-IE/STEM-IE-attendance.csv")
class_data <- read_csv("/Volumes/SCHMIDTLAB/PSE/data/STEM-IE/STEM-IE-class-video.csv")
demographics <- read_csv("/Volumes/SCHMIDTLAB/PSE/data/STEM-IE/STEM-IE-demographics.csv")
pm <- read_csv("/Volumes/SCHMIDTLAB/PSE/Data/STEM-IE/STEM-IE-program-match.csv")
```

```{r, loading-rdata}
# save.image("~/desktop/sandbox-01.Rdata")
load("~/desktop/sandbox-01.Rdata")
```

```{r, processing-attendance-demo-esm-data}
attendance <- rename(attendance, participant_ID = ParticipantID)
attendance <- mutate(attendance, prop_attend = DaysAttended / DaysScheduled, 
                     participant_ID = as.integer(participant_ID))
attendance <- select(attendance, participant_ID, prop_attend)

demographics <- filter(demographics, participant_ID!= 7187)
demographics <- left_join(demographics, attendance)

esm$overall_engagement <- jmRtools::composite_mean_maker(esm, hard_working, concentrating, enjoy, interest)
```

```{r, joining-to-df}
df <- left_join(esm, pre_survey_data_processed, by = "participant_ID") # df & post-survey
df <- left_join(df, video, by = c("program_ID", "response_date", "sociedad_class", "signal_number")) # df & video
df <- left_join(df, demographics, by = c("participant_ID", "program_ID")) # df and demographics
```

```{r, proc-beep-actvariables, echo = F}
df$participant_ID <- as.factor(df$participant_ID)
df$program_ID <- as.factor(df$program_ID)
df$beep_ID <- as.factor(df$beep_ID)
df$beep_ID_new <- as.factor(df$beep_ID_new)

df$youth_activity_rc <- ifelse(df$youth_activity == "Off Task", "Not Focused", df$youth_activity)

df$youth_activity_rc <- ifelse(df$youth_activity_rc == "Student Presentation" | df$youth_activity_rc == "Problem Solving", "Creating Product", df$youth_activity_rc)

df$youth_activity_rc <- ifelse(df$youth_activity_rc == "Showing Video", "Program Staff Led", df$youth_activity_rc)

df$youth_activity_rc <- as.factor(df$youth_activity_rc)

df$youth_activity_rc <- forcats::fct_relevel(df$youth_activity_rc, "Not Focused")

df$relevance <- jmRtools::composite_mean_maker(df, use_outside, future_goals, important)
```

```{r proc-demographics}
df$urm <- ifelse(df$race %in% c("White", "Asian"), 0, 1)
df$race <- as.factor(df$race)
df$race <- fct_lump(df$race, n = 2)
df$race_other <- fct_relevel(df$race, "Other")
df$gender_female <- as.factor(df$gender) # female is comparison_group
df$gender_female <- ifelse(df$gender_female == "F", 1, 
                           ifelse(df$gender_female == "M", 0, NA))
```

```{r, proc-pqa-data}
pqa <- mutate(pqa, 
              active = active_part_1 + active_part_2,
              ho_thinking = ho_thinking_1 + ho_thinking_2 + ho_thinking_3,
              belonging = belonging_1 + belonging_2,
              agency = agency_1 + agency_2 + agency_3 + agency_4,
              youth_development_overall = active_part_1 + active_part_2 + ho_thinking_1 + ho_thinking_2 + ho_thinking_3 + belonging_1 + belonging_2 + agency_1 + agency_2 + agency_3 + agency_4,
              making_observations = stem_sb_8,
              data_modeling = stem_sb_2 + stem_sb_3 + stem_sb_9,
              interpreting_communicating = stem_sb_6,
              generating_data = stem_sb_4,
              asking_questions = stem_sb_1,
              stem_sb = stem_sb_1 + stem_sb_2 + stem_sb_3 + stem_sb_4 + stem_sb_5 + stem_sb_6 + stem_sb_7 + stem_sb_8 + stem_sb_9)

pqa$sociedad_class <- ifelse(pqa$eighth_math == 1, "8th Math",
                             ifelse(pqa$seventh_math == 1, "7th Math",
                                    ifelse(pqa$sixth_math == 1, "6th Math",
                                           ifelse(pqa$robotics == 1, "Robotics",
                                                  ifelse(pqa$dance == 1, "Dance", NA)))))

pqa <- rename(pqa, 
              program_ID = SiteIDNumeric,
              response_date = resp_date,
              signal_number = signal)

pqa$program_ID <- as.character(pqa$program_ID)

df <- left_join(df, pqa, by = c("response_date", "program_ID", "signal_number", "sociedad_class"))
```

```{r, proc-vars-for-modeling}
df <- df %>% 
  mutate(dm_cog_eng = learning,
         dm_beh_eng = hard_working,
         dm_aff_eng = enjoy,
         dm_challenge = challenge,
         dm_competence = good_at) %>% 
  rename(ssb_predict = stem_sb_1,
         ssb_model = stem_sb_2 ,
         ssb_analyze = stem_sb_3,
         ssb_measure = stem_sb_4,
         ssb_tools = stem_sb_5,
         ssb_precision = stem_sb_6,
         ssb_vocabulary = stem_sb_7,
         ssb_classification = stem_sb_8,
         ssb_symbols = stem_sb_9) %>% 
  mutate(dm_ask = ssb_predict,
         dm_obs = ssb_classification,
         dm_gen = ifelse(ssb_measure == 1 | ssb_precision == 1, 1, 0),
         dm_mod = ssb_model,
         dm_com = ifelse(ssb_symbols == 1 | ssb_analyze == 1, 1, 0)) %>% 
  mutate(ov_cog_eng = (important + future_goals) / 2,
         ov_beh_eng = (hard_working + concentrating) / 2,
         ov_aff_eng = (enjoy + interest) / 2) %>% 
  mutate(dm_composite = dm_ask + dm_obs + dm_gen + dm_mod + dm_com,
         dm_composite_di = ifelse(dm_ask == 1 | dm_obs == 1 | dm_gen == 1 | dm_mod == 1 | dm_com == 1, 1, 0))

df$dm_overall_eng <- composite_mean_maker(df, dm_cog_eng, dm_beh_eng, dm_aff_eng)

df <- mutate(df, inquiry_based = ifelse(youth_activity_rc == "Creating Product" | youth_activity_rc == "Lab Activity", 1, 0),
             inquiry_based_three = ifelse(youth_activity_rc == "Creating Product" | youth_activity_rc == "Lab Activity", "inquiry-based",
                                          ifelse(youth_activity_rc == "Not Focused", "not-focused", "other"))) 
```

## Preliminary analysis

### Descriptive statistics for study variables

First, descriptive statistics for all of the study variables--overall pre-interest, the five variables that are used to estimate the PECs, and the variables for each of the five aspects of work with data (which are dichotomous variables)--are presented. Overall pre-interest and the variables used to estimate the PECs are presented first. The composite variable for instructional support for work with data was constructed as the sum of each of the five dichotomous variables that represented the aspects of instructional support for work with data; thus, its possible values ranged between zero and five. 

```{r}
d_red <- df %>% 
  group_by(participant_ID) %>% 
  mutate(rownum = row_number()) %>% 
  mutate(overall_pre_interest = ifelse(rownum == 1, overall_pre_interest, NA)) %>% 
  ungroup() %>% 
  select(-participant_ID)

o <- d_red %>% 
  select(overall_pre_interest, 
         dm_cog_eng,
         dm_beh_eng,
         dm_aff_eng,
         dm_challenge,
         dm_competence,
         dm_ask:dm_com, dm_composite) %>% 
  knitr::kable(format = "latex", booktabs = TRUE, caption = "Descriptive statistics for study variables", linesep = "", digits = 3) %>% 
  kableExtra::kable_styling(latex_options = "scale_down")
```

### Correlations among study variables

Next, correlations between individual aspects of work with data (and the composite) and the variables that are used to create the PECs are presented. These correlations suggest that the aspects of work with data are not related to the aspects of work with data to a large degree, which is not surprising given the small ICC values for the momentary level, as the aspects of work with data are associated with this level. Most noteworthy is the very small correlations between the aspects of work with data and the profiles; these correlations range (in absolute values) from .00 to .05. Only the relations between communicating and profile six are statistically significant. The composite variable was correlated with the profiles from (in absolute values) 0.002 to 0.035, none statistically significant. The aspects of work with data are modestly correlated with one another, with correlations ranging from .16 to .46; all were significant.  The Spearman rank correlations were also considered for all of the correlations that involved the individual aspects of instructional support for work with data; these are presented in the appendix because these were all within a .02 value (i.e., each Spearman's *rho* compared to its corresponding Pearson's *r* was within .02).

```{r, rq2-1-corr-components, eval = T}
d_red %>% 
  select(dm_ask:dm_com, dm_cog_eng:dm_competence) %>%
  corrr::correlate() %>% 
  corrr::shave() %>%
  corrr::fashion() %>% 
  knitr::kable(booktabs = TRUE, format = "latex", linesep = "", caption = "Correlations among study variables") %>% 
  kableExtra::kable_styling(latex_options = "scale_down") %>% 
  kableExtra::landscape()
```

```{r, rq2-1-corr-p, eval = F}
p_vals <- d %>% 
  select(dm_ask:dm_com, dm_composite,
         profile_1_p:profile_6_p,
         overall_pre_interest) %>%
  psych::corr.test() %>%
  pluck(4) %>% 
  round(3)
```

```{r, eval = FALSE}
dfs %>% 
  count(dm_ask) %>% 
  filter(!is.na(dm_ask)) %>% 
  mutate(prop = n / sum(n))

dfs %>% 
  count(dm_obs) %>% 
  filter(!is.na(dm_obs)) %>% 
  mutate(prop = n / sum(n))

dfs %>% 
  count(dm_gen) %>% 
  filter(!is.na(dm_gen)) %>% 
  mutate(prop = n / sum(n))

dfs %>% 
  count(dm_mod) %>% 
  filter(!is.na(dm_mod)) %>% 
  mutate(prop = n / sum(n))

dfs %>% 
  count(dm_com) %>% 
  filter(!is.na(dm_com)) %>% 
  mutate(prop = n / sum(n))

dfs %>% 
  count(dm_composite) %>% 
  filter(!is.na(dm_com)) %>% 
  mutate(prop = n / sum(n))
```

From the coding with the STEM-PQA, work with data appears common. Out of the 248 segments, 236 were coded for instructional support for work with data; for the other, not-coded segments, issues with the video-recordings were the primary source of the missing data; in these cases, students may have still replied to signals, but it was not possible to code for instructional support for work with data associated with these responses. 

```{r}
data_frame(Aspect = c("Asking Questions", "Making Observations", "Generating Data", "Data Modeling", "Communicating Findings"),
           Proportion = c(.389, .258, .453, .288, .470)) %>% 
    knitr::kable(booktabs = TRUE, linesep = "", caption = "Proportion of signals for which each of the aspects of work with data was present")
```

## Statistical software developed

The MPlus software is used to carry out LPA as part of this study. In order to more flexibly carry out LPA, an open-source tool, tidyLPA (Rosenberg, Schmidt, Beymer, & Steingut, 2018), was developed. This tool provides interfaces to both the MPlus software and to the open-source mclust software. In addition to being used as part of this study, this package is provided free of use to other analysts as the first tool dedicated to carrying out LPA as part of the R software. More details on the statistical software developed and included in the Appendix.

## Results for Research Question #2

This question addresses what profiles emerged from the data. This section first provides information about the statistical software that was developed and solutions for all models (whether models converged and the log-likelihood was replicated). Then, fit statistics for models that converged and for which the log-likelihood was replicated are described, followed by a comparison of specific, candidate solutions. At the end of this section, models selected are described in detail. Note that while the posterior probability was used as the outcome, there are two approaches to their use in subsequent analyses. One way is to only use the largest posterior probability, setting the other posterior probabilities to a value of zero; in this way, the uncertainty in the profile assignment is accounted for, but partial assignment to other profiles is not considered in their use in subsequent models. The other way is to use the posterior probabilities for all of the subsequent models. In this analysis, the latter option is used: posterior probabilities are used as-is (i.e., none are assigned to zero), though the former approach was used and was found to yield comparable results.

### Exploration of a wide range of models a

First, I examined a wide range of model types (i.e., the parameterization of the model) and the numbers of profiles. Note that six model types are able to be specified. These roughly became more complex, with additional parameters estimated, as the number for the model type increases from one to six.

This step is taken to select candidate solutions to investigate in more detail. In order to carry out this analysis, I followed guidelines recommended by the developers of the MPlus software (Asparouhov & Muthen, 2012; Muthen & Muthen, 2017) as well as those making recommendations about its use (Geiser, 2012). In particular, I set the number of starts to 600 for initial stage starts, and to 120 for the number of starts to be optimized. This means that for each model estimated, 600 random starting values for the parameters were used to initialize the EM algorithm. Of these 600, 120 that demonstrated the lowest log-likelihood were allowed to continue until they reached convergence or the limit for the number of iterations. In order for a model to me considered trustworthy, of these 120 runs, the lowest log-likelihood must be replicated at least one time. 

```{r, compare-solutions-overall-stats, eval = FALSE}
d <- compare_solutions_mplus(df,  
                             dm_cog_eng, dm_beh_eng, dm_aff_eng, dm_challenge, dm_competence,
                             starts = c(600, 120),
                             n_profiles_min = 2, 
                             n_profiles_max = 10,
                             return_stats_df = TRUE,
                             return_table = TRUE,
                             n_processors = 6, 
                             save_models = TRUE,
                             include_BLRT = TRUE)

write_rds(d, "data/overall-stats-for-all-models.rds")
```

```{r, printing-solutions-overall-stats, eval = F}
# d <- read_rds("data/overall-stats-for-all-models.rds")
overall_stats_for_all_models[[1]] %>% 
  knitr::kable(format = "latex", booktabs = TRUE, caption = "Overall statistics for all models", linesep = "") %>%
  kableExtra::kable_styling(latex_options = "scale_down") %>% 
  kableExtra::landscape()
```

If the log-likelihood is not replicated, then the estimation completed one or more times, but because the same log-likelihood value (and parameter estimates) were not obtained, then the solution can be considered to be "under-identified", a term used to describe solutions that depend strongly upon minor fluctuations in the data (Asparouhov & Muthen, 2007). Accordingly, these solutions may not represent meaningful values and may not be replicable in light of very small changes to the data; these are not considered as candidate solutions for use in subsequent analyses. If no log-likelihood is obtained for any of the random starts, then the software returns an error; in these cases, the convergence criteria--values that determine when a solution has been obtained--are not met. This may be due to a large number of parameters that are estimated relative to the data, such that the number of iterations that the estimation is allowed to go through are not sufficient to obtain a solution (Asparouhov & Muthen, 2007). Like when the log-likelihood is not replicated, these solutions are not considered for use in subsequent analyses.

For every combination of models one through six and from two through ten profiles, only solutions associated with model specifications 1 and 2 (and among these two solutions, only those associated with particular number of profiles) converged. Thus, only solutions associated with models 1 (the model with varying means, equal variances, and covariances fixed to zero) and model 2 (varying means, equal variances, and equal covariances) are explored in subsequent sections. This suggests that the more complex models were too complex given the systematic variability in the data used for the analysis.

### In-depth statistics for particular models

After investigating the general information about a range of model solutions, solutions associated with models 1 and 2 are explored in greater detail, following recommendations associated with mixture modeling (Collins and Lanza, 2009; Geiser, 2012) and the authors of the MPlus software (Muthen & Muthen, 2017) as well as recent peer-reviewed articles (Pastor et al., 2007). For these models, the log-likelihood (LL), a range of information criteria (AIC, BIC, sample adjusted BIC [SABIC], consistent AIC [CAIC]), statistics about the quality of the profile assignments (entropy, which represents the mean posterior probability) are presented. 

The information criteria are based on the log-likelihood but take various steps to penalize complex models, and so can be used to directly compare models (i.e., the model with the lowest values for these statistics can be considered to better reflect the underlying properties of the profiles). Simulation studies have suggested that BIC, CAIC, SABIC, and BLRT are most helpful for selecting the correct number of profiles (Nylund, Asparouhov, & Muthen, 2007). For the entropy statistic, higher values are considered better, though scholars have suggested that the entropy statistic not be used for model selection (Lubke & Muthen, 2007).The log-likelihood should not be interpreted directly but is presented in conjunction with the information criteria for context about how each of them differs from the log-likelihood. These are also presented in the figures.

In addition to these statistics, a number of modified likelihood ratio tests (LRTs) are used, as the test statistics associated with unmodified LRT do not follow the distribution that the test is based on (Muthen & Muthen, 2017). These are the Vu-Lo-Mendell-Rubin LRT, Lo-Mendell-Rubin LRT, and the bootstrapped LRT. Of the three, the bootstrapped is considered to be the best indicator of which of two models, one nested (with certain parameters fixed to 0) within the other, fits better, but it is also the most computationally-intensive to carry out (Asparouhov & Muthen, 2012). For each of the LRTs, the test statistic and its associated p-value are provided; a p-value greater than .05 suggests that the model with fewer profiles should be preferred.

```{r, reading-overall-stats}
overall_stats_for_all_models <- readr::read_rds("data/overall-stats-for-all-models.rds")
```

```{r, printing-solutions-spec-stats, eval = TRUE}
overall_stats_for_all_models[[2]] %>% 
  arrange(model, n_profile) %>% 
  select(-model, `Number of Profiles` = n_profile) %>% 
  mutate(VLMR = paste_stats(VLMR_val, VLMR_p),
         LMR = paste_stats(LMR_val, LMR_p),
         BLRT = paste_stats(BLRT_val, BLRT_p)) %>% 
  select(everything(), -VLMR_val, -VLMR_p, -LMR_val, -LMR_p, -BLRT_val, -BLRT_p) %>% 
  knitr::kable(format = "latex", caption = "Solutions for models that converged with replicated LL", booktabs = TRUE, linesep = "") %>% 
  kableExtra::kable_styling(latex_options = "scale_down") %>% 
  kableExtra::landscape() %>% 
  kableExtra::group_rows("Model 1", 1, 7) %>% 
  kableExtra::group_rows("Model 2", 8, 11)
```

```{r, model1, eval = T, cache = F, out.width = "50%", fig.cap = "Fit statistics for model 1 solutions"}
overall_stats_for_all_models[[2]] %>% 
  select(n_profile:CAIC, Entropy) %>% 
  filter(model == 1) %>% 
  mutate(AIC = AIC * -1,
         LL = LL * -1) %>% 
  gather(key, val, -n_profile, -model) %>% 
  ggplot(aes(x = n_profile, y = val)) +
  geom_point() +
  geom_line() +
  facet_grid(key ~ model, scales = "free") +
  theme_bw() +
  xlab("Number of Profiles") +
  ylab(NULL)
```

```{r, model2, eval = T, cache = F, out.width = "40%", fig.cap = "Fit statistics for model 2 solutions"}

overall_stats_for_all_models[[2]] %>% 
  select(n_profile:CAIC, Entropy) %>% 
  filter(model == 2) %>% 
  mutate(AIC = AIC * -1,
         LL = LL * -1) %>% 
  gather(key, val, -n_profile, -model) %>% 
  ggplot(aes(x = n_profile, y = val)) +
  geom_point() +
  geom_line() +
  facet_grid(key ~ model, scales = "free") +
  theme_bw() +
  xlab("Number of Profiles") +
  ylab(NULL)
```

Looking across the statistics presented, some general ideas about which models are to be preferred emerge. Solutions are interpreted first for each model individually and then across models with the goal of choosing a smaller number of models to investigate in more detail.

For solutions associated with model 1, the decrease (indicating a preferred model) in information criteria becomes smaller as the number of profiles increases from 5 to 6 and 6 to 7. A solution associated with 8 profiles did not replicate the log-likelihood and the VLMR and LMR suggest that the solution associated with 9 profiles did not fit better than that with 8 profiles, suggesting that models with 7 or fewer profiles be preferred. Considering these models, the entropy statistic increases by a large amount between the solution associated with 4 and 5 profiles (and then decreases slightly between 5 and 6 and 6 and 7 profile solutions), suggesting (but not providing conclusive evidence) that models 5, 6, or 7 may be preferred. The bootstrapped LRT suggests that, until the log-likelihood is not replicated, every more complex model be selected. Taking these pieces of evidence into conclusion, for model 1, solutions associated with 4 through 7 may be considered in more depth, with an emphasis on solutions associated with profiles with 5 and 6 profiles on the basis of the slowing of the decrease in the information criteria associated with the solutions with greater profiles than these, and the increase in the entropy from 4 to 5 (and 6) profile solutions.

For solutions associated with model 2, only those associated with 2-5 profile solutions were associated with log-likelihoods that were replicated. For these four models, the log-likelihood decreased in a mostly consistent way, such that changes in the decrease are not as evident as those associated with model 1. The entropy statistic decreases from 2 to 3 profile solutions, increases from 3 to 4 profile solutions, and then decreases slightly from 4 to 5 profile solutions, providing some information that models associated with 4 profiles be preferred to the others. All of the LRTs suggest that the more complex model be selected, not providing clear information about which solutions are to be preferred. On the basis of these pieces of evidence, models with 3, 4, and 5 solutions may be considered in more depth. However, there is a lack of consistent evidence favoring more or less complex models.

### Comparison of model 1 and model 2 type solutions

When looking across solutions, some overall patterns in terms of what profiles emerge and some directions for which models are to be selected for use in subsequent analysis can be identified. First, overall patterns are discussed. In the table, which profiles emerge from which solution is presented.

There is a wide range of profiles. Some appear very commonly, particularly those (full and universally low) characterized by high or low levels across all of the variables. Moderate profiles, both all moderate (characterized by moderately high levels across all of the variables) and moderately low (characterized by low levels across all of the variables), also appeared commonly, particularly for the solutions for model 1.

```{r, compare-profiles-by-solution}
d <- readxl::read_xlsx("tables/prof-assignments.xlsx")

d[, -1] %>% 
  knitr::kable(format = "latex", booktabs = TRUE, caption = "Profile assignments by LPA solution (models and numbers of profiles)", linesep = "", align=rep('c', 13)) %>% 
  kableExtra::kable_styling(latex_options = "scale_down") %>% 
  kableExtra::landscape() %>%
  kableExtra::row_spec(0, angle = -60) %>% 
  kableExtra::group_rows("Model 1", 1, 6) %>% 
  kableExtra::group_rows("Model 2", 7, 9)
```

### Examination of specific candidate models 

Following from the in-depth exploration of the candidate solutions, in this section, model solutions associated with specific model types and the number of profiles are investigated. In particular, the model one type, six profile, and model one type, seven profile solutions are described. Descriptions of other candidate solutions is included in the appendix. For all of the solutions, the raw data and the data that are centered to have a mean equal to 0 and a standard deviation of 1 (thus, the y-axis on each of the plots is labeled "Z-score").

#### Model type: 1, Profiles: 6

This solution is characterized by: 

- A *full* profile, profile 6
- An *universally low* profile, profile 2
- An *all moderate* profile, profile 5--and, like, the model 1, six profile solution--with moderate levels of affective engagement
- An *only behaviorally engaged* profile, profile 1, with moderate levels of behavioral engagement, very low affective engagement, and moderately (low) levels of cognitive engagement and challenge and competence
- An *only affectively engaged* profile, profile 4, with moderate levels of affective engagement, low levels of behavioral engagement, and moderately (low) levels of cognitive engagement and challenge and competence
- An *engaged and competent but not challenged* profile, profile 3, characterized by high levels of each of the three dimensions of engagement and of competence, but with low levels of challenge

The number of observations associated with each of the profiles is somewhat balanced, with the universally low profile with the largest number of observations (*n* = 667; the same number for this profile as in the model 1, five profile solution), followed by the all moderate profile (*n* = 638). Each of the other four profiles were associated with 300 to 400 observations. Unlike the model 1, four and five profile solutions, which distinguished observations on *either* a condition of engagement (i.e., competence) or one of its dimensions (i.e., cognitive, behavioral, and affective), this solution was associated with profiles that distinguished observations on the basis of both: There were profiles for only behaviorally and affectively engaged and for engaged and competent but not challenged. This solution is compelling because it appears to group students on the basis of multiple of the indicators, and demonstrate viability on the basis of the fit statistics (i.e., the tables and figure). The log-likelihood was replicated two times, with the next lowest log-likelihood not being replicated, followed by a log-likelihood that was replicated (at least) seven times. This solution (associated with the log-likelihood that was replicated [at least] seven times) could be investigated in further detail, to see whether--and if so, how--it differs from the solution interpreted here. This solution is a strong candidate for use in subsequent analyses. 

```{r, spec-solutions-m1_6, cache = FALSE, eval = FALSE, fig.width = 6, out.width = "100%"}
m1_6 <- estimate_profiles_mplus(df,  
                                dm_cog_eng, dm_beh_eng, dm_aff_eng, dm_challenge, dm_competence,
                                starts = c(600, 120),
                                model = 1,
                                n_profiles = 6,
                                include_BLRT=TRUE,
                                n_processors = 6, remove_tmp_files = FALSE)
write_rds(m1_6, "data/models/m1_6.rds")
```

```{r, m1_6p, cache = FALSE, eval = TRUE, fig.width = 7, fig.asp = .618, out.width = "95%", fig.align = "center"}
m1_6 <- read_rds("data/models/m1_6.rds")

p <- m1_6 %>% 
  plot_profiles_mplus(to_center = TRUE, to_scale = TRUE) +
  scale_x_discrete("", labels = c("Only behavioral (n = 370)",
                                  "Universally low (n = 667)",
                                  "Engaged and competent but not challenged (n = 450)",
                                  "Only affective (n = 345)",
                                  "All moderate (n = 638)",
                                  "Full (n = 488)")) +
  xlab(NULL) +
  ylab("Z-score") +
  viridis::scale_fill_viridis("", 
                              limits = c("DM_AFF_E", "DM_BEH_E", "DM_COG_E", "DM_CHALL", "DM_COMPE"),
                              labels = c("Affective", "Behavioral", "Cognitive", "Challenge", "Competence"), discrete = TRUE) +
  theme(plot.margin = margin(1, 0, 0, 1, "cm"))

p

m1_6 <- read_rds("data/models/m1_6.rds")

p <- m1_6 %>% 
  plot_profiles_mplus(to_center = FALSE, to_scale = FALSE) +
  scale_x_discrete("", labels = c("Only behavioral (n = 370)",
                                  "Universally low (n = 667)",
                                  "Engaged and competent but not challenged (n = 450)",
                                  "Only affective (n = 345)",
                                  "All moderate (n = 638)",
                                  "Full (n = 488)")) +
  xlab(NULL) +
  ylab("Value") +
  viridis::scale_fill_viridis("", 
                              limits = c("DM_AFF_E", "DM_BEH_E", "DM_COG_E", "DM_CHALL", "DM_COMPE"),
                              labels = c("Affective", "Behavioral", "Cognitive", "Challenge", "Competence"), discrete = TRUE) +
  theme(plot.margin = margin(1, 0, 0, 1, "cm"))

p
```

```{r, m1_6p-ll, eval = TRUE, cache = FALSE}
extract_LL_mplus() %>% slice(1:10) %>% knitr::kable()
```

#### Model type: 1, Profiles: 7

This solution is characterized by: 

- A *full* profile, profile 7
- A *universally low* profile, profile 1
- A *competent but not engaged or challenged* profile, profile 2, characterized by high competence and moderate (low) or low levels of engagement and challenge
- A *moderately low* profile, profile 3, characterized by moderately low levels of all of the variables
- A *challenged* profile, profile 4, characterized by high challenge, moderate (high) levels of engagement, and moderate (low) levels of competence
- A *highly challenged* profile, profile 5, characterized by patterns similar to those of the challenged profile, but with higher challenge and with low levels of both engagement and challenge
- A *challenged but not engaged or competent* profile, profile 6, characterized by low levels of challenge, and high levels of engagement and competence

The number of observations associated with each of the profiles is not very balanced, with few (*n* = 181) observations associated with the universally low profile and few (*n* = 222) observations associated with the highly challenged profile. The number of observations associated with the other profiles ranged from 317 to 651. Distinct from other solutions, none of the other five profiles were found in the other model 1 solutions. Two pairs of the profiles--challenged and highly challenged and universally low and moderately low--exhibited similar patterns among the variables that were distinguished by different mean levels. The log-likelihood was replicated twice, with the next lowest log-likelihood being replicate four times, possibly warranting further investigation. Taken together, this solution raises questions about whether it may be too complex, possibly suggesting preference for model 1 five and six profile solutions. 

```{r, spec-solutions-m1_7, cache = FALSE, eval = FALSE}
m1_7 <- estimate_profiles_mplus(df,  
                                dm_cog_eng, dm_beh_eng, dm_aff_eng, dm_challenge, dm_competence,
                                starts = c(600, 120),
                                model = 1,
                                n_profiles = 7,
                                include_BLRT=TRUE,
                                n_processors = 6, remove_tmp_files = FALSE)
write_rds(m1_7, "data/models/m1_7.rds")
```

```{r, m1_7p, cache = FALSE, eval = TRUE,  eval = FALSE, fig.width = 7, fig.asp = .618, out.width = "90%", fig.align = "center"}
m1_7 <- readr::read_rds("data/models/m1_7.rds")

m1_7 %>% 
  tidyLPA::plot_profiles_mplus(to_center = TRUE, to_scale = TRUE) +
  scale_x_discrete(labels = c("Universally low (n = 181)",
                              "Competent but not engaged or challenged (n = 317)",
                              "Moderately low (n = 651)",
                              "Challenged (n = 569)",
                              "Highly challenged (n = 222)",
                              "Engaged and competent but not challenged (n = 568)",
                              "Full (n = 450)")) +
  xlab(NULL) +
  ylab("Z-score") +
  viridis::scale_fill_viridis("", 
                              limits = c("DM_AFF_E", "DM_BEH_E", "DM_COG_E", "DM_CHALL", "DM_COMPE"),
                              labels = c("Affective", "Behavioral", "Cognitive", "Challenge", "Competence"), discrete = TRUE) +
  theme(plot.margin = margin(1, 0, 0, 1, "cm"))

m1_7 <- readr::read_rds("data/models/m1_7.rds")

m1_7 %>% 
  tidyLPA::plot_profiles_mplus(to_center = FALSE, to_scale = FALSE) +
  scale_x_discrete(labels = c("Universally low (n = 181)",
                              "Competent but not engaged or challenged (n = 317)",
                              "Moderately low (n = 651)",
                              "Challenged (n = 569)",
                              "Highly challenged (n = 222)",
                              "Engaged and competent but not challenged (n = 568)",
                              "Full (n = 450)")) +
  xlab(NULL) +
  ylab("Value") +
  viridis::scale_fill_viridis("", 
                              limits = c("DM_AFF_E", "DM_BEH_E", "DM_COG_E", "DM_CHALL", "DM_COMPE"),
                              labels = c("Affective", "Behavioral", "Cognitive", "Challenge", "Competence"), discrete = TRUE) +
  theme(plot.margin = margin(1, 0, 0, 1, "cm"))
```

```{r, m1_7-ll, cache = FALSE, eval = FALSE}
extract_LL_mplus() %>% slice(1:10) %>% knitr::kable()
```

#### Looking across model 1 and model 2 type solutions

The model 1, six and seven profile solutions are compelling because both show profiles that are distinguished by dimensions of engagement and its conditions (challenge and competence). Note that for this model, only the means and variances are estimated (and so no covariances are estimated), and the variances are constrained to be the same across the profiles. While this is a very restrictive model, it, along with the model 3 type (which did not lead to solutions for any of the numbers of profiles specified) also is a standard model for LPA, in that it meets the assumption of local independence (of the variables that make up the profiles--unlike for models in which covariances are estimated) typical common to LPA (see Muthen & Muthen, 2016). While some of the solutions associated with the model 2 type did reach solutions, these demonstrated less appealing properties in terms of their fit statistics as well as their interpretability and with respect to concerns of parsimony. Thus, while no covariances are estimated for the model 1 type solutions, there is no requirement that these be specified; their benefit, when models associated with them are preferred, is that they can provide better fit: they can be used to better explain or predict the data in a sample, but their inclusion also means that over-fitting the model to the data can become a greater concern.

For each solution, alternate solutions associated with higher log-likelihoods were explored. One advantage of the six profile solution is that most of its profiles can also be identified in solutions with fewer profiles. For the six profile solutions, this alternate solution was very different, whereas for the seven profile solutions, this alternate solution was highly similar. The model solutions exhibit a less clear pattern in terms of which profiles appear when. All else being equal, on the basis of parsimony, the model 1, six profile solution may be preferred and is selected for use in subsequent analyses.

As a type of sensitivity analysis focused on alternate model specifications (different from the kind described earlier for quantifying how robust an inference is to potential sources of bias or confounding variables, e.g. Frank, 2003), the model 1, seven profile solution is also explored, but results for it are included in the appendix. This model is less restrictive but does not meet the assumption of independence; some scholars refer to it, as such, as a general or Guassian mixture model solution, instead of an LPA solution (Bauer, 2004). Because covariances are estimated, relationships between the variables not captured in their mean levels estimated for each profile are also estimated. This suggests that these models may be modeling different relations between the variables than those associated with model 1 and that they may fit the data better, but they are also more complex and so should be interpreted with consideration these added parameters.

## Research Question #2: Relations Between Instructional Support for Work With Data and the PECs

Broadly, this question is focused on how instructional support for work with data, as coded from video-recordings of the programs, relates to the PECs. For the primary results for this question, linear models that account for the cross-classification of the moment and youth are used and for the "nesting" of both within each of the nine programs are used. For the outcome (*y* variable), the probability of a response belonging to the profile is used; thus, there are six models, for each of the six profiles, for each specification of the predictor (*x*) variables. 

Null models showing the proportion of variance (via the intra-class correlation) are interpreted. The more detailed results (in a table) are presented in the appendix. These are followed by the interpretation of findings related to a more variable-centered approach, namely, correlations between individual aspects of work with data and the composite and the profiles (and the variables that make them up) and individual interest. Finally, results of mixed effects models with the work with data variables added separate and then with the composite for instructional support for work with data are interpreted and presented. 

```{r}
df <- read_csv("data/for-seven-profiles.csv")
cc <- df %>% select(dm_cog_eng:dm_competence) %>% complete.cases()
C <- select(m1_6, C)

C_p_m <- select(m1_6, CPROB1:CPROB6) %>% 
  apply(MARGIN = 1, FUN = max)

C_p <- select(m1_6, CPROB1:CPROB6)

df_ss <- df[cc, ]

df_ss <- bind_cols(df_ss, C_p)
df_ss$profile <- C
df_ss$profile_p <- C_p_m

d <- df_ss %>% select(contains("dm"), participant_ID, program_ID, beep_ID = beep_ID_new, profile, profile_p, overall_pre_interest, youth_activity_rc, inquiry_based, inquiry_based_three, classroom_versus_field_enrichment, gender_female, urm, contains("cprob"), dm_composite_di)

d <- d %>% 
  mutate(
    profile_1 = ifelse(profile == 1, 1, 0),
    profile_2 = ifelse(profile == 2, 1, 0),
    profile_3 = ifelse(profile == 3, 1, 0),
    profile_4 = ifelse(profile == 4, 1, 0),
    profile_5 = ifelse(profile == 5, 1, 0),
    profile_6 = ifelse(profile == 6, 1, 0),
    profile_1_p = CPROB1,
    profile_2_p = CPROB2,
    profile_3_p = CPROB3,
    profile_4_p = CPROB4,
    profile_5_p = CPROB5,
    profile_6_p = CPROB6
  )
```

```{r, rq2-0-null, cache = FALSE, eval = TRUE}
m1 <- lmer(profile_1_p ~ 1 +
             (1 | participant_ID) +
             (1 | beep_ID) +
             (1 | program_ID),
           data = d)

m2 <- lmer(profile_2_p ~ 1 +
             (1 | participant_ID) +
             (1 | beep_ID) +
             (1 | program_ID),
           data = d)

m3 <- lmer(profile_3_p ~ 1 +
             (1 | participant_ID) +
             (1 | beep_ID) +
             (1 | program_ID),
           data = d)

m4 <- lmer(profile_4_p ~ 1 +
             (1 | participant_ID) +
             (1 | beep_ID) +
             (1 | program_ID),
           data = d)

m5 <- lmer(profile_5_p ~ 1 +
             (1 | participant_ID) +
             (1 | beep_ID) +
             (1 | program_ID),
           data = d)

m6 <- lmer(profile_6_p ~ 1 +
             (1 | participant_ID) +
             (1 | beep_ID) +
             (1 | program_ID),
           data = d)
```

```{r, rq2-1-all-vars-com-keep, cache = FALSE}
m1a <- lmer(profile_1_p ~ 1 +
              # gender_female +
              # urm + 
              dm_ask + dm_obs + dm_gen + dm_mod + dm_com +
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m2a <- lmer(profile_2_p ~ 1 +
              # gender_female +
              # urm + 
              dm_ask + dm_obs + dm_gen + dm_mod + dm_com +
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m3a <- lmer(profile_3_p ~ 1 +
              # gender_female +
              # urm + 
              dm_ask + dm_obs + dm_gen + dm_mod + dm_com +
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m4a <- lmer(profile_4_p ~ 1 +
              # gender_female +
              # urm + 
              dm_ask + dm_obs + dm_gen + dm_mod + dm_com +
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m5a <- lmer(profile_5_p ~ 1 +
              # gender_female +
              # urm + 
              dm_ask + dm_obs + dm_gen + dm_mod + dm_com +
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m6a <- lmer(profile_6_p ~ 1 +
              # gender_female +
              # urm + 
              dm_ask + dm_obs + dm_gen + dm_mod + dm_com +
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)
```

```{r, rq2-2-composite-keep, cache = FALSE}
m1b <- lmer(profile_1_p ~ 1 +
              dm_composite +
              # gender_female +
              # urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m2b <- lmer(profile_2_p ~ 1 +
              dm_composite +
              # gender_female +
              # urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m3b <- lmer(profile_3_p ~ 1 +
              dm_composite +
              # gender_female +
              # urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m4b <- lmer(profile_4_p ~ 1 +
              dm_composite +
              # gender_female +
              # urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m5b <- lmer(profile_5_p ~ 1 +
              dm_composite +
              # gender_female +
              # urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m6b <- lmer(profile_6_p ~ 1 +
              dm_composite +
              # gender_female +
              # urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)
```

```{r, rq2-2-composite-keep-2, cache = FALSE}
m1bi <- lmer(profile_1_p ~ 1 +
              dm_composite_di +
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m2bi <- lmer(profile_2_p ~ 1 +
              dm_composite_di +
              # gender_female +
              # urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m3bi <- lmer(profile_3_p ~ 1 +
              dm_composite_di +
              # gender_female +
              # urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m4bi <- lmer(profile_4_p ~ 1 +
              dm_composite_di +
              # gender_female +
              # urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m5bi <- lmer(profile_5_p ~ 1 +
              dm_composite_di +
              # gender_female +
              # urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m6bi <- lmer(profile_6_p ~ 1 +
              dm_composite_di +
              # gender_female +
              # urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)
```

### Null models

The null models presented in the table provide insight into the levels at which predictors may be able to explain the outcome. For all six profiles, the ICCs were very small, from 0.00 to 0.023. This suggests that very little variability can be explained simply by the program. For the momentary level, the ICCs were also very small, ranging from 0.004 to 0.011. Finally, the youth-level ICCs ranged from .099 to .427. Looking across these values, considering variability at the program, momentary, and youth levels, most of the explained variability in the responses is associated with youth; the program and momentary levels were associated with very small values, suggesting that variables at these levels have minimal variability that is able to be explained. In turn, this suggests that these variables, including those for instructional support for work with data, may not have strong effects in terms of their relations with the PECs. 

In terms of specific ICCs at the youth level, the value for the youth-level ICC was highest for the *full* profile, suggesting that some youth have a strong tendency to be fully engaged (possibly due to their initial interest or other individual characteristics and differences). The other profile characterized by a consistent pattern across all of the variables--the *universally low* profile--had a modest ICC, .265. Finally, a large amount of variability is associated with the residual (variance that is not associated with the program, momentary, or youth levels). This suggests that there is wide variation in students' responses that may not be readily explained or predicted. 

### Models with variables for aspects of instructional support for work with data added separately

When the predictor variables for work with data are added, some overall patterns and specific findings can be identified. The only relations with *p*-values that were below the criterion for statistical significance (.05) were for the relations between modeling data and the *full* profile (*B* = 0.036 (0.016), *p* = .016) and between generating data and the full profile (*B* = 0.029 (0.015), *p* = .024). 

Adding these variables changed the (conditional upon the random effects) r-squared values from, .002 to .018, very small changes suggesting that the aspects of work with data do not strongly predict the PECs. This is in-line with the correlations for these variables with those variables that make up the profiles, and the ICC values at the momentary level.

```{r-r2-1, eval = FALSE}
MuMIn::r.squaredGLMM(m1) # .108
MuMIn::r.squaredGLMM(m2) # .295
MuMIn::r.squaredGLMM(m3) # .324
MuMIn::r.squaredGLMM(m4) # .109
MuMIn::r.squaredGLMM(m5) # .268
MuMIn::r.squaredGLMM(m6) # .482

MuMIn::r.squaredGLMM(m1a) # .115
MuMIn::r.squaredGLMM(m2a) # .300
MuMIn::r.squaredGLMM(m3a) # .321
MuMIn::r.squaredGLMM(m4a) # .101
MuMIn::r.squaredGLMM(m5a) # .270
MuMIn::r.squaredGLMM(m6a) # .50
```

The sensitivity analysis for the effect of generating data suggested that 1.884% of the inference would have to be due to bias to invalidate the inference, suggesting that this effect is not very robust to potential sources of bias, such as an omitted (in this analysis) confounding (or control) variable. For the effect of modeling, 9.835% would need to be due to bias to invalidate the inference. This effect, then, is less sensitive to possible sources of bias, but is still not highly robust. 

```{r, rq2-1-tab, eval = FALSE}
l <- list(m1a, m2a, m3a, m4a, m5a, m6a)
o <- map_df(l, tidy_model)
write_rds(o, "data/rq2-1-tab.rds")
```

```{r, rq2-1-tab-pres}
o <- read_rds("data/rq2-1-tab.rds")

o <- mutate(o, model = c(
  str_c("profile_", 1:6)))

o %>% 
  select(model, 
         intercept = `(Intercept)`,
         dm_ask, dm_obs, dm_gen, dm_mod, dm_com,
         beep_ID_ICC = beep_ID_ICC,
         participant_ID_ICC,
         program_ID_ICC) %>% 
  mutate(model = c("Only behavioral",
                   "Universally low",
                   "Engaged and competent but not challenged",
                   "Only affective",
                   "All moderate",
                   "Full")) %>% 
  knitr::kable(format = "latex", booktabs = TRUE, caption = "Results of mixed effects models for instructional support for work with data as separate variables", linesep = "") %>% 
  kableExtra::kable_styling(latex_options = "scale_down") %>% 
  kableExtra::landscape()
```

```{r, sensitivity-analysis-for-rq1, eval = FALSE, cache = FALSE}
konfound::konfound(m6a, dm_gen)
konfound::konfound(m6a, dm_mod)
konfound::konfound(m6a, gender_female) # need to add
```

### Models with the composite added

For the composite of work with data, the composite predicted the profile for *only behavioral* (*B* = 0.007 (0.004), *p* = .021), but not any of the other profiles. However, this coefficient is very small in practical terms, and 12.261% would need to be due to bias to invalidate the inference. The change in r-squared values ranged from .003 to .020, suggesting minimal potential relations among factors (such as support for work with data as measured by the composite variable) at the momentary level. When the composite was treated as a dichotomous (instead of a continuous) variable, so that the variable takes a value of one if any of the aspects of work with data are present, the results are similar in terms of the magnitude of the effects and their significance, as none of the relations are statistically significant when the dichotomous variable is used.

```{r, sens-2, eval = FALSE, cache = FALSE}
konfound(m1b, dm_composite)
```

```{r-r2-2, eval = FALSE}
MuMIn::r.squaredGLMM(m1) # .108
MuMIn::r.squaredGLMM(m2) # .295
MuMIn::r.squaredGLMM(m3) # .324
MuMIn::r.squaredGLMM(m4) # .109
MuMIn::r.squaredGLMM(m5) # .268
MuMIn::r.squaredGLMM(m6) # .482

MuMIn::r.squaredGLMM(m1b) # .113
MuMIn::r.squaredGLMM(m2b) # .298
MuMIn::r.squaredGLMM(m3b) # .320
MuMIn::r.squaredGLMM(m4b) # .100
MuMIn::r.squaredGLMM(m5b) # .269
MuMIn::r.squaredGLMM(m6b) # .502
```

```{r, just-composite-mod, eval = FALSE}
l <- list(m1b, m2b, m3b, m4b, m5b, m6b)
o <- map_df(l, tidy_model)
write_rds(o, "data/comp-l.rds")
```

```{r, just-composite-red}
o <- read_rds("data/comp-l.rds")
o <- mutate(o, model = c(
  str_c("profile_", 1:6)))

o %>% 
  select(model, 
         intercept = `(Intercept)`,
         dm_composite,
         beep_ID_ICC = beep_ID_ICC,
         participant_ID_ICC,
         program_ID_ICC) %>% 
  mutate(model = c("Only behavioral",
                   "Universally low",
                   "Engaged and competent but not challenged",
                   "Only affective",
                   "All moderate",
                   "Full")) %>% 
  knitr::kable(format = "latex", booktabs = TRUE, caption = "Results of mixed effects models for the composite", linesep = "") %>% 
  kableExtra::kable_styling(latex_options = "scale_down") %>% 
  kableExtra::landscape()
```

```{r, just-composite-mod-2, eval = FALSE}
l <- list(m1bi, m2bi, m3bi, m4bi, m5bi, m6bi)
o <- map_df(l, tidy_model)
write_rds(o, "data/comp-l-2.rds")
```

```{r, just-composite-red-2, eval = FALSE}
o <- read_rds("data/comp-l-2.rds")
o <- mutate(o, model = c(
  str_c("profile_", 1:6)))

o %>% 
  select(model, 
         intercept = `(Intercept)`,
         dm_composite_di,
         beep_ID_ICC = beep_ID_ICC,
         participant_ID_ICC,
         program_ID_ICC) %>% 
  mutate(model = c("Only behavioral",
                   "Universally low",
                   "Engaged and competent but not challenged",
                   "Only affective",
                   "All moderate",
                   "Full")) %>% 
  knitr::kable(format = "latex", booktabs = TRUE, caption = "Results of mixed effects models for the composite", linesep = "") %>% 
  kableExtra::kable_styling(latex_options = "scale_down") %>% 
  kableExtra::landscape()
```

### Summary of findings for research question #2

When looking across findings, we find few relations between instructional support for work with data and the profiles, though there were notable effects of modeling, though they were small effects (i.e., when students are doing this, they are around 3% more likely to be responding in a way associated with the *full* profile). The composite for work with data had a relation of around 0.01 with the *only behavioral* profile, suggesting that for each one-value increase in the composite (which has a range from one to five), this profile is around 1% more likely. These findings are similar to those obtained when the model 1 type, seven profile solution is used for the outcome variables; see the appendix for more detail. Broadly, further explanations and investigations of these effects --focusing on the characteristics of instructional support for work with data in the context of summer STEM programs and how this support is measured in terms of codes from the video--are the focus on research question #4 and are discussed in the next chapter. Moreover, these findings are deepened in subsequent analyses for research questions #3.

## Results for Research Question #3

Research question #3 is focused on how the relationships of instructional support for work with data differ on the basis of pre-program interest and other youth characteristics. Like for the previous two research questions, linear models that account for the cross-classification of the moment and the youth--and their nesting within the programs--are used. Findings from models with pre interest, gender, and URM status are first presented. Then, models with these variable and the individual aspects and composite of work with data are added and then models with the interaction between these characteristics and the composite.

```{r, rq3-2-all-vars-sep-interaction-keep, eval = TRUE}
m1c <- lmer(profile_1_p ~ 1 +
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m2c <- lmer(profile_2_p ~ 1 +
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m3c <- lmer(profile_3_p ~ 1 +
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m4c <- lmer(profile_4_p ~ 1 +
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m5c <- lmer(profile_5_p ~ 1 +
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m6c <- lmer(profile_6_p ~ 1 +
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)
```

```{r, rq3-2-all-vars-com-keep, eval = TRUE}
m1d <- lmer(profile_1_p ~ 1 +
              dm_composite + 
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m2d <- lmer(profile_2_p ~ 1 +
              dm_composite + 
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m3d <- lmer(profile_3_p ~ 1 +
              dm_composite + 
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m4d <- lmer(profile_4_p ~ 1 +
              dm_composite + 
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m5d <- lmer(profile_5_p ~ 1 +
              dm_composite + 
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m6d <- lmer(profile_6_p ~ 1 +
              dm_composite + 
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)
```

```{r, rq3-2-all-vars-sep-keep, eval = TRUE}
m1di <- lmer(profile_1_p ~ 1 +
              dm_ask + dm_obs + dm_gen + dm_mod + dm_com +
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m2di <- lmer(profile_2_p ~ 1 +
              dm_ask + dm_obs + dm_gen + dm_mod + dm_com +
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m3di <- lmer(profile_3_p ~ 1 +
              dm_ask + dm_obs + dm_gen + dm_mod + dm_com +
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m4di <- lmer(profile_4_p ~ 1 +
              dm_ask + dm_obs + dm_gen + dm_mod + dm_com +
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m5di <- lmer(profile_5_p ~ 1 +
              dm_ask + dm_obs + dm_gen + dm_mod + dm_com +
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m6di <- lmer(profile_6_p ~ 1 +
              dm_ask + dm_obs + dm_gen + dm_mod + dm_com +
              overall_pre_interest +
              gender_female +
              urm + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)
```

```{r, rq3-2-all-vars-interaction-inq-keep, eval = TRUE}
m1e <- lmer(profile_1_p ~ 1 +
              overall_pre_interest*dm_composite + 
              gender_female*dm_composite +
              urm*dm_composite + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m2e <- lmer(profile_2_p ~ 1 +
              overall_pre_interest*dm_composite + 
              gender_female*dm_composite +
              urm*dm_composite + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m3e <- lmer(profile_3_p ~ 1 +
              overall_pre_interest*dm_composite + 
              gender_female*dm_composite +
              urm*dm_composite + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m4e <- lmer(profile_4_p ~ 1 +
              overall_pre_interest*dm_composite + 
              gender_female*dm_composite +
              urm*dm_composite + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m5e <- lmer(profile_5_p ~ 1 +
              overall_pre_interest*dm_composite + 
              gender_female*dm_composite +
              urm*dm_composite + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)

m6e <- lmer(profile_6_p ~ 1 +
              overall_pre_interest*dm_composite + 
              gender_female*dm_composite +
              urm*dm_composite + 
              (1 | participant_ID) +
              (1 | beep_ID) +
              (1 | program_ID),
            data = d)
```

### Models with pre interest, gender, and under-represented minority (URM) status

These results show that overall pre-interest is associated with the *engaged and competent but not challenged* profile (*B* = 0.039 (0.021), p = .009). The effect of being a female has a relation of 0.059 (0.036, p = .054) upon the probability of a response being associated with the *universally low* profile; though this effect did not meet the criteria for statistical significance, sensitivity analysis to determine how much more robust the effect would need to be to make an inference. For the effect of overall pre-interest upon the *engaged and competent but not challenged* profile, 17.879% would be needed to invalidate the inference, suggesting a moderately robust effect. For the effect of gender upon the *universally low* profile, 16.996% of the bias would need to be removed (or the effect would need to be larger by this percentage) to sustain the inference. The change in r-squared values ranged from .004 to .007, suggesting that pre-interest and other individual characteristics - in addition to the aspects of work with data - have minimal relations with the PECs. Thai is more surprising than the similarly minimal relations observed for work with data: as the null models indicate, there were large ICCs (a large proportion of the variability in the outcome variables) at the youth-level (as pre-interest, gender, and URM status are variables associated with this level) This is discussed further in the next chapter.

```{r-r2-3, eval = FALSE}
MuMIn::r.squaredGLMM(m1) # .108
MuMIn::r.squaredGLMM(m2) # .295
MuMIn::r.squaredGLMM(m3) # .324
MuMIn::r.squaredGLMM(m4) # .109
MuMIn::r.squaredGLMM(m5) # .268
MuMIn::r.squaredGLMM(m6) # .482

MuMIn::r.squaredGLMM(m1c) # .112
MuMIn::r.squaredGLMM(m2c) # .302
MuMIn::r.squaredGLMM(m3c) # .328
MuMIn::r.squaredGLMM(m4c) # .112
MuMIn::r.squaredGLMM(m5c) # .275
MuMIn::r.squaredGLMM(m6c) # .487
```

```{r, md-block-for-rq4, eval = FALSE}
l <- list(m1c, m2c, m3c, m4c, m5c, m6c)
o <- map_df(l, tidy_model)
write_rds(o, "data/md-block-for-rq4")
```

```{r, reading-for-rq4, eval = TRUE}
o <-read_rds("data/md-block-for-rq4")

o <- mutate(o, model = c(
  str_c("profile_", 1:6)))

o %>% 
  select(model, 
         intercept = `(Intercept)`,
              overall_pre_interest,
              gender_female,
              urm,
         beep_ID_ICC = beep_ID_ICC,
         participant_ID_ICC,
         program_ID_ICC) %>% 
    mutate(model = c("Only behavioral",
                   "Universally low",
                   "Engaged and competent but not challenged",
                   "Only affective",
                   "All moderate",
                   "Full")) %>% 
  knitr::kable(format = "latex", booktabs = TRUE, caption = "Results of mixed effects models with interest and other characteristics", linesep = "") %>% 
  kableExtra::kable_styling(latex_options = "scale_down") %>% 
  kableExtra::landscape()
```

```{r, sensitivity-analysis-for-rq2a, eval = FALSE, cache = FALSE}
konfound::konfound(m3c, overall_pre_interest)
konfound::konfound(m2c, gender_female)
```

### Models with pre interest, gender, and URM status and the aspects of work with data

These results show very similar patterns to those observed in the models with pre-interest and the other individual characteristics and the models with the aspects of work with data separate. Like in the models with only pre-interest and the other individual characteristics, pre-interest is related to the *only behavioral* profile (*B* = 0.039 (0.016), p = .009). Being female is again related but not to a level that it meets the criteria for statistical significance (*B* = 0.06 (0.037), p = .051). Generating data (*B* = 0.027 (0.015), p = .033) and modeling data (*B* = 0.034 (0.017), p = .020) were both related to the *full* profile to a similar extent and with similar robustness as found in the separate models. Compared to the null models, the r-squared values changed from .001 to .029, suggesting small improvements from the additions of the individual characteristics and the codes for the aspects of work with data.

```{r-r2-2i, eval = FALSE}
MuMIn::r.squaredGLMM(m1) # .108
MuMIn::r.squaredGLMM(m2) # .295
MuMIn::r.squaredGLMM(m3) # .324
MuMIn::r.squaredGLMM(m4) # .109
MuMIn::r.squaredGLMM(m5) # .268
MuMIn::r.squaredGLMM(m6) # .482

MuMIn::r.squaredGLMM(m1di) # .119
MuMIn::r.squaredGLMM(m2di) # .307
MuMIn::r.squaredGLMM(m3di) # .328
MuMIn::r.squaredGLMM(m4di) # .100
MuMIn::r.squaredGLMM(m5di) # .276
MuMIn::r.squaredGLMM(m6di) # .511
```

```{r, pre-int-ind, eval = FALSE}
l <- list(m1di, m2di, m3di, m4di, m5di, m6di)
o <- map_df(l, tidy_model)
write_rds(o, "data/pre-int-ind-interaction.rds")
```

```{r, pre-int-ind-present, eval = TRUE}
o <- read_rds("data/pre-int-ind-interaction.rds")
o <- mutate(o, model = c(
  str_c("profile_", 1:6)))

o %>% 
  select(model, 
         intercept = `(Intercept)`,
         dm_ask, dm_obs, dm_gen, dm_mod, dm_com,
         overall_pre_interest,
         gender_female,
         urm,
         beep_ID_ICC = beep_ID_ICC,
         participant_ID_ICC,
         program_ID_ICC) %>% 
  mutate(model = c("Only behavioral",
                   "Universally low",
                   "Engaged and competent but not challenged",
                   "Only affective",
                   "All moderate",
                   "Full")) %>% 
  knitr::kable(format = "latex", booktabs = TRUE, caption = "Results of mixed effects models with interest and other characteristics and the aspects of work with data", linesep = "") %>% 
  kableExtra::kable_styling(latex_options = "scale_down") %>% 
  kableExtra::landscape()
```

### Models with pre-interest, gender, and URM status and work with data composite

Like for the individual aspects, these models with the composite for work with data instead of the individual aspects. These results show very similar patterns to those observed in the models with pre-interest and the other individual characteristics and the models with the aspects of work with data separate. Like in the models with only pre-interest and the other individual characteristics alone (and like in the model with the individual aspects), pre-interest is related to the *only behavioral* profile (*B* = 0.039 (0.016), p = .009). Being female is again related but not to a level that it meets the criteria for statistical significance (*B* = 0.06 (0.037), p = .052). The composite was significantly related to the *only behavioral* profile (*B* = 0.007 (0.004), p = .027) to a similar extent and with similar robustness as found in the separate model. Compared to the null models, the r-squared values changed from .008 to .026, once again suggesting small improvements from the additions of the individual characteristics and the composite for the aspects of work with data.

```{r-r2-2iii, eval = FALSE}
MuMIn::r.squaredGLMM(m1) # .108
MuMIn::r.squaredGLMM(m2) # .295
MuMIn::r.squaredGLMM(m3) # .324
MuMIn::r.squaredGLMM(m4) # .109
MuMIn::r.squaredGLMM(m5) # .268
MuMIn::r.squaredGLMM(m6) # .482

MuMIn::r.squaredGLMM(m1d) # .118
MuMIn::r.squaredGLMM(m2d) # .307
MuMIn::r.squaredGLMM(m3d) # .327
MuMIn::r.squaredGLMM(m4d) # .099
MuMIn::r.squaredGLMM(m5d) # .276
MuMIn::r.squaredGLMM(m6d) # .508
```

```{r, pre-int, eval = FALSE}
l <- list(m1d, m2d, m3d, m4d, m5d, m6d)
o <- map_df(l, tidy_model)
write_rds(o, "data/pre-int-no-interaction.rds")
```

```{r, sensitivity-analysis-for-rq2b, eval = FALSE, cache = FALSE}
konfound::konfound(m3d, overall_pre_interest)
```

```{r, pre-int-int-present, eval = TRUE}
o <- read_rds("data/pre-int-no-interaction.rds")
o <- mutate(o, model = c(
  str_c("profile_", 1:6)))

o %>% 
  select(model, 
         intercept = `(Intercept)`,
         dm_composite,
         overall_pre_interest,
         gender_female,
         urm,
         beep_ID_ICC = beep_ID_ICC,
         participant_ID_ICC,
         program_ID_ICC) %>% 
  mutate(model = c("Only behavioral",
                   "Universally low",
                   "Engaged and competent but not challenged",
                   "Only affective",
                   "All moderate",
                   "Full")) %>% 
  knitr::kable(format = "latex", booktabs = TRUE, caption = "Results of mixed effects models with interest and other characteristics and the composite work with data", linesep = "") %>% 
  kableExtra::kable_styling(latex_options = "scale_down") %>% 
  kableExtra::landscape()
```

### Models with interactions between pre interest, gender, and URM status and work with data composite

These results show similar patterns to the earlier models.Like in the models with only pre-interest and the other individual characteristics alone (and like in the model with the individual aspects), pre-interest is related to the *only behavioral* profile (*B* = 0.033 (0.018), p = .033). Being female is again related but not to a level that it meets the criteria for statistical significance (*B* = 0.064 (0.041), p = .059). With the interactions added, the composite was no significantly related to the *only behavioral* profile (*B* = 0.016 (0.016), p = .156) to a similar extent and with similar robustness as found in the separate model. One interaction, between pre-interest and being female, had a significant effect upon the profile for *full* engagement (*B* = 0.012 (0.006), p = .026). However, only 1.953% of the effect would need to be due to bias to invalidate the inference. The r-squared values, relative to the models with only random effects (the null models), increased from .003 to .028, again suggesting small effects of the predictors upon the PECs.

```{r-r2-2ii, eval = FALSE}
MuMIn::r.squaredGLMM(m1) # .108
MuMIn::r.squaredGLMM(m2) # .295
MuMIn::r.squaredGLMM(m3) # .324
MuMIn::r.squaredGLMM(m4) # .109
MuMIn::r.squaredGLMM(m5) # .268
MuMIn::r.squaredGLMM(m6) # .482

MuMIn::r.squaredGLMM(m1e) # .118
MuMIn::r.squaredGLMM(m2e) # .307
MuMIn::r.squaredGLMM(m3e) # .327
MuMIn::r.squaredGLMM(m4e) # .102
MuMIn::r.squaredGLMM(m5e) # .276
MuMIn::r.squaredGLMM(m6e) # .510
```

```{r, pre-int-interactions, eval = FALSE}
l <- list(m1e, m2e, m3e, m4e, m5e, m6e)
o <- map_df(l, tidy_model)
write_rds(o, "data/pre-int-interaction.rds")
```

```{r, sensitivity-analysis-for-rq2c, eval = FALSE, cache = FALSE}
konfound::konfound(m6e, `dm_composite:gender_female`)
```

```{r, pre-int-comp, eval = TRUE}
o <- read_rds("data/pre-int-interaction.rds")
o <- mutate(o, model = c(
  str_c("profile_", 1:6)))

o %>% 
  select(model, 
         intercept = `(Intercept)`,
         dm_composite,
         overall_pre_interest,
         gender_female,
         urm,
         `overall_pre_interest:dm_composite`,
         `dm_composite:gender_female`,
         `dm_composite:urm`,
         beep_ID_ICC = beep_ID_ICC,
         participant_ID_ICC,
         program_ID_ICC) %>% 
  mutate(model = c("Only behavioral",
                   "Universally low",
                   "Engaged and competent but not challenged",
                   "Only affective",
                   "All moderate",
                   "Full")) %>% 
  knitr::kable(format = "latex", booktabs = TRUE, caption = "Results of mixed effects models with the interactions between interest and other characactistics and the composite for work with data", linesep = "") %>% 
  kableExtra::kable_styling(latex_options = "scale_down") %>% 
  kableExtra::landscape()
```

### Summary of findings for research question #3

When looking across findings, we find minimal relations between pre-interest and other individual characteristics. In particular, we found that pre-interest was related to the *engaged and comptent but not challenged* profile to a modest extent. Being female did not demonstrate statistically significant relations with the *univerally low* profile, though some moderately-sized effects that were nearly statistically significant were observed and interpreted in terms of how much bias would need to be reduced (or how much the larger the effect would need to be) in order for this relation to be statistically significant. These results, like those for research question #2, are similar to those obtained when the model 1 type, seven profile solution is used for the outcome variables. There were few interactive effects observed; the magnitude of the effect of the composite and gender interaction was small (as were the changes in the r-squared value as a consequence of adding this interaction), and the effect appears to not be highly robust to potential sources of bias. Like for research question #2, reasons for why this may be are explored in the next chapter. The effect of the activity appears robust, as in research question #3

## Results for Research Question #4

```{r, loading-spreadsheets, eval = FALSE}
library(googlesheets)
library(dplyr)

g1 <- gs_title("USE THIS! New Coding Frame - KMS")
g2 <- gs_title("USE THIS! New Coding Frame - HM")
g3 <- gs_title("USE THIS! New Coding Frame - KS")

d1 <- gs_read(g1, ws = 1)
d2 <- gs_read(g2, ws = 1)
d3 <- gs_read(g3, ws = 1)

d1 <- dplyr::select(d1, program_name, response_date, signal_number, KMS_qual = `Qualitative Coding`)
d2 <- dplyr::select(d2, program_name, response_date, signal_number, HM_qual = `Qualitative Coding`)
d3 <- rename(d3, KLS_qual = `Qualitative Coding`)

d3 <- d3 %>%
  left_join(d2) %>% 
  left_join(d1)

d_proc <- d3 %>% 
  select(everything(), contains("qual"), -`Josh notes`, -Initials)

# d1i <- dplyr::pull(d1, `Qualitative Coding`)[!d1_na]
# d2i <- dplyr::pull(d2, `Qualitative Coding`)[!d2_na]
# d3i <- dplyr::pull(d3, `Qualitative Coding`)[!d3_na]

readr::write_csv(d_proc, "qual-coding.csv")
```

```{r, read-proc-sheet, eval = F}
qual_sheets <- readr::read_csv("qual-coding.csv")
```

To code the data, three research assistants were trained for approximately eight hours over four meetings. Then, each research assistant coded all of the segments associated with one of the videos. After the coding was complete, the three research assistants and I met to discuss how well the coding frame and potential sources of disagreement. Then, two coders coded every segment that was coded for at least one of the aspects of instructional support for work with data. This coding took around 75 hours of coding by the research assistants. After each program, the coders met to discuss potential issues that emerged throughout the coding, and to clarify how they applied the coding frame. As this was open-ended coding with the aim to provide greater detail and context for the findings associated with research questions #2 and #3, establishing reliability among the coders was not carried out. The coders sought to document a) the characteristics of instructional support for work with data and b) other aspects of the instructional context that impacts student work with data.

Note that while the first of the two aspects focuses on the support provided by the instructor, the second aspect focuses on how students engage in work with data in ways that on occasion diverge (in ways productive and not productive in terms of student work with data) from what would be expected on the basis of the instructional support. This coding resulted in around three to four sentence notes associated with each segment from each of two raters. Then, I reviewed these notes with the aim to identify themes based on enriching and better understanding the findings for research questions #2-#4 and, beyond these findings, to better understand the nature of work with data in summer STEM programs.

### Affordances and constraints of summer STEM programs for work with data

Summer STEM program have affordances and constraints work with data. Thus, different from the previous theme that was focused on a study-related issue, this theme concerns differences in the nature of the instruction and learning opportunities that learners experienced as part of their time in the summer STME programs.

#### Affordances

Affordances included the community setting and the relevance of the program to youth's lives.

For example, in the *Marine Investigators*, youth participated in activities designed to help them understand water quality in their ecosystem. Youth collected trash from sites around their community (in different "districts") and then brought the trash and recyclable plastic back to the classroom. Then, the youth activity leaders asked students to figure out how much plastic enters local waterways. As a part of this activity, youth activity leaders asked students not only to determine the quantity of trash that entered the waterways, but asked students about *why* they used math in particular ways (i.e., adding the quantity of trash collected and then extrapolating from this quantity to the amount from across the entire city over the course of the year). This appeared to be a powerful activity, one that was coded as involving all five aspects of work with data according to the measures for instructional support for work with data; this type of activity seemed to suggest that instructional support for work with data may impact youth's engagement.

Another affordance concerned the relevance of the program to youth's lives. For example, in the *Building Mania* program, youth are involved in engineering design (i.e., identifying a problem and designing a solution), particularly around the use of simple machines. In a day in the classroom setting, youth are creating, testing, and revising catapults. In the next day, youth visit an area University, and are led in a discussion by a physicist who works with particle colliders. In this example, the expertise of the physicist, who explicitly mentions the benefits of engaging in the engineering design process and the importance of combining engineering to addressing problems (such as mitigating the damage of earthquakes), seems to be highly relevant to what youth are doing in their class. In these two days of class, youth are engaged in different aspects of work with data as indicated by the codes for instructional support for work with data (collecting data on the efficacy of their designs in the classroom day, and asking questions in the subsequent day, particularly); these seem to suggest, like the example of work work with data from the *Marine Investigators* program, affordances of work with data for summer STEM programs.

#### Constraints

Constraints included the challenge of linking activities as a part of a complete cycle of investigation and an emphasis on different aspects of work with data as part of programming.

Youth activity leaders faced challenges linking activities as part of a complete cycle of investigation. For example, in the *Ecosphere* program, youth collected water samples in the field. They then brought these samples to the classroom and tested the water, involving students in both collecting and, to a degree, generating data (by noting the pH levels of the water). However, later in the day, youth created a small-scale model (with inclined trays of dirt, rocks, and plants) of an ecosystem, in which they added food coloring to determine the impacts of chemicals and acid rain. Youth then interpreted and discussed these findings, but did not connect the discussion to the water samples youth collected and tested earlier. This activity presented an opportunity for deeper engagement, in which youth could interpret and communicate findings related to the state of the water in their ecosystem, but, instead, it was potentially limiting in terms of youth's engagement in work with data.

A theme related to the challenge of linking activities concerned what the programs focused on. For example, the mathematics-focused programs, such as the *Adventures in Mathematics* program, the youth activity leaders recognizing that youth had difficulty solving equations, used duct tape and a "hippity hoppity", building on an earlier activity in which youth considered what constituted a rate, on how many "hops" it would take someone to move from one end of the line of duct tape to the other; the youth activity leader than asked youth to consider how far they could move in one hop and to consider how they could find out many hops it would take, using a mathematical equation. In this activity, youth were supported to approach mathematics problem-solving in creative ways. However, apart from data modeling, other aspects of work with data were rarely present, and most of the data that youth worked with was provided by the teacher or considered in the abstract. Programs focused on science or engineering, similarly, emphasized other aspects of work with data: The science-focused programs (*Island Explorers*, *The Ecosphere*, and *Marine Investigators*) all emphasized collecting and generating data, but data, particularly the data collected or generated, was rarely modeled or interpreted. In the engineering-focused programs (*Uptown Architecture*, *Crazy Machines*, and *Dorchester House*, youth often collected data that resulted from their engineering designs, and communicated and interpreted their findings, but, did not generate data, and, accordingly, (and like the science-focused programs) did not model data as a regular part of their activities. This finding suggests that while work with data may have been common overall, different aspects of instructional support for work with data were emphasized to different degrees based on the focus of the program.

<!-- A final constraint Regarding emphasizing outcomes other than the capacity to work with data, the programs emphasized outcomes such as youth's confidence in their learning, planning skills, and ability to collaborate. These outcomes are complimentary to building youth's ability to work with data, but at the same time, they meant that over the approximately four weeks of the program, limited time was dedicated to work with data ... Related, on occasion, youth demonstrated a reluctance to engage in what they perceived as "school" activities. -->

### How instructional support for work with data was measured

This theme concerned how instructional support was measured and how this impacted the findings presented in research questions #2 and #3. As an example, in a video associated with a mathematics-focused activity in the Comunidad de Aprendizaje program, a youth activity leader is discussing with youth opportunities for them to market products that they developed to sell in their communities and highlighting the expense of creating the product, its sale price, and its potential process. In this example, observing data is coded, but this aspect of instructional support for work with data does not appear to be present. Considering the STEM-PQA code on which the code for making observations is based, this difference is possibly due to a distinction in what both codes are focused on. The STEM-PQA code is for *classifying or abstracting*, and its operationalization emphasizes staff supporting youth in linking concrete examples to principles, categories, or formulas. The conceptual definition of *making observations*, though, emphasizes watching and noticing what is happening with respect to the phenomena being investigated. In this case, the application of the STEM-PQA code was sensible, as the youth activity leader was connecting the products youth created to mathematical ideas (formulas) for how much they could expect to earn from the sale of their products; in terms of work with data, however, youth were not observing or noticing phenomena. This suggests that differences in how work with data was conceptualized and operationalized may lead, in some cases, to codes that do not reflect work with data accurately, and can lead to some findings that seem unexpected given what we know about the potential for work with data to be engaging to youth.

### Summary of Findings for Research Question #4

These findings focused on the affordances and constrained of work with data in summer STEM programs and how instructional support for work with data was measured. Broadly, the qualitative analysis suggested possible explanations for the findings for research questions #2 and #3. For these questions, little variability was found to exist at the momentary level, and the predictors at the momentary level (instructional support for work with data) and at the youth level (pre-interest, gender, and URM status) demonstrated modest relations with the profiles. These relations can be due to a variety of reasons, particularly 1) how the variables for the PECs and how instructional support for work with data is measured, and 2) how suitable of summer STEM programs for work with data. Accordingly, this analysis resulted in findings organized around the following two themes. The first theme concerned *affordances and constraints of summer STEM programs for work with data*. The second concerned *howinstructional support for work with data was measured*. Both are described in the remainder of this section. Another possible explanation related to whether PECs and the variables that make them up are appropriate outcomes, and how the PECS are measured, is an important question, but one that cannot readily be assessed from the video data that was analyzed; however, this topic is explored in the next chapter.
