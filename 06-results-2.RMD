# Results for Research Question #2

```{r, setup-results-2, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE, 
                      collapse = TRUE,
                      error = TRUE,
                      fig.width = 6,
                      fig.asp = .618,
                      out.width = "80%", 
                      fig.align = "center", 
                      results = "hold",
                      knitr.kable.na = '') 

```

## Statistical software developed

The MPlus software is used to carry out LPA as part of this study. In order to more flexibly carry out LPA, an open-source tool, tidyLPA (Rosenberg, Schmidt, Beymer, & Steingut, 2018), was developed. This tool provides interfaces to both the MPlus software and to the open-source mclust software. In addition to being used as part of this study, this package is provided free of use to other analysts as the first tool dedicated to carrying out LPA as part of the R software. More details on the statistical software developed and included in the Appendix.


## Results for Research Question #2: What profiles of youth engagement and its conditions emerge from experiential data collected in the programs?

This question addresses what profiles emerged from the data. This section first provides information about the statistical software that was developed and solutions for all models (whether models converged and the log-likelihood was replicated). Then, fit statistics for models that converged and for which the log-likelihood was replicated are described, followed by a comparison of specific, candidate solutions. At the end of this section, models selected are described in detail. Note that while the posterior probability was used as the outcome, there are two approaches to their use in subsequent analyses. One way is to only use the largest posterior probability, setting the other posterior probabilities to a value of zero; in this way, the uncertainty in the profile assignment is accounted for, but partial assignment to other profiles is not considered in their use in subsequent models. The other way is to use the posterior probabilities for all of the subsequent models. In this analysis, the latter option is used: posterior probabilities are used as-is (i.e., none are assigned to zero), though the former approach was used and was found to yield comparable results.

### Exploration of a wide range of models a

First, I examined a wide range of model types (i.e., the parameterization of the model) and the numbers of profiles. Note that six model types are able to be specified. These roughly became more complex, with additional parameters estimated, as the number for the model type increases from one to six.

This step is taken to select candidate solutions to investigate in more detail. In order to carry out this analysis, I followed guidelines recommended by the developers of the MPlus software (Asparouhov & Muthen, 2012; Muthen & Muthen, 2017) as well as those making recommendations about its use (Geiser, 2012). In particular, I set the number of starts to 600 for initial stage starts, and to 120 for the number of starts to be optimized. This means that for each model estimated, 600 random starting values for the parameters were used to initialize the EM algorithm. Of these 600, 120 that demonstrated the lowest log-likelihood were allowed to continue until they reached convergence or the limit for the number of iterations. In order for a model to me considered trustworthy, of these 120 runs, the lowest log-likelihood must be replicated at least one time.

```{r, compare-solutions-overall-stats, eval = FALSE}
d <- compare_solutions_mplus(df,
                             dm_cog_eng, dm_beh_eng, dm_aff_eng, dm_challenge, dm_competence,
                             starts = c(600, 120),
                             n_profiles_min = 2,
                             n_profiles_max = 10,
                             return_stats_df = TRUE,
                             return_table = TRUE,
                             n_processors = 6,
                             save_models = TRUE,
                             include_BLRT = TRUE)

write_rds(d, "data/overall-stats-for-all-models.rds")
```

```{r, printing-solutions-overall-stats, eval = F}
# d <- read_rds("data/overall-stats-for-all-models.rds")
overall_stats_for_all_models[[1]] %>%
  knitr::kable(format = "latex", booktabs = TRUE, caption = "Overall statistics for all models", linesep = "") %>%
  kableExtra::kable_styling(latex_options = "scale_down") %>%
  kableExtra::landscape()
```

If the log-likelihood is not replicated, then the estimation completed one or more times, but because the same log-likelihood value (and parameter estimates) were not obtained, then the solution can be considered to be "under-identified", a term used to describe solutions that depend strongly upon minor fluctuations in the data (Asparouhov & Muthen, 2007). Accordingly, these solutions may not represent meaningful values and may not be replicable in light of very small changes to the data; these are not considered as candidate solutions for use in subsequent analyses. If no log-likelihood is obtained for any of the random starts, then the software returns an error; in these cases, the convergence criteria--values that determine when a solution has been obtained--are not met. This may be due to a large number of parameters that are estimated relative to the data, such that the number of iterations that the estimation is allowed to go through are not sufficient to obtain a solution (Asparouhov & Muthen, 2007). Like when the log-likelihood is not replicated, these solutions are not considered for use in subsequent analyses.

For every combination of models one through six and from two through ten profiles, only solutions associated with model specifications 1 and 2 (and among these two solutions, only those associated with particular number of profiles) converged. Thus, only solutions associated with models 1 (the model with varying means, equal variances, and covariances fixed to zero) and model 2 (varying means, equal variances, and equal covariances) are explored in subsequent sections. This suggests that the more complex models were too complex given the systematic variability in the data used for the analysis.

### In-depth statistics for particular models

After investigating the general information about a range of model solutions, solutions associated with models 1 and 2 are explored in greater detail, following recommendations associated with mixture modeling (Collins and Lanza, 2009; Geiser, 2012) and the authors of the MPlus software (Muthen & Muthen, 2017) as well as recent peer-reviewed articles (Pastor et al., 2007). For these models, the log-likelihood (LL), a range of information criteria (AIC, BIC, sample adjusted BIC [SABIC], consistent AIC [CAIC]), statistics about the quality of the profile assignments (entropy, which represents the mean posterior probability) are presented.

The information criteria are based on the log-likelihood but take various steps to penalize complex models, and so can be used to directly compare models (i.e., the model with the lowest values for these statistics can be considered to better reflect the underlying properties of the profiles). Simulation studies have suggested that BIC, CAIC, SABIC, and BLRT are most helpful for selecting the correct number of profiles (Nylund, Asparouhov, & Muthen, 2007). For the entropy statistic, higher values are considered better, though scholars have suggested that the entropy statistic not be used for model selection (Lubke & Muthen, 2007).The log-likelihood should not be interpreted directly but is presented in conjunction with the information criteria for context about how each of them differs from the log-likelihood. These are also presented in the figures.

In addition to these statistics, a number of modified likelihood ratio tests (LRTs) are used, as the test statistics associated with unmodified LRT do not follow the distribution that the test is based on (Muthen & Muthen, 2017). These are the Vu-Lo-Mendell-Rubin LRT, Lo-Mendell-Rubin LRT, and the bootstrapped LRT. Of the three, the bootstrapped is considered to be the best indicator of which of two models, one nested (with certain parameters fixed to 0) within the other, fits better, but it is also the most computationally-intensive to carry out (Asparouhov & Muthen, 2012). For each of the LRTs, the test statistic and its associated p-value are provided; a p-value greater than .05 suggests that the model with fewer profiles should be preferred.

```{r, reading-overall-stats}
overall_stats_for_all_models <- readr::read_rds("data/overall-stats-for-all-models.rds")
```

```{r, printing-solutions-spec-stats, eval = TRUE}
overall_stats_for_all_models[[2]] %>%
  arrange(model, n_profile) %>%
  select(-model, `Number of Profiles` = n_profile) %>%
  mutate(VLMR = paste_stats(VLMR_val, VLMR_p),
         LMR = paste_stats(LMR_val, LMR_p),
         BLRT = paste_stats(BLRT_val, BLRT_p)) %>%
  select(everything(), -VLMR_val, -VLMR_p, -LMR_val, -LMR_p, -BLRT_val, -BLRT_p) %>%
  knitr::kable(format = "latex", caption = "Solutions for models that converged with replicated LL", booktabs = TRUE, linesep = "") %>%
  kableExtra::kable_styling(latex_options = "scale_down") %>%
  kableExtra::landscape() %>%
  kableExtra::group_rows("Model 1", 1, 7) %>%
  kableExtra::group_rows("Model 2", 8, 11)
```

```{r, model1, eval = T, cache = F, out.width = "50%", fig.cap = "Fit statistics for model 1 solutions"}
overall_stats_for_all_models[[2]] %>%
  select(n_profile:CAIC, Entropy) %>%
  filter(model == 1) %>%
  mutate(AIC = AIC * -1,
         LL = LL * -1) %>%
  gather(key, val, -n_profile, -model) %>%
  ggplot(aes(x = n_profile, y = val)) +
  geom_point() +
  geom_line() +
  facet_grid(key ~ model, scales = "free") +
  theme_bw() +
  xlab("Number of Profiles") +
  ylab(NULL)
```

```{r, model2, eval = T, cache = F, out.width = "40%", fig.cap = "Fit statistics for model 2 solutions"}

overall_stats_for_all_models[[2]] %>%
  select(n_profile:CAIC, Entropy) %>%
  filter(model == 2) %>%
  mutate(AIC = AIC * -1,
         LL = LL * -1) %>%
  gather(key, val, -n_profile, -model) %>%
  ggplot(aes(x = n_profile, y = val)) +
  geom_point() +
  geom_line() +
  facet_grid(key ~ model, scales = "free") +
  theme_bw() +
  xlab("Number of Profiles") +
  ylab(NULL)
```

Looking across the statistics presented, some general ideas about which models are to be preferred emerge. Solutions are interpreted first for each model individually and then across models with the goal of choosing a smaller number of models to investigate in more detail.

For solutions associated with model 1, the decrease (indicating a preferred model) in information criteria becomes smaller as the number of profiles increases from 5 to 6 and 6 to 7. A solution associated with 8 profiles did not replicate the log-likelihood and the VLMR and LMR suggest that the solution associated with 9 profiles did not fit better than that with 8 profiles, suggesting that models with 7 or fewer profiles be preferred. Considering these models, the entropy statistic increases by a large amount between the solution associated with 4 and 5 profiles (and then decreases slightly between 5 and 6 and 6 and 7 profile solutions), suggesting (but not providing conclusive evidence) that models 5, 6, or 7 may be preferred. The bootstrapped LRT suggests that, until the log-likelihood is not replicated, every more complex model be selected. Taking these pieces of evidence into conclusion, for model 1, solutions associated with 4 through 7 may be considered in more depth, with an emphasis on solutions associated with profiles with 5 and 6 profiles on the basis of the slowing of the decrease in the information criteria associated with the solutions with greater profiles than these, and the increase in the entropy from 4 to 5 (and 6) profile solutions.

For solutions associated with model 2, only those associated with 2-5 profile solutions were associated with log-likelihoods that were replicated. For these four models, the log-likelihood decreased in a mostly consistent way, such that changes in the decrease are not as evident as those associated with model 1. The entropy statistic decreases from 2 to 3 profile solutions, increases from 3 to 4 profile solutions, and then decreases slightly from 4 to 5 profile solutions, providing some information that models associated with 4 profiles be preferred to the others. All of the LRTs suggest that the more complex model be selected, not providing clear information about which solutions are to be preferred. On the basis of these pieces of evidence, models with 3, 4, and 5 solutions may be considered in more depth. However, there is a lack of consistent evidence favoring more or less complex models.

### Comparison of model 1 and model 2 type solutions

When looking across solutions, some overall patterns in terms of what profiles emerge and some directions for which models are to be selected for use in subsequent analysis can be identified. First, overall patterns are discussed. In the table, which profiles emerge from which solution is presented.

There is a wide range of profiles. Some appear very commonly, particularly those (full and universally low) characterized by high or low levels across all of the variables. Moderate profiles, both all moderate (characterized by moderately high levels across all of the variables) and moderately low (characterized by low levels across all of the variables), also appeared commonly, particularly for the solutions for model 1.

```{r, compare-profiles-by-solution}
d <- readxl::read_xlsx("tables/prof-assignments.xlsx")

d[, -1] %>%
  knitr::kable(format = "latex", booktabs = TRUE, caption = "Profile assignments by LPA solution (models and numbers of profiles)", linesep = "", align=rep('c', 13)) %>%
  kableExtra::kable_styling(latex_options = "scale_down") %>%
  kableExtra::landscape() %>%
  kableExtra::row_spec(0, angle = -60) %>%
  kableExtra::group_rows("Model 1", 1, 6) %>%
  kableExtra::group_rows("Model 2", 7, 9)
```

### Examination of specific candidate models

Following from the in-depth exploration of the candidate solutions, in this section, model solutions associated with specific model types and the number of profiles are investigated. In particular, the model one type, six profile, and model one type, seven profile solutions are described. Descriptions of other candidate solutions is included in the appendix. For all of the solutions, the raw data and the data that are centered to have a mean equal to 0 and a standard deviation of 1 (thus, the y-axis on each of the plots is labeled "Z-score").

#### Model type: 1, Profiles: 6

This solution is characterized by:

- A *full* profile, profile 6
- An *universally low* profile, profile 2
- An *all moderate* profile, profile 5--and, like, the model 1, six profile solution--with moderate levels of affective engagement
- An *only behaviorally engaged* profile, profile 1, with moderate levels of behavioral engagement, very low affective engagement, and moderately (low) levels of cognitive engagement and challenge and competence
- An *only affectively engaged* profile, profile 4, with moderate levels of affective engagement, low levels of behavioral engagement, and moderately (low) levels of cognitive engagement and challenge and competence
- An *engaged and competent but not challenged* profile, profile 3, characterized by high levels of each of the three dimensions of engagement and of competence, but with low levels of challenge

The number of observations associated with each of the profiles is somewhat balanced, with the universally low profile with the largest number of observations (*n* = 667; the same number for this profile as in the model 1, five profile solution), followed by the all moderate profile (*n* = 638). Each of the other four profiles were associated with 300 to 400 observations. Unlike the model 1, four and five profile solutions, which distinguished observations on *either* a condition of engagement (i.e., competence) or one of its dimensions (i.e., cognitive, behavioral, and affective), this solution was associated with profiles that distinguished observations on the basis of both: There were profiles for only behaviorally and affectively engaged and for engaged and competent but not challenged. This solution is compelling because it appears to group students on the basis of multiple of the indicators, and demonstrate viability on the basis of the fit statistics (i.e., the tables and figure). The log-likelihood was replicated two times, with the next lowest log-likelihood not being replicated, followed by a log-likelihood that was replicated (at least) seven times. This solution (associated with the log-likelihood that was replicated [at least] seven times) could be investigated in further detail, to see whether--and if so, how--it differs from the solution interpreted here. This solution is a strong candidate for use in subsequent analyses.

```{r, spec-solutions-m1_6, cache = FALSE, eval = FALSE, fig.width = 6, out.width = "100%"}
m1_6 <- estimate_profiles_mplus(df,
                                dm_cog_eng, dm_beh_eng, dm_aff_eng, dm_challenge, dm_competence,
                                starts = c(600, 120),
                                model = 1,
                                n_profiles = 6,
                                include_BLRT=TRUE,
                                n_processors = 6, remove_tmp_files = FALSE)
write_rds(m1_6, "data/models/m1_6.rds")
```

```{r, m1_6p, cache = FALSE, eval = TRUE, fig.width = 7, fig.asp = .618, out.width = "95%", fig.align = "center"}
m1_6 <- read_rds("data/models/m1_6.rds")

p <- m1_6 %>%
  plot_profiles_mplus(to_center = TRUE, to_scale = TRUE) +
  scale_x_discrete("", labels = c("Only behavioral (n = 370)",
                                  "Universally low (n = 667)",
                                  "Engaged and competent but not challenged (n = 450)",
                                  "Only affective (n = 345)",
                                  "All moderate (n = 638)",
                                  "Full (n = 488)")) +
  xlab(NULL) +
  ylab("Z-score") +
  viridis::scale_fill_viridis("",
                              limits = c("DM_AFF_E", "DM_BEH_E", "DM_COG_E", "DM_CHALL", "DM_COMPE"),
                              labels = c("Affective", "Behavioral", "Cognitive", "Challenge", "Competence"), discrete = TRUE) +
  theme(plot.margin = margin(1, 0, 0, 1, "cm"))

p

m1_6 <- read_rds("data/models/m1_6.rds")

p <- m1_6 %>%
  plot_profiles_mplus(to_center = FALSE, to_scale = FALSE) +
  scale_x_discrete("", labels = c("Only behavioral (n = 370)",
                                  "Universally low (n = 667)",
                                  "Engaged and competent but not challenged (n = 450)",
                                  "Only affective (n = 345)",
                                  "All moderate (n = 638)",
                                  "Full (n = 488)")) +
  xlab(NULL) +
  ylab("Value") +
  viridis::scale_fill_viridis("",
                              limits = c("DM_AFF_E", "DM_BEH_E", "DM_COG_E", "DM_CHALL", "DM_COMPE"),
                              labels = c("Affective", "Behavioral", "Cognitive", "Challenge", "Competence"), discrete = TRUE) +
  theme(plot.margin = margin(1, 0, 0, 1, "cm"))

p
```

```{r, m1_6p-ll, eval = TRUE, cache = FALSE}
extract_LL_mplus() %>% slice(1:10) %>% knitr::kable()
```

#### Model type: 1, Profiles: 7

This solution is characterized by:

- A *full* profile, profile 7
- A *universally low* profile, profile 1
- A *competent but not engaged or challenged* profile, profile 2, characterized by high competence and moderate (low) or low levels of engagement and challenge
- A *moderately low* profile, profile 3, characterized by moderately low levels of all of the variables
- A *challenged* profile, profile 4, characterized by high challenge, moderate (high) levels of engagement, and moderate (low) levels of competence
- A *highly challenged* profile, profile 5, characterized by patterns similar to those of the challenged profile, but with higher challenge and with low levels of both engagement and challenge
- A *challenged but not engaged or competent* profile, profile 6, characterized by low levels of challenge, and high levels of engagement and competence

The number of observations associated with each of the profiles is not very balanced, with few (*n* = 181) observations associated with the universally low profile and few (*n* = 222) observations associated with the highly challenged profile. The number of observations associated with the other profiles ranged from 317 to 651. Distinct from other solutions, none of the other five profiles were found in the other model 1 solutions. Two pairs of the profiles--challenged and highly challenged and universally low and moderately low--exhibited similar patterns among the variables that were distinguished by different mean levels. The log-likelihood was replicated twice, with the next lowest log-likelihood being replicate four times, possibly warranting further investigation. Taken together, this solution raises questions about whether it may be too complex, possibly suggesting preference for model 1 five and six profile solutions.

```{r, spec-solutions-m1_7, cache = FALSE, eval = FALSE}
m1_7 <- estimate_profiles_mplus(df,
                                dm_cog_eng, dm_beh_eng, dm_aff_eng, dm_challenge, dm_competence,
                                starts = c(600, 120),
                                model = 1,
                                n_profiles = 7,
                                include_BLRT=TRUE,
                                n_processors = 6, remove_tmp_files = FALSE)
write_rds(m1_7, "data/models/m1_7.rds")
```

```{r, m1_7p, cache = FALSE, eval = TRUE,  eval = FALSE, fig.width = 7, fig.asp = .618, out.width = "90%", fig.align = "center"}
m1_7 <- readr::read_rds("data/models/m1_7.rds")

m1_7 %>%
  tidyLPA::plot_profiles_mplus(to_center = TRUE, to_scale = TRUE) +
  scale_x_discrete(labels = c("Universally low (n = 181)",
                              "Competent but not engaged or challenged (n = 317)",
                              "Moderately low (n = 651)",
                              "Challenged (n = 569)",
                              "Highly challenged (n = 222)",
                              "Engaged and competent but not challenged (n = 568)",
                              "Full (n = 450)")) +
  xlab(NULL) +
  ylab("Z-score") +
  viridis::scale_fill_viridis("",
                              limits = c("DM_AFF_E", "DM_BEH_E", "DM_COG_E", "DM_CHALL", "DM_COMPE"),
                              labels = c("Affective", "Behavioral", "Cognitive", "Challenge", "Competence"), discrete = TRUE) +
  theme(plot.margin = margin(1, 0, 0, 1, "cm"))

m1_7 <- readr::read_rds("data/models/m1_7.rds")

m1_7 %>%
  tidyLPA::plot_profiles_mplus(to_center = FALSE, to_scale = FALSE) +
  scale_x_discrete(labels = c("Universally low (n = 181)",
                              "Competent but not engaged or challenged (n = 317)",
                              "Moderately low (n = 651)",
                              "Challenged (n = 569)",
                              "Highly challenged (n = 222)",
                              "Engaged and competent but not challenged (n = 568)",
                              "Full (n = 450)")) +
  xlab(NULL) +
  ylab("Value") +
  viridis::scale_fill_viridis("",
                              limits = c("DM_AFF_E", "DM_BEH_E", "DM_COG_E", "DM_CHALL", "DM_COMPE"),
                              labels = c("Affective", "Behavioral", "Cognitive", "Challenge", "Competence"), discrete = TRUE) +
  theme(plot.margin = margin(1, 0, 0, 1, "cm"))
```

```{r, m1_7-ll, cache = FALSE, eval = FALSE}
extract_LL_mplus() %>% slice(1:10) %>% knitr::kable()
```

#### Looking across model 1 and model 2 type solutions

The model 1, six and seven profile solutions are compelling because both show profiles that are distinguished by dimensions of engagement and its conditions (challenge and competence). Note that for this model, only the means and variances are estimated (and so no covariances are estimated), and the variances are constrained to be the same across the profiles. While this is a very restrictive model, it, along with the model 3 type (which did not lead to solutions for any of the numbers of profiles specified) also is a standard model for LPA, in that it meets the assumption of local independence (of the variables that make up the profiles--unlike for models in which covariances are estimated) typical common to LPA (see Muthen & Muthen, 2016). While some of the solutions associated with the model 2 type did reach solutions, these demonstrated less appealing properties in terms of their fit statistics as well as their interpretability and with respect to concerns of parsimony. Thus, while no covariances are estimated for the model 1 type solutions, there is no requirement that these be specified; their benefit, when models associated with them are preferred, is that they can provide better fit: they can be used to better explain or predict the data in a sample, but their inclusion also means that over-fitting the model to the data can become a greater concern.

For each solution, alternate solutions associated with higher log-likelihoods were explored. One advantage of the six profile solution is that most of its profiles can also be identified in solutions with fewer profiles. For the six profile solutions, this alternate solution was very different, whereas for the seven profile solutions, this alternate solution was highly similar. The model solutions exhibit a less clear pattern in terms of which profiles appear when. All else being equal, on the basis of parsimony, the model 1, six profile solution may be preferred and is selected for use in subsequent analyses.

As a type of sensitivity analysis focused on alternate model specifications (different from the kind described earlier for quantifying how robust an inference is to potential sources of bias or confounding variables, e.g. Frank, 2003), the model 1, seven profile solution is also explored, but results for it are included in the appendix. This model is less restrictive but does not meet the assumption of independence; some scholars refer to it, as such, as a general or Guassian mixture model solution, instead of an LPA solution (Bauer, 2004). Because covariances are estimated, relationships between the variables not captured in their mean levels estimated for each profile are also estimated. This suggests that these models may be modeling different relations between the variables than those associated with model 1 and that they may fit the data better, but they are also more complex and so should be interpreted with consideration these added parameters.
